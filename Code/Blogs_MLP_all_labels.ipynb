{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textklassifikation mit Vektorisierung in scikit-learn und MLPClassifier \n",
    "Labels (Themen und Disziplinen) sind nicht reduziert (all_labels)\n",
    "\n",
    "Autorin: Maria Hartmann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # module to one-hot-encode the labels\n",
    "from sklearn.pipeline import Pipeline # assemples transormers \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # module to transform a count matrix to a normalized tf-idf representation\n",
    "from sklearn.neural_network import MLPClassifier # MultiLayerPerceptron classifier \n",
    "from sklearn.model_selection import RandomizedSearchCV # module for paramter optimization\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "np.random.seed(7) # fix random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einlesen des Trainings- und Testdatensatzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = '../Datasets/all_labels_trainset.csv' \n",
    "testset = '../Datasets/all_labels_testset.csv' \n",
    "\n",
    "trainset_csv = pd.read_csv(trainset, delimiter=';')\n",
    "X_train = trainset_csv['text'].values\n",
    "y_train = trainset_csv['classes'].values\n",
    "z_train = trainset_csv['filename'].values\n",
    "\n",
    "testset_csv = pd.read_csv(testset, delimiter=';')\n",
    "X_test = testset_csv['text'].values\n",
    "y_test = testset_csv['classes'].values\n",
    "z_test = testset_csv['filename'].values\n",
    "\n",
    "# Splitten der Labels pro Blogbeitrag\n",
    "y_train = [e.split(', ') for e in y_train]\n",
    "y_test = [e.split(', ') for e in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nummer_212.txt\n",
      "['histoire_d', \"sciences de l'information et de la communication_d\", 'bibliothéconomie_d', 'histoire_t', 'histoire intellectuelle_t', 'histoire et sociologie des médias_t', 'histoire culturelle_t']\n",
      "die gemälde der habsburgischen sammlungen zu wien wurden von der stallburg ins belvedere transferiert und dort von christian von mechel neu angeordnet und aufgehängt\n"
     ]
    }
   ],
   "source": [
    "print(z_train[0])\n",
    "print(y_train[0])\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-hot-Kodierung der Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# k-hot-encode labels mit MultiLabelBinarizer\n",
    "label_encoder = MultiLabelBinarizer()\n",
    "encoded_y_train = label_encoder.fit_transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "print(encoded_y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n",
      "0 1914-1918_t\n",
      "1 1918-1939_t\n",
      "2 1939-1945_t\n",
      "3 1945-1989_t\n",
      "4 administration publique et développement_d\n",
      "5 anthropologie politique_t\n",
      "6 approches de corpus_t\n",
      "7 archives_t\n",
      "8 archéologie_d\n",
      "9 arts et humanités_d\n",
      "10 arts_d\n",
      "11 asie_t\n",
      "12 bas moyen âge_t\n",
      "13 bibliothéconomie_d\n",
      "14 biomédecine_d\n",
      "15 chine_t\n",
      "16 communication_d\n",
      "17 conflits_t\n",
      "18 digital humanities_t\n",
      "19 enquêtes_t\n",
      "20 europe centrale et orientale_t\n",
      "21 europe_t\n",
      "22 france_t\n",
      "23 guerres_t\n",
      "24 haut moyen âge_t\n",
      "25 histoire culturelle_t\n",
      "26 histoire de l'art_t\n",
      "27 histoire des religions_t\n",
      "28 histoire des sciences sociales_d\n",
      "29 histoire des sciences_t\n",
      "30 histoire du droit_t\n",
      "31 histoire et archéologie_d\n",
      "32 histoire et philosophie des sciences_d\n",
      "33 histoire et sociologie des médias_t\n",
      "34 histoire industrielle_t\n",
      "35 histoire intellectuelle_t\n",
      "36 histoire politique_t\n",
      "37 histoire sociale_t\n",
      "38 histoire urbaine_t\n",
      "39 histoire économique_t\n",
      "40 histoire_d\n",
      "41 histoire_t\n",
      "42 historiographie_t\n",
      "43 humanités pluridisciplinaires_d\n",
      "44 information_t\n",
      "45 langage_t\n",
      "46 langue et linguistique_d\n",
      "47 linguistique appliquée_d\n",
      "48 linguistique_t\n",
      "49 littérature_d\n",
      "50 littératures_t\n",
      "51 monde germanique_t\n",
      "52 moyen âge_t\n",
      "53 musique_d\n",
      "54 méthodes de traitement et de représentation_t\n",
      "55 patrimoine_t\n",
      "56 pays baltes et scandinaves_t\n",
      "57 pensée_t\n",
      "58 philosophie_d\n",
      "59 philosophie_t\n",
      "60 pluridisciplinarité_d\n",
      "61 prospectives_t\n",
      "62 psychisme_t\n",
      "63 psychologie expérimentale_d\n",
      "64 psychologie_d\n",
      "65 psychologie_t\n",
      "66 relations internationales_t\n",
      "67 religions_d\n",
      "68 religions_t\n",
      "69 représentations_t\n",
      "70 révolution française_t\n",
      "71 sciences auxiliaires de l'histoire_t\n",
      "72 sciences cognitives_t\n",
      "73 sciences de l'information et bibliothéconomie_d\n",
      "74 sciences de l'information et de la communication_d\n",
      "75 sciences de l'information_t\n",
      "76 sciences de l'éducation_t\n",
      "77 sciences politiques_d\n",
      "78 sciences sociales interdisciplinaires_d\n",
      "79 sociologie de la culture_t\n",
      "80 sociologie des religions_t\n",
      "81 sociologie des sciences_t\n",
      "82 sociologie du travail_t\n",
      "83 sociologie et anthropologie_d\n",
      "84 sociologie urbaine_t\n",
      "85 sociologie économique_t\n",
      "86 sociologie_d\n",
      "87 sociologie_t\n",
      "88 travail social et politique sociale_d\n",
      "89 travail social_d\n",
      "90 vie de la recherche_t\n",
      "91 violence_t\n",
      "92 xvie siècle_t\n",
      "93 xviie siècle_t\n",
      "94 xviiie siècle_t\n",
      "95 xxe siècle_t\n",
      "96 xxie siècle_t\n",
      "97 âges de la vie_t\n",
      "98 économie_d\n",
      "99 édition électronique_t\n",
      "100 éducation et sciences de l'éducation_d\n",
      "101 éducation spécialisée_d\n",
      "102 éducation_d\n",
      "103 éducation_t\n",
      "104 épistémologie et méthodes_t\n",
      "105 époque contemporaine_t\n",
      "106 époque moderne_t\n",
      "107 étude des genres_t\n",
      "108 études anciennes_d\n",
      "109 études asiatiques_d\n",
      "110 études des sciences_t\n",
      "111 études régionales_d\n",
      "112 études sur la famille_d\n",
      "113 études visuelles_t\n"
     ]
    }
   ],
   "source": [
    "print(len(label_encoder.classes_))\n",
    "for i, element in enumerate(label_encoder.classes_):\n",
    "    print(i, element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vektorisierung und Klassifikation der Daten mit scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "stopwords = open('../Preprocessing/filtered_words.txt', 'r', encoding='utf-8').read().splitlines()\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1), max_features=max_features, stop_words=stopwords)\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# first try with best params for vect and tfidf from kNN classification\\ntext_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,4), max_df=0.9, min_df=0.01)),#min_df=0.0 auf min_df=0.01 geändert\\n                     #('vect', CountVectorizer(ngram_range=(1,4), max_features=max_features)),\\n                     ('tfidf', TfidfTransformer(use_idf=True)),\\n                     ('clf', MLPClassifier(hidden_layer_sizes=(1024,512), max_iter=500, validation_fraction=0.1, early_stopping=True, verbose=True, random_state=1)),\\n                    ])\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# first try with best params for vect and tfidf from kNN classification\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,4), max_df=0.9, min_df=0.01)),#min_df=0.0 auf min_df=0.01 geändert\n",
    "                     #('vect', CountVectorizer(ngram_range=(1,4), max_features=max_features)),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                     ('clf', MLPClassifier(hidden_layer_sizes=(1024,512), max_iter=500, validation_fraction=0.1, early_stopping=True, verbose=True, random_state=1)),\n",
    "                    ])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', vectorizer),\n",
    "                     ('tfidf', tfidf_transformer),\n",
    "                     ('clf', MLPClassifier(hidden_layer_sizes=(4096, 1024), tol=0.0001, early_stopping=True, validation_fraction=0.1, verbose=True, random_state=1))\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 19.21366700\n",
      "Validation score: 0.384570\n",
      "Iteration 2, loss = 9.46798991\n",
      "Validation score: 0.458796\n",
      "Iteration 3, loss = 5.49554281\n",
      "Validation score: 0.513735\n",
      "Iteration 4, loss = 2.98014910\n",
      "Validation score: 0.540620\n",
      "Iteration 5, loss = 1.62345869\n",
      "Validation score: 0.542957\n",
      "Iteration 6, loss = 0.94501900\n",
      "Validation score: 0.544126\n",
      "Iteration 7, loss = 0.62290702\n",
      "Validation score: 0.541788\n",
      "Iteration 8, loss = 0.48465459\n",
      "Validation score: 0.538866\n",
      "Iteration 9, loss = 0.40534940\n",
      "Validation score: 0.542373\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "start = time.time()\n",
    "text_clf = text_clf.fit(X_train, encoded_y_train)\n",
    "processing_time = (time.time() - start) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=['und', 'die', 'der'], strip_accents=None,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(4096, 1024), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
      "       warm_start=False))], 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=['und', 'die', 'der'], strip_accents=None,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(4096, 1024), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
      "       warm_start=False), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': 10000, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': ['und', 'die', 'der'], 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__activation': 'relu', 'clf__alpha': 0.0001, 'clf__batch_size': 'auto', 'clf__beta_1': 0.9, 'clf__beta_2': 0.999, 'clf__early_stopping': True, 'clf__epsilon': 1e-08, 'clf__hidden_layer_sizes': (4096, 1024), 'clf__learning_rate': 'constant', 'clf__learning_rate_init': 0.001, 'clf__max_iter': 200, 'clf__momentum': 0.9, 'clf__nesterovs_momentum': True, 'clf__power_t': 0.5, 'clf__random_state': 1, 'clf__shuffle': True, 'clf__solver': 'adam', 'clf__tol': 0.0001, 'clf__validation_fraction': 0.1, 'clf__verbose': True, 'clf__warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "clf_params = text_clf.get_params()\n",
    "print(clf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predicted = text_clf.predict(X_test)\n",
    "#predicted_proba = text_clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8211091373433589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# precision is a measure of result relevancy\n",
    "precision = precision_score(encoded_y_test, predicted, average='samples')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7192799817449186\n"
     ]
    }
   ],
   "source": [
    "# recall is a measure of how many truly relevant results are returned\n",
    "recall = recall_score(encoded_y_test, predicted, average='samples')  \n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7443742252177321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# F1 score is a weighted average of the precision and recall\n",
    "f1 = f1_score(encoded_y_test, predicted, average='samples') \n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic\n"
     ]
    }
   ],
   "source": [
    "out_act = tfidf_output = text_clf.named_steps.clf.out_activation_\n",
    "print(out_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = '../MLP'\n",
    "if not os.path.exists(output):\n",
    "    os.makedirs(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with open(output+\\'/MLP_all_labels_first_params.txt\\',\"w+\", encoding=\"utf8\") as params:\\n#with open(output+\\'/MLP_all_labels_first_params_max_features.txt\\',\"w+\", encoding=\"utf8\") as params:\\n    params.write(\"First parameters for classification with MLP (all labels):\")\\n    params.write(\"\\nprocessing_time: %s\" % processing_time)\\n    for key, value in clf_params.items():\\n        params.write(\"\\n%s: %s\" % (key, value))\\n    params.write(\"\\nactivation function output layer: %s\" % text_clf.named_steps.clf.out_activation_)    \\n    params.write(\"\\nprecision: %s\" % precision)\\n    params.write(\"\\nrecall: %s\" % recall)\\n    params.write(\"\\nf1-score: %s\" % f1)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write first parameters and scores to file\n",
    "\"\"\"with open(output+'/MLP_all_labels_first_params.txt',\"w+\", encoding=\"utf8\") as params:\n",
    "#with open(output+'/MLP_all_labels_first_params_max_features.txt',\"w+\", encoding=\"utf8\") as params:\n",
    "    params.write(\"First parameters for classification with MLP (all labels):\")\n",
    "    params.write(\"\\nprocessing_time: %s\" % processing_time)\n",
    "    for key, value in clf_params.items():\n",
    "        params.write(\"\\n%s: %s\" % (key, value))\n",
    "    params.write(\"\\nactivation function output layer: %s\" % text_clf.named_steps.clf.out_activation_)    \n",
    "    params.write(\"\\nprecision: %s\" % precision)\n",
    "    params.write(\"\\nrecall: %s\" % recall)\n",
    "    params.write(\"\\nf1-score: %s\" % f1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write parameters and scores to file\n",
    "\n",
    "with open(output+'/MLP_all_labels_params.txt',\"a\", encoding=\"utf8\") as params:\n",
    "    params.write(\"\\n*********************************************************************************************\")\n",
    "    params.write(\"\\nParameters for classification with MLP (all labels):\")\n",
    "    params.write(\"\\n*********************************************************************************************\")\n",
    "    params.write(\"\\n%s\" % text_clf.named_steps.vect)\n",
    "    params.write(\"\\n%s\" % text_clf.named_steps.tfidf)\n",
    "    params.write(\"\\n%s\" % text_clf.named_steps.clf)\n",
    "    #for key, value in clf_params.items():\n",
    "        #params.write(\"\\n%s: %s\" % (key, value))\n",
    "    params.write(\"\\nclasses: %s\" % text_clf.named_steps.clf.n_outputs_)\n",
    "    params.write(\"\\nlayers: %s\" % text_clf.named_steps.clf.n_layers_)\n",
    "    params.write(\"\\nactivation function output layer: %s\" % text_clf.named_steps.clf.out_activation_) \n",
    "    params.write(\"\\nepochs: %s\" % text_clf.named_steps.clf.n_iter_)\n",
    "    params.write(\"\\nprocessing time: %s\" % processing_time)\n",
    "    params.write(\"\\nSCORES:\")\n",
    "    params.write(\"\\nprecision: %s\" % precision)\n",
    "    params.write(\"\\nrecall: %s\" % recall)\n",
    "    params.write(\"\\nf1-score: %s\" % f1)\n",
    "    params.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED:\n",
      "('approches de corpus_t', 'archives_t', 'enquêtes_t', 'histoire et archéologie_d', 'histoire_t', 'sciences sociales interdisciplinaires_d')\n",
      "TRUE:\n",
      "['histoire et archéologie_d', 'sciences sociales interdisciplinaires_d', 'histoire_t', 'approches de corpus_t', 'enquêtes_t', 'archives_t']\n"
     ]
    }
   ],
   "source": [
    "# write real labels and predictions to file\n",
    "\n",
    "inverse_prediction = label_encoder.inverse_transform(predicted)\n",
    "print('PREDICTED:')\n",
    "print(inverse_prediction[0])\n",
    "print('TRUE:')\n",
    "print(y_test[0])\n",
    "\n",
    "with open(output+'/MLP_all_labels_predictions.txt',\"w+\", encoding=\"utf8\") as preds:\n",
    "    preds.write(\"Predictions from classification with Multi-Layer-Perzeptron and vectorization in scikit-learn (all labels):\\n\\n\")\n",
    "    for ident, label, pred in zip(z_test, y_test, inverse_prediction):\n",
    "        label = sorted(label)\n",
    "        pred = sorted(pred)\n",
    "        preds.write(ident)\n",
    "        preds.write('\\n')\n",
    "        preds.write('TRUE: ')\n",
    "        for element in label:\n",
    "            preds.write('%s, ' % element)\n",
    "        preds.write('\\n')\n",
    "        preds.write('PRED: ')\n",
    "        for element in pred:\n",
    "            preds.write('%s, ' % element)\n",
    "        preds.write('\\n')\n",
    "        preds.write('\\n*********************\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speicherung der vektrorisierten Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17109\n",
      "nummer.hypotheses.org/212\n"
     ]
    }
   ],
   "source": [
    "z_train = [e.replace('.txt', '') for e in z_train]\n",
    "z_test = [e.replace('.txt', '') for e in z_test]\n",
    "ident_train = [e.replace('_', '.hypotheses.org/') for e in z_train]\n",
    "ident_test = [e.replace('_', '.hypotheses.org/') for e in z_test]\n",
    "\n",
    "print(len(ident_train))\n",
    "print(ident_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17109, 10000)\n"
     ]
    }
   ],
   "source": [
    "# vectorize textdata\n",
    "train_vect = vectorizer.transform(X_train)\n",
    "train_tfidf = tfidf_transformer.transform(train_vect)\n",
    "print(train_tfidf.shape)\n",
    "\n",
    "test_vect = vectorizer.transform(X_test)\n",
    "test_tfidf = tfidf_transformer.transform(test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<17109x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2388086 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(test_tfidf))\n",
    "train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filename, classes, textvectors in csv file\n",
    "# trainset\n",
    "# speichert vektorisierten Text\n",
    "output_file_train = 'Datasets/all_labels_train_scikit-learn_sparse_matrix.npz'\n",
    "scipy.sparse.save_npz('../'+output_file_train, train_tfidf)\n",
    "\n",
    "# speichert filenames und classes\n",
    "with open('../Datasets/all_labels_train_idents_labels.csv', 'w', newline='', encoding=\"utf-8\") as traincsv:\n",
    "    train = csv.writer(traincsv, delimiter = \";\")\n",
    "    train.writerow([\"url\", \"classes\", \"filename\"])\n",
    "    \n",
    "    for ident, labels in zip(ident_train, y_train):\n",
    "        labellist = \", \".join(labels)\n",
    "        train.writerow([ident, labellist, output_file_train])\n",
    "\n",
    "# testset\n",
    "# speichert vektorisierten Text\n",
    "output_file_test = 'Datasets/all_labels_test_scikit-learn_sparse_matrix.npz'\n",
    "scipy.sparse.save_npz('../'+output_file_test, test_tfidf)\n",
    "\n",
    "# speichert filenames und classes\n",
    "with open('../Datasets/all_labels_test_idents_labels.csv', 'w', newline='', encoding=\"utf-8\") as testcsv:\n",
    "    test = csv.writer(testcsv, delimiter = \";\")\n",
    "    test.writerow([\"url\", \"classes\", \"filename\"])\n",
    "    \n",
    "    for ident, labels in zip(ident_test, y_test):\n",
    "        labellist = \", \".join(labels)\n",
    "        test.writerow([ident, labellist, output_file_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speicherung der korpusspezifischen Stoppwörter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269816\n"
     ]
    }
   ],
   "source": [
    "# write corpus specific stopwords to file \n",
    "\n",
    "stopwords = text_clf.named_steps.vect.stop_words_\n",
    "print(len(stopwords))\n",
    "#print(stopwords)\n",
    "with open('../Preprocessing/filtered_words_MLP.txt',\"w+\", encoding=\"utf8\") as stops:\n",
    "    for element in stopwords:\n",
    "        stops.write(element)\n",
    "        stops.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameteroptimierung mit Rastersuche (RandomizedSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', MLPClassifier(validation_fraction=0.1, early_stopping=True, verbose=True, random_state=1)),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter tuning with RandomSearch\n",
    "stopwords = open('../Preprocessing/filtered_words.txt', 'r', encoding='utf-8').read().splitlines()\n",
    "rs_parameters = {'vect__ngram_range': [(1,1),(1,2),(1,3),(1,4)], \n",
    "                 #'vect__max_df' : (0.7, 0.8, 0.9), #1.0 \n",
    "                 #'vect__min_df' : (0.01, 0.05, 0.1), #0.0\n",
    "                 'vect__stop_words' : (stopwords, None),\n",
    "                 'vect__max_features': (100000,50000,25000,10000,7500,5000,2500,1000,500,300,100),\n",
    "                 'tfidf__use_idf': (True, False),\n",
    "                 'clf__hidden_layer_sizes': ((4096,2048),(2048,1024),(1024,512),(512,256),(256,128),(4096,1024),(2048,512),(512,128))\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "[CV] vect__stop_words=['und', 'die', 'der'], vect__ngram_range=(1, 1), vect__max_features=1000, tfidf__use_idf=True, clf__hidden_layer_sizes=(2048, 512) \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-345fdeef2b04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrs_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrs_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mrs_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrs_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded_y_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mrs_processing_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \"\"\"\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[0;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                     **fit_params_steps[name])\n\u001b[0m\u001b[0;32m    214\u001b[0m                 \u001b[1;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[1;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                        **fit_params):\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "rs_clf = RandomizedSearchCV(clf, rs_parameters, cv=3, n_iter=50, n_jobs=1, verbose=10, random_state=1)\n",
    "start = time.time()\n",
    "rs_clf = rs_clf.fit(X_train, encoded_y_train)\n",
    "rs_processing_time = (time.time() - start) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = rs_clf.best_score_\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = rs_clf.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_clf_params = rs_clf.get_params()\n",
    "print(rs_clf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict \n",
    "rs_predicted = rs_clf.predict(X_test)\n",
    "#print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_score computes the accuracy of correct predictions\n",
    "rs_accuracy_score = accuracy_score(encoded_y_test, rs_predicted)\n",
    "print(rs_accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision is a measure of result relevancy\n",
    "rs_precision = precision_score(encoded_y_test, rs_predicted, average='samples')\n",
    "print(rs_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall is a measure of how many truly relevant results are returned\n",
    "rs_recall = recall_score(encoded_y_test, rs_predicted, average='samples')  \n",
    "print(rs_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score is a weighted average of the precision and recall\n",
    "rs_f1 = f1_score(encoded_y_test, rs_predicted, average='samples') \n",
    "print(rs_f1)\n",
    "#49,46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(encoded_y_test, rs_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ergebnisse in Dateien speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = '../MLP'\n",
    "if not os.path.exists(output):\n",
    "    os.makedirs(output)\n",
    "    \n",
    "timestamp = time.strftime('%Y-%m-%d_%H.%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write real labels and predictions to file\n",
    "\n",
    "inverse_prediction = label_encoder.inverse_transform(rs_predicted)\n",
    "print('PREDICTED:')\n",
    "print(inverse_prediction[0])\n",
    "print('TRUE:')\n",
    "print(y_test[0])\n",
    "\n",
    "with open(output+'/MLP_all_labels_rs_predictions_%s.txt' % timestamp,\"w+\", encoding=\"utf8\") as preds:\n",
    "    preds.write(\"Predictions from classification with Multi-Layer-Perzeptron and vectorization in scikit-learn (all labels):\\n\\n\")\n",
    "    for ident, label, pred in zip(z_test, y_test, inverse_prediction):\n",
    "        label = sorted(label)\n",
    "        pred = sorted(pred)\n",
    "        preds.write(ident)\n",
    "        preds.write('\\n')\n",
    "        preds.write('TRUE: ')\n",
    "        for element in label:\n",
    "            preds.write('%s, ' % element)\n",
    "        preds.write('\\n')\n",
    "        preds.write('PRED: ')\n",
    "        for element in pred:\n",
    "            preds.write('%s, ' % element)\n",
    "        preds.write('\\n')\n",
    "        preds.write('\\n*********************\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write parameters and scores to file\n",
    "\n",
    "with open(output+'/MLP_all_labels_rs_params_%s.txt' % timestamp,\"w+\", encoding=\"utf8\") as params:\n",
    "    params.write(\"Parameters for classification with Multi-Layer-Perceptron and vectorization in scikit-learn from randomized search (all labels):\")\n",
    "    params.write(\"\\nprocessing_time: %s\" % rs_processing_time)\n",
    "    params.write(\"\\nparams:\")\n",
    "    for key, value in rs_clf_params.items():\n",
    "        params.write(\"\\n%s: %s\" % (key, value))\n",
    "    params.write(\"\\nbest params:\")\n",
    "    for key, value in best_params.items():\n",
    "        params.write(\"\\n%s: %s\" % (key, value))\n",
    "    params.write(\"\\nbest_score: %s\" % best_score)\n",
    "    params.write(\"\\nprecision: %s\" % rs_precision)\n",
    "    params.write(\"\\nrecall: %s\" % rs_recall)\n",
    "    params.write(\"\\nf1-score: %s\" % rs_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rs_clf.cv_results_\n",
    "df = pd.DataFrame(data=results)\n",
    "print(df)\n",
    "df.to_csv(output+'/MLP_all_labels_rs_results_%s.csv' % timestamp, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
