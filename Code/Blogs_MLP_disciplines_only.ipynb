{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textklassifikation mit Vektorisierung in scikit-learn und MLPClassifier \n",
    "Labels sind auf Disziplinen reduziert (disciplines_only)\n",
    "\n",
    "Autorin: Maria Hartmann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # module to one-hot-encode the labels\n",
    "from sklearn.pipeline import Pipeline # assemples transormers \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # module to transform a count matrix to a normalized tf-idf representation\n",
    "from sklearn.neural_network import MLPClassifier # MultiLayerPerceptron classifier \n",
    "from sklearn.model_selection import RandomizedSearchCV # module for paramter optimization\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "np.random.seed(7) # fix random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einlesen des Trainings- und Testdatensatzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = '../Datasets/disciplines_only_trainset.csv' \n",
    "testset = '../Datasets/disciplines_only_testset.csv' \n",
    "\n",
    "trainset_csv = pd.read_csv(trainset, delimiter=';')\n",
    "X_train = trainset_csv['text'].values\n",
    "y_train = trainset_csv['classes'].values\n",
    "z_train = trainset_csv['filename'].values\n",
    "\n",
    "testset_csv = pd.read_csv(testset, delimiter=';')\n",
    "X_test = testset_csv['text'].values\n",
    "y_test = testset_csv['classes'].values\n",
    "z_test = testset_csv['filename'].values\n",
    "\n",
    "# Splitten der Labels pro Blogbeitrag\n",
    "y_train = [e.split(', ') for e in y_train]\n",
    "y_test = [e.split(', ') for e in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archivewk1_2023.txt\n",
      "['histoire et archéologie_d']\n",
      "stadtarchiv goch bestand völcker janssen niederrheinisches volksblatt gocher zeitung vom lokalteil foto lazarett wilhelm anton hospital nachdem ein weiterer verwundetentransport eingetroffen ist werden verwundete im wilhelm anton hospital gepflegt ein neuer verwundetentransport ist gestern abend wieder hier angekommen damit ist die zahl der verwundeten die im wilhelm anton hospital aufnahme gefunden haben auf gestiegen mit der einrichtung des van gulikschen hauses in der brückenstraße für die aufnahme von verwundeten ist begonnen worden und dürfte im laufe der nächsten woche als lazarett fertig eingerichtet werden\n"
     ]
    }
   ],
   "source": [
    "print(z_train[0])\n",
    "print(y_train[0])\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-hot-Kodierung der Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# k-hot-encode labels mit MultiLabelBinarizer\n",
    "label_encoder = MultiLabelBinarizer()\n",
    "encoded_y_train = label_encoder.fit_transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "print(encoded_y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "0 administration publique et développement_d\n",
      "1 arts et humanités_d\n",
      "2 bibliothéconomie_d\n",
      "3 histoire et archéologie_d\n",
      "4 langue et linguistique_d\n",
      "5 littérature_d\n",
      "6 pluridisciplinarité_d\n",
      "7 psychologie_d\n",
      "8 sciences de l'information et de la communication_d\n",
      "9 sciences de la santé et de la santé publique_d\n",
      "10 sciences politiques_d\n",
      "11 sociologie et anthropologie_d\n",
      "12 travail social et politique sociale_d\n",
      "13 éducation_d\n"
     ]
    }
   ],
   "source": [
    "print(len(label_encoder.classes_))\n",
    "for i, element in enumerate(label_encoder.classes_):\n",
    "    print(i, element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vektorisierung und Klassifikation der Daten mit scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "stopwords = open('../Preprocessing/filtered_words.txt','r', encoding='utf-8').read().splitlines()\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1),  max_features=max_features, stop_words=stopwords)\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# first try with best params for vect and tfidf from kNN classification\\ntext_clf = Pipeline([#('vect', CountVectorizer(ngram_range=(1,4), max_df=0.9, min_df=0.01)),#min_df=0.0 auf min_df=0.01 geändert\\n                     ('vect', CountVectorizer(ngram_range=(1,4), max_features=max_features)),\\n                     ('tfidf', TfidfTransformer(use_idf=True)),\\n                     ('clf', MLPClassifier(hidden_layer_sizes=(1024,512), max_iter=500, validation_fraction=0.1, early_stopping=True, verbose=True, random_state=1)),\\n                    ])\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# first try with best params for vect and tfidf from kNN classification\n",
    "text_clf = Pipeline([#('vect', CountVectorizer(ngram_range=(1,4), max_df=0.9, min_df=0.01)),#min_df=0.0 auf min_df=0.01 geändert\n",
    "                     ('vect', CountVectorizer(ngram_range=(1,4), max_features=max_features)),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                     ('clf', MLPClassifier(hidden_layer_sizes=(1024,512), max_iter=500, validation_fraction=0.1, early_stopping=True, verbose=True, random_state=1)),\n",
    "                    ])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', vectorizer),\n",
    "                     ('tfidf', tfidf_transformer),\n",
    "                     ('clf', MLPClassifier(hidden_layer_sizes=(2048, 512), tol=0.0001, early_stopping=True, validation_fraction=0.1, verbose=True, random_state=1))\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.16798568\n",
      "Validation score: 0.617681\n",
      "Iteration 2, loss = 1.34563578\n",
      "Validation score: 0.697892\n",
      "Iteration 3, loss = 0.59118765\n",
      "Validation score: 0.698478\n",
      "Iteration 4, loss = 0.26165635\n",
      "Validation score: 0.690867\n",
      "Iteration 5, loss = 0.14563018\n",
      "Validation score: 0.680328\n",
      "Iteration 6, loss = 0.11413968\n",
      "Validation score: 0.686183\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "start = time.time()\n",
    "text_clf = text_clf.fit(X_train, encoded_y_train)\n",
    "processing_time = (time.time() - start) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=['und', 'die', 'der'], strip_accents=None,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(2048, 512), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
      "       warm_start=False))], 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=['und', 'die', 'der'], strip_accents=None,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(2048, 512), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
      "       warm_start=False), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': 10000, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': ['und', 'die', 'der'], 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__activation': 'relu', 'clf__alpha': 0.0001, 'clf__batch_size': 'auto', 'clf__beta_1': 0.9, 'clf__beta_2': 0.999, 'clf__early_stopping': True, 'clf__epsilon': 1e-08, 'clf__hidden_layer_sizes': (2048, 512), 'clf__learning_rate': 'constant', 'clf__learning_rate_init': 0.001, 'clf__max_iter': 200, 'clf__momentum': 0.9, 'clf__nesterovs_momentum': True, 'clf__power_t': 0.5, 'clf__random_state': 1, 'clf__shuffle': True, 'clf__solver': 'adam', 'clf__tol': 0.0001, 'clf__validation_fraction': 0.1, 'clf__verbose': True, 'clf__warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "clf_params = text_clf.get_params()\n",
    "print(clf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predicted = text_clf.predict(X_test)\n",
    "#predicted_proba = text_clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8613517521267462\n"
     ]
    }
   ],
   "source": [
    "# precision is a measure of result relevancy\n",
    "precision = precision_score(encoded_y_test, predicted, average='samples')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8411535159603527\n"
     ]
    }
   ],
   "source": [
    "# recall is a measure of how many truly relevant results are returned\n",
    "recall = recall_score(encoded_y_test, predicted, average='samples')  \n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8348383152527381\n"
     ]
    }
   ],
   "source": [
    "# F1 score is a weighted average of the precision and recall\n",
    "f1 = f1_score(encoded_y_test, predicted, average='samples') \n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = '../MLP'\n",
    "if not os.path.exists(output):\n",
    "    os.makedirs(output)\n",
    "    \n",
    "timestamp = time.strftime('%Y-%m-%d_%H.%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# write first parameters and scores to file\\n\\n#with open(output+\\'/MLP_disciplines_only_first_params.txt\\',\"w+\", encoding=\"utf8\") as params:\\nwith open(output+\\'/MLP_disciplines_only_first_params_max_features.txt\\',\"w+\", encoding=\"utf8\") as params:\\n    params.write(\"First parameters for classification with MLP (disciplines only):\")\\n    params.write(\"\\nprocessing_time: %s\" % processing_time)\\n    for key, value in clf_params.items():\\n        params.write(\"\\n%s: %s\" % (key, value))\\n    params.write(\"\\nactivation function output layer: %s\" %  text_clf.named_steps.clf.out_activation_)    \\n    params.write(\"\\nprecision: %s\" % precision)\\n    params.write(\"\\nrecall: %s\" % recall)\\n    params.write(\"\\nf1-score: %s\" % f1)'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# write first parameters and scores to file\n",
    "\n",
    "#with open(output+'/MLP_disciplines_only_first_params.txt',\"w+\", encoding=\"utf8\") as params:\n",
    "with open(output+'/MLP_disciplines_only_first_params_max_features.txt',\"w+\", encoding=\"utf8\") as params:\n",
    "    params.write(\"First parameters for classification with MLP (disciplines only):\")\n",
    "    params.write(\"\\nprocessing_time: %s\" % processing_time)\n",
    "    for key, value in clf_params.items():\n",
    "        params.write(\"\\n%s: %s\" % (key, value))\n",
    "    params.write(\"\\nactivation function output layer: %s\" %  text_clf.named_steps.clf.out_activation_)    \n",
    "    params.write(\"\\nprecision: %s\" % precision)\n",
    "    params.write(\"\\nrecall: %s\" % recall)\n",
    "    params.write(\"\\nf1-score: %s\" % f1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write parameters and scores to file\n",
    "\n",
    "with open(output+'/MLP_disciplines_only_params.txt',\"a\", encoding=\"utf8\") as params:\n",
    "    params.write(\"\\n*********************************************************************************************\")\n",
    "    params.write(\"\\nParameters for classification with MLP (disciplines only):\")\n",
    "    params.write(\"\\n*********************************************************************************************\")\n",
    "    params.write(\"\\n%s\" % text_clf.named_steps.vect)\n",
    "    params.write(\"\\n%s\" % text_clf.named_steps.tfidf)\n",
    "    params.write(\"\\n%s\" % text_clf.named_steps.clf)\n",
    "    #for key, value in clf_params.items():\n",
    "        #params.write(\"\\n%s: %s\" % (key, value))\n",
    "    params.write(\"\\nclasses: %s\" % text_clf.named_steps.clf.n_outputs_)\n",
    "    params.write(\"\\nlayers: %s\" % text_clf.named_steps.clf.n_layers_)\n",
    "    params.write(\"\\nactivation function output layer: %s\" %  text_clf.named_steps.clf.out_activation_) \n",
    "    params.write(\"\\nepochs: %s\" % text_clf.named_steps.clf.n_iter_)\n",
    "    params.write(\"\\nprocessing time: %s\" % processing_time)\n",
    "    params.write(\"\\nSCORES:\")\n",
    "    params.write(\"\\nprecision: %s\" % precision)\n",
    "    params.write(\"\\nrecall: %s\" % recall)\n",
    "    params.write(\"\\nf1-score: %s\" % f1)\n",
    "    params.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED:\n",
      "('histoire et archéologie_d',)\n",
      "TRUE:\n",
      "['pluridisciplinarité_d', 'histoire et archéologie_d']\n"
     ]
    }
   ],
   "source": [
    "# write real labels and predictions to file\n",
    "\n",
    "inverse_prediction = label_encoder.inverse_transform(predicted)\n",
    "print('PREDICTED:')\n",
    "print(inverse_prediction[0])\n",
    "print('TRUE:')\n",
    "print(y_test[0])\n",
    "\n",
    "with open(output+'/MLP_disciplines_only_predictions.txt',\"w+\", encoding=\"utf8\") as preds:\n",
    "    preds.write(\"Predictions from classification with Multi-Layer-Perzeptron and vectorization in scikit-learn (disciplines only):\\n\\n\")\n",
    "    for ident, label, pred in zip(z_test, y_test, inverse_prediction):\n",
    "        label = sorted(label)\n",
    "        pred = sorted(pred)\n",
    "        preds.write(ident)\n",
    "        preds.write('\\n')\n",
    "        preds.write('TRUE: ')\n",
    "        for element in label:\n",
    "            preds.write('%s, ' % element)\n",
    "        preds.write('\\n')\n",
    "        preds.write('PRED: ')\n",
    "        for element in pred:\n",
    "            preds.write('%s, ' % element)\n",
    "        preds.write('\\n')\n",
    "        preds.write('\\n*********************\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speicherung der vektorisierten Textdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17080\n",
      "archivewk1.hypotheses.org/2023\n"
     ]
    }
   ],
   "source": [
    "z_train = [e.replace('.txt', '') for e in z_train]\n",
    "z_test = [e.replace('.txt', '') for e in z_test]\n",
    "ident_train = [e.replace('_', '.hypotheses.org/') for e in z_train]\n",
    "ident_test = [e.replace('_', '.hypotheses.org/') for e in z_test]\n",
    "\n",
    "print(len(ident_train))\n",
    "print(ident_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17080, 10000)\n"
     ]
    }
   ],
   "source": [
    "# vectorize textdata\n",
    "train_vect = vectorizer.transform(X_train)\n",
    "train_tfidf = tfidf_transformer.transform(train_vect)\n",
    "print(train_tfidf.shape)\n",
    "\n",
    "test_vect = vectorizer.transform(X_test)\n",
    "test_tfidf = tfidf_transformer.transform(test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<17080x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2373689 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(test_tfidf))\n",
    "train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filename, classes, textvectors in csv file\n",
    "# trainset\n",
    "# speichert vektorisierten Text\n",
    "output_file_train = 'Datasets/disciplines_only_train_scikit-learn_sparse_matrix.npz'\n",
    "scipy.sparse.save_npz('../'+output_file_train, train_tfidf)\n",
    "\n",
    "# speichert filenames und classes\n",
    "with open('../Datasets/disciplines_only_train_idents_labels.csv', 'w', newline='', encoding=\"utf-8\") as traincsv:\n",
    "    train = csv.writer(traincsv, delimiter = \";\")\n",
    "    train.writerow([\"url\", \"classes\", \"filename\"])\n",
    "    \n",
    "    for ident, labels in zip(ident_train, y_train):\n",
    "        labellist = \", \".join(labels)\n",
    "        train.writerow([ident, labellist, output_file_train])\n",
    "\n",
    "# testset\n",
    "# speichert vektorisierten Text\n",
    "output_file_test = 'Datasets/disciplines_only_test_scikit-learn_sparse_matrix.npz'\n",
    "scipy.sparse.save_npz('../'+output_file_test, test_tfidf)\n",
    "\n",
    "# speichert filenames und classes\n",
    "with open('../Datasets/disciplines_only_test_idents_labels.csv', 'w', newline='', encoding=\"utf-8\") as testcsv:\n",
    "    test = csv.writer(testcsv, delimiter = \";\")\n",
    "    test.writerow([\"url\", \"classes\", \"filename\"])\n",
    "    \n",
    "    for ident, labels in zip(ident_test, y_test):\n",
    "        labellist = \", \".join(labels)\n",
    "        test.writerow([ident, labellist, output_file_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameteroptimierung mit Rastersuche (RandomizedSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', MLPClassifier(validation_fraction=0.1, early_stopping=True, verbose=True, random_state=1)),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter tuning with RandomSearch\n",
    "stopwords = open('../Preprocessing/filtered_words.txt', 'r', encoding='utf-8').read().splitlines()\n",
    "rs_parameters = {'vect__ngram_range': [(1,1),(1,2),(1,3),(1,4)], \n",
    "                 #'vect__max_df' : (0.7, 0.8, 0.85, 0.9, 0.95), #1.0 \n",
    "                 #'vect__min_df' : (0.01, 0.025, 0.05, 0.075, 0.1, 0.2), #0.0\n",
    "                 'vect__max_features': (100000,7500,50000,25000,10000,5000,2500,1000,500,300,100), \n",
    "                 'tfidf__use_idf': (True, False),\n",
    "                 'clf__hidden_layer_sizes': ((2048,1024),(2048,512),(1024,512),(512,128),(4096,1024),(4096,512),(2048,1024,512),(1024,512,128))\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "[CV] vect__ngram_range=(1, 3), vect__max_features=7500, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 5.33313601\n",
      "Validation score: 0.345794\n",
      "Iteration 2, loss = 3.31710757\n",
      "Validation score: 0.528037\n",
      "Iteration 3, loss = 2.52894313\n",
      "Validation score: 0.542056\n",
      "Iteration 4, loss = 1.96432080\n",
      "Validation score: 0.593458\n",
      "Iteration 5, loss = 1.49209314\n",
      "Validation score: 0.565421\n",
      "Iteration 6, loss = 1.07256055\n",
      "Validation score: 0.556075\n",
      "Iteration 7, loss = 0.72331682\n",
      "Validation score: 0.570093\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 3), vect__max_features=7500, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024), score=0.5351123595505618, total= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] vect__ngram_range=(1, 3), vect__max_features=7500, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 5.50608338\n",
      "Validation score: 0.214953\n",
      "Iteration 2, loss = 3.60370046\n",
      "Validation score: 0.434579\n",
      "Iteration 3, loss = 2.76881989\n",
      "Validation score: 0.462617\n",
      "Iteration 4, loss = 2.26087348\n",
      "Validation score: 0.500000\n",
      "Iteration 5, loss = 1.76660575\n",
      "Validation score: 0.500000\n",
      "Iteration 6, loss = 1.31323577\n",
      "Validation score: 0.546729\n",
      "Iteration 7, loss = 0.90371415\n",
      "Validation score: 0.537383\n",
      "Iteration 8, loss = 0.56291376\n",
      "Validation score: 0.560748\n",
      "Iteration 9, loss = 0.32241808\n",
      "Validation score: 0.570093\n",
      "Iteration 10, loss = 0.18187342\n",
      "Validation score: 0.546729\n",
      "Iteration 11, loss = 0.10825919\n",
      "Validation score: 0.551402\n",
      "Iteration 12, loss = 0.06586573\n",
      "Validation score: 0.560748\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 3), vect__max_features=7500, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024), score=0.5723653395784544, total= 4.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  7.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] vect__ngram_range=(1, 3), vect__max_features=2500, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 5.29870823\n",
      "Validation score: 0.364486\n",
      "Iteration 2, loss = 3.31106911\n",
      "Validation score: 0.467290\n",
      "Iteration 3, loss = 2.73045922\n",
      "Validation score: 0.467290\n",
      "Iteration 4, loss = 2.34032261\n",
      "Validation score: 0.504673\n",
      "Iteration 5, loss = 2.00753216\n",
      "Validation score: 0.528037\n",
      "Iteration 6, loss = 1.68956904\n",
      "Validation score: 0.570093\n",
      "Iteration 7, loss = 1.37429917\n",
      "Validation score: 0.579439\n",
      "Iteration 8, loss = 1.09930940\n",
      "Validation score: 0.537383\n",
      "Iteration 9, loss = 0.83960148\n",
      "Validation score: 0.556075\n",
      "Iteration 10, loss = 0.57336039\n",
      "Validation score: 0.584112\n",
      "Iteration 11, loss = 0.39075777\n",
      "Validation score: 0.574766\n",
      "Iteration 12, loss = 0.26591486\n",
      "Validation score: 0.584112\n",
      "Iteration 13, loss = 0.18673769\n",
      "Validation score: 0.579439\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 3), vect__max_features=2500, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024), score=0.5074906367041199, total= 2.4min\n",
      "[CV] vect__ngram_range=(1, 3), vect__max_features=2500, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  9.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 5.35328429\n",
      "Validation score: 0.299065\n",
      "Iteration 2, loss = 3.54180998\n",
      "Validation score: 0.373832\n",
      "Iteration 3, loss = 2.90447722\n",
      "Validation score: 0.420561\n",
      "Iteration 4, loss = 2.49454053\n",
      "Validation score: 0.439252\n",
      "Iteration 5, loss = 2.13254880\n",
      "Validation score: 0.453271\n",
      "Iteration 6, loss = 1.78016769\n",
      "Validation score: 0.476636\n",
      "Iteration 7, loss = 1.43439968\n",
      "Validation score: 0.462617\n",
      "Iteration 8, loss = 1.08763751\n",
      "Validation score: 0.500000\n",
      "Iteration 9, loss = 0.77350251\n",
      "Validation score: 0.495327\n",
      "Iteration 10, loss = 0.53524933\n",
      "Validation score: 0.485981\n",
      "Iteration 11, loss = 0.36654226\n",
      "Validation score: 0.485981\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 3), vect__max_features=2500, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024), score=0.5377049180327869, total= 2.1min\n",
      "[CV] vect__ngram_range=(1, 4), vect__max_features=5000, tfidf__use_idf=False, clf__hidden_layer_sizes=(512, 128) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 11.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 8.83581379\n",
      "Validation score: 0.383178\n",
      "Iteration 2, loss = 5.41196698\n",
      "Validation score: 0.271028\n",
      "Iteration 3, loss = 3.42851509\n",
      "Validation score: 0.406542\n",
      "Iteration 4, loss = 3.26281754\n",
      "Validation score: 0.514019\n",
      "Iteration 5, loss = 3.02271073\n",
      "Validation score: 0.514019\n",
      "Iteration 6, loss = 2.81014140\n",
      "Validation score: 0.532710\n",
      "Iteration 7, loss = 2.59534352\n",
      "Validation score: 0.518692\n",
      "Iteration 8, loss = 2.38309032\n",
      "Validation score: 0.504673\n",
      "Iteration 9, loss = 2.18400242\n",
      "Validation score: 0.537383\n",
      "Iteration 10, loss = 2.00276074\n",
      "Validation score: 0.532710\n",
      "Iteration 11, loss = 1.83457190\n",
      "Validation score: 0.542056\n",
      "Iteration 12, loss = 1.67196940\n",
      "Validation score: 0.542056\n",
      "Iteration 13, loss = 1.52242310\n",
      "Validation score: 0.532710\n",
      "Iteration 14, loss = 1.37428019\n",
      "Validation score: 0.537383\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 4), vect__max_features=5000, tfidf__use_idf=False, clf__hidden_layer_sizes=(512, 128), score=0.5131086142322098, total=  36.4s\n",
      "[CV] vect__ngram_range=(1, 4), vect__max_features=5000, tfidf__use_idf=False, clf__hidden_layer_sizes=(512, 128) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 12.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 8.81824255\n",
      "Validation score: 0.313084\n",
      "Iteration 2, loss = 5.44619390\n",
      "Validation score: 0.219626\n",
      "Iteration 3, loss = 3.68119501\n",
      "Validation score: 0.350467\n",
      "Iteration 4, loss = 3.49287820\n",
      "Validation score: 0.415888\n",
      "Iteration 5, loss = 3.23791914\n",
      "Validation score: 0.481308\n",
      "Iteration 6, loss = 3.02521719\n",
      "Validation score: 0.462617\n",
      "Iteration 7, loss = 2.80466927\n",
      "Validation score: 0.471963\n",
      "Iteration 8, loss = 2.58664498\n",
      "Validation score: 0.485981\n",
      "Iteration 9, loss = 2.37982389\n",
      "Validation score: 0.509346\n",
      "Iteration 10, loss = 2.18614167\n",
      "Validation score: 0.509346\n",
      "Iteration 11, loss = 2.00643994\n",
      "Validation score: 0.514019\n",
      "Iteration 12, loss = 1.82585279\n",
      "Validation score: 0.523364\n",
      "Iteration 13, loss = 1.65728088\n",
      "Validation score: 0.509346\n",
      "Iteration 14, loss = 1.49481943\n",
      "Validation score: 0.514019\n",
      "Iteration 15, loss = 1.33788840\n",
      "Validation score: 0.514019\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 4), vect__max_features=5000, tfidf__use_idf=False, clf__hidden_layer_sizes=(512, 128), score=0.5508196721311476, total=  37.1s\n",
      "[CV] vect__ngram_range=(1, 2), vect__max_features=25000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 13.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 7.79836566\n",
      "Validation score: 0.238318\n",
      "Iteration 2, loss = 3.68475211\n",
      "Validation score: 0.439252\n",
      "Iteration 3, loss = 3.19411326\n",
      "Validation score: 0.574766\n",
      "Iteration 4, loss = 2.50451381\n",
      "Validation score: 0.574766\n",
      "Iteration 5, loss = 1.86278211\n",
      "Validation score: 0.612150\n",
      "Iteration 6, loss = 1.35187616\n",
      "Validation score: 0.612150\n",
      "Iteration 7, loss = 0.93871513\n",
      "Validation score: 0.635514\n",
      "Iteration 8, loss = 0.62110331\n",
      "Validation score: 0.635514\n",
      "Iteration 9, loss = 0.37315331\n",
      "Validation score: 0.640187\n",
      "Iteration 10, loss = 0.21470657\n",
      "Validation score: 0.644860\n",
      "Iteration 11, loss = 0.11906382\n",
      "Validation score: 0.644860\n",
      "Iteration 12, loss = 0.07151888\n",
      "Validation score: 0.640187\n",
      "Iteration 13, loss = 0.04545086\n",
      "Validation score: 0.630841\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 2), vect__max_features=25000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512), score=0.5688202247191011, total= 3.4min\n",
      "[CV] vect__ngram_range=(1, 2), vect__max_features=25000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 16.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 7.84921997\n",
      "Validation score: 0.261682\n",
      "Iteration 2, loss = 3.90872060\n",
      "Validation score: 0.457944\n",
      "Iteration 3, loss = 3.34325442\n",
      "Validation score: 0.415888\n",
      "Iteration 4, loss = 2.65124140\n",
      "Validation score: 0.490654\n",
      "Iteration 5, loss = 1.96908431\n",
      "Validation score: 0.490654\n",
      "Iteration 6, loss = 1.40559253\n",
      "Validation score: 0.528037\n",
      "Iteration 7, loss = 0.93430085\n",
      "Validation score: 0.528037\n",
      "Iteration 8, loss = 0.57339464\n",
      "Validation score: 0.523364\n",
      "Iteration 9, loss = 0.33957077\n",
      "Validation score: 0.537383\n",
      "Iteration 10, loss = 0.19413274\n",
      "Validation score: 0.537383\n",
      "Iteration 11, loss = 0.11308101\n",
      "Validation score: 0.546729\n",
      "Iteration 12, loss = 0.06876573\n",
      "Validation score: 0.546729\n",
      "Iteration 13, loss = 0.04335390\n",
      "Validation score: 0.546729\n",
      "Iteration 14, loss = 0.02932721\n",
      "Validation score: 0.546729\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 2), vect__max_features=25000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512), score=0.5920374707259953, total= 3.7min\n",
      "[CV] vect__ngram_range=(1, 2), vect__max_features=50000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 20.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 7.18809883\n",
      "Validation score: 0.252336\n",
      "Iteration 2, loss = 3.59460795\n",
      "Validation score: 0.481308\n",
      "Iteration 3, loss = 2.91145859\n",
      "Validation score: 0.560748\n",
      "Iteration 4, loss = 2.12055872\n",
      "Validation score: 0.612150\n",
      "Iteration 5, loss = 1.46224997\n",
      "Validation score: 0.630841\n",
      "Iteration 6, loss = 0.98694384\n",
      "Validation score: 0.621495\n",
      "Iteration 7, loss = 0.64894409\n",
      "Validation score: 0.640187\n",
      "Iteration 8, loss = 0.38340985\n",
      "Validation score: 0.630841\n",
      "Iteration 9, loss = 0.20850616\n",
      "Validation score: 0.644860\n",
      "Iteration 10, loss = 0.10819835\n",
      "Validation score: 0.640187\n",
      "Iteration 11, loss = 0.06416286\n",
      "Validation score: 0.635514\n",
      "Iteration 12, loss = 0.03815432\n",
      "Validation score: 0.649533\n",
      "Iteration 13, loss = 0.02924462\n",
      "Validation score: 0.640187\n",
      "Iteration 14, loss = 0.02043942\n",
      "Validation score: 0.640187\n",
      "Iteration 15, loss = 0.01713707\n",
      "Validation score: 0.654206\n",
      "Iteration 16, loss = 0.01459405\n",
      "Validation score: 0.635514\n",
      "Iteration 17, loss = 0.01287035\n",
      "Validation score: 0.640187\n",
      "Iteration 18, loss = 0.01093768\n",
      "Validation score: 0.635514\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 2), vect__max_features=50000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512), score=0.5641385767790262, total= 9.0min\n",
      "[CV] vect__ngram_range=(1, 2), vect__max_features=50000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 29.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 7.29522349\n",
      "Validation score: 0.247664\n",
      "Iteration 2, loss = 3.84663528\n",
      "Validation score: 0.434579\n",
      "Iteration 3, loss = 3.07404664\n",
      "Validation score: 0.537383\n",
      "Iteration 4, loss = 2.26939009\n",
      "Validation score: 0.556075\n",
      "Iteration 5, loss = 1.54583149\n",
      "Validation score: 0.574766\n",
      "Iteration 6, loss = 1.00653019\n",
      "Validation score: 0.556075\n",
      "Iteration 7, loss = 0.61232715\n",
      "Validation score: 0.593458\n",
      "Iteration 8, loss = 0.32599035\n",
      "Validation score: 0.593458\n",
      "Iteration 9, loss = 0.17087228\n",
      "Validation score: 0.612150\n",
      "Iteration 10, loss = 0.09177163\n",
      "Validation score: 0.598131\n",
      "Iteration 11, loss = 0.05337019\n",
      "Validation score: 0.588785\n",
      "Iteration 12, loss = 0.03552243\n",
      "Validation score: 0.588785\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 2), vect__max_features=50000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512), score=0.6023419203747072, total= 6.0min\n",
      "[CV] vect__ngram_range=(1, 4), vect__max_features=10000, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 5.31718774\n",
      "Validation score: 0.415888\n",
      "Iteration 2, loss = 3.14359992\n",
      "Validation score: 0.509346\n",
      "Iteration 3, loss = 2.15770874\n",
      "Validation score: 0.518692\n",
      "Iteration 4, loss = 1.46613686\n",
      "Validation score: 0.542056\n",
      "Iteration 5, loss = 0.92240288\n",
      "Validation score: 0.537383\n",
      "Iteration 6, loss = 0.52673311\n",
      "Validation score: 0.556075\n",
      "Iteration 7, loss = 0.26623089\n",
      "Validation score: 0.556075\n",
      "Iteration 8, loss = 0.12308643\n",
      "Validation score: 0.532710\n",
      "Iteration 9, loss = 0.06059888\n",
      "Validation score: 0.551402\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 4), vect__max_features=10000, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024), score=0.5529026217228464, total= 4.5min\n",
      "[CV] vect__ngram_range=(1, 4), vect__max_features=10000, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 5.51743787\n",
      "Validation score: 0.074766\n",
      "Iteration 2, loss = 3.27106034\n",
      "Validation score: 0.448598\n",
      "Iteration 3, loss = 2.29489909\n",
      "Validation score: 0.518692\n",
      "Iteration 4, loss = 1.54388614\n",
      "Validation score: 0.565421\n",
      "Iteration 5, loss = 0.94796013\n",
      "Validation score: 0.579439\n",
      "Iteration 6, loss = 0.51178476\n",
      "Validation score: 0.579439\n",
      "Iteration 7, loss = 0.24301492\n",
      "Validation score: 0.579439\n",
      "Iteration 8, loss = 0.11059822\n",
      "Validation score: 0.584112\n",
      "Iteration 9, loss = 0.05244268\n",
      "Validation score: 0.579439\n",
      "Iteration 10, loss = 0.02973431\n",
      "Validation score: 0.588785\n",
      "Iteration 11, loss = 0.01990680\n",
      "Validation score: 0.588785\n",
      "Iteration 12, loss = 0.01502260\n",
      "Validation score: 0.584112\n",
      "Iteration 13, loss = 0.01264728\n",
      "Validation score: 0.584112\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 4), vect__max_features=10000, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024), score=0.5831381733021077, total= 6.3min\n",
      "[CV] vect__ngram_range=(1, 1), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(1024, 512, 128) \n",
      "Iteration 1, loss = 7.21897659\n",
      "Validation score: 0.383178\n",
      "Iteration 2, loss = 3.61985637\n",
      "Validation score: 0.383178\n",
      "Iteration 3, loss = 3.38373429\n",
      "Validation score: 0.294393\n",
      "Iteration 4, loss = 3.12875867\n",
      "Validation score: 0.392523\n",
      "Iteration 5, loss = 2.99659949\n",
      "Validation score: 0.490654\n",
      "Iteration 6, loss = 2.83861087\n",
      "Validation score: 0.462617\n",
      "Iteration 7, loss = 2.70147104\n",
      "Validation score: 0.453271\n",
      "Iteration 8, loss = 2.59006224\n",
      "Validation score: 0.504673\n",
      "Iteration 9, loss = 2.47651830\n",
      "Validation score: 0.518692\n",
      "Iteration 10, loss = 2.36563013\n",
      "Validation score: 0.518692\n",
      "Iteration 11, loss = 2.27077775\n",
      "Validation score: 0.532710\n",
      "Iteration 12, loss = 2.17209960\n",
      "Validation score: 0.528037\n",
      "Iteration 13, loss = 2.06397469\n",
      "Validation score: 0.532710\n",
      "Iteration 14, loss = 1.96124620\n",
      "Validation score: 0.504673\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 1), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(1024, 512, 128), score=0.4756554307116105, total=  13.7s\n",
      "[CV] vect__ngram_range=(1, 1), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(1024, 512, 128) \n",
      "Iteration 1, loss = 7.29316419\n",
      "Validation score: 0.327103\n",
      "Iteration 2, loss = 3.84724260\n",
      "Validation score: 0.317757\n",
      "Iteration 3, loss = 3.53727749\n",
      "Validation score: 0.434579\n",
      "Iteration 4, loss = 3.31200718\n",
      "Validation score: 0.439252\n",
      "Iteration 5, loss = 3.17413633\n",
      "Validation score: 0.453271\n",
      "Iteration 6, loss = 3.00481946\n",
      "Validation score: 0.443925\n",
      "Iteration 7, loss = 2.85960479\n",
      "Validation score: 0.443925\n",
      "Iteration 8, loss = 2.74604139\n",
      "Validation score: 0.457944\n",
      "Iteration 9, loss = 2.62909982\n",
      "Validation score: 0.467290\n",
      "Iteration 10, loss = 2.51810907\n",
      "Validation score: 0.481308\n",
      "Iteration 11, loss = 2.40403007\n",
      "Validation score: 0.476636\n",
      "Iteration 12, loss = 2.28638707\n",
      "Validation score: 0.481308\n",
      "Iteration 13, loss = 2.17786593\n",
      "Validation score: 0.467290\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 1), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(1024, 512, 128), score=0.5030444964871195, total=  12.6s\n",
      "[CV] vect__ngram_range=(1, 3), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(2048, 1024, 512) \n",
      "Iteration 1, loss = 5.68246644\n",
      "Validation score: 0.219626\n",
      "Iteration 2, loss = 3.46190872\n",
      "Validation score: 0.303738\n",
      "Iteration 3, loss = 3.08016153\n",
      "Validation score: 0.439252\n",
      "Iteration 4, loss = 2.84607558\n",
      "Validation score: 0.434579\n",
      "Iteration 5, loss = 2.66573895\n",
      "Validation score: 0.453271\n",
      "Iteration 6, loss = 2.51939471\n",
      "Validation score: 0.476636\n",
      "Iteration 7, loss = 2.37644700\n",
      "Validation score: 0.471963\n",
      "Iteration 8, loss = 2.22969903\n",
      "Validation score: 0.476636\n",
      "Iteration 9, loss = 2.10421665\n",
      "Validation score: 0.476636\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 3), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(2048, 1024, 512), score=0.4648876404494382, total=  33.9s\n",
      "[CV] vect__ngram_range=(1, 3), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(2048, 1024, 512) \n",
      "Iteration 1, loss = 5.81884041\n",
      "Validation score: 0.046729\n",
      "Iteration 2, loss = 3.65951575\n",
      "Validation score: 0.285047\n",
      "Iteration 3, loss = 3.26865250\n",
      "Validation score: 0.415888\n",
      "Iteration 4, loss = 3.05094033\n",
      "Validation score: 0.429907\n",
      "Iteration 5, loss = 2.88358252\n",
      "Validation score: 0.448598\n",
      "Iteration 6, loss = 2.72175376\n",
      "Validation score: 0.448598\n",
      "Iteration 7, loss = 2.58483410\n",
      "Validation score: 0.453271\n",
      "Iteration 8, loss = 2.43338499\n",
      "Validation score: 0.453271\n",
      "Iteration 9, loss = 2.29045785\n",
      "Validation score: 0.443925\n",
      "Iteration 10, loss = 2.11573420\n",
      "Validation score: 0.448598\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 3), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(2048, 1024, 512), score=0.49789227166276345, total=  36.1s\n",
      "[CV] vect__ngram_range=(1, 1), vect__max_features=50000, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 5.72162744\n",
      "Validation score: 0.406542\n",
      "Iteration 2, loss = 3.19445422\n",
      "Validation score: 0.504673\n",
      "Iteration 3, loss = 1.95358208\n",
      "Validation score: 0.523364\n",
      "Iteration 4, loss = 1.12490043\n",
      "Validation score: 0.528037\n",
      "Iteration 5, loss = 0.65061001\n",
      "Validation score: 0.570093\n",
      "Iteration 6, loss = 0.33671971\n",
      "Validation score: 0.556075\n",
      "Iteration 7, loss = 0.13355506\n",
      "Validation score: 0.551402\n",
      "Iteration 8, loss = 0.05292252\n",
      "Validation score: 0.560748\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 1), vect__max_features=50000, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024), score=0.5575842696629213, total=15.7min\n",
      "[CV] vect__ngram_range=(1, 1), vect__max_features=50000, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 5.90102152\n",
      "Validation score: 0.415888\n",
      "Iteration 2, loss = 3.49367292\n",
      "Validation score: 0.556075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 2.13359146\n",
      "Validation score: 0.621495\n",
      "Iteration 4, loss = 1.25452738\n",
      "Validation score: 0.630841\n",
      "Iteration 5, loss = 0.68386318\n",
      "Validation score: 0.649533\n",
      "Iteration 6, loss = 0.30442466\n",
      "Validation score: 0.640187\n",
      "Iteration 7, loss = 0.11746369\n",
      "Validation score: 0.649533\n",
      "Iteration 8, loss = 0.05038857\n",
      "Validation score: 0.621495\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 1), vect__max_features=50000, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024), score=0.5981264637002341, total=15.9min\n",
      "[CV] vect__ngram_range=(1, 2), vect__max_features=2500, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 5.29970598\n",
      "Validation score: 0.467290\n",
      "Iteration 2, loss = 3.21261873\n",
      "Validation score: 0.467290\n",
      "Iteration 3, loss = 2.53431133\n",
      "Validation score: 0.500000\n",
      "Iteration 4, loss = 2.06359254\n",
      "Validation score: 0.542056\n",
      "Iteration 5, loss = 1.63052865\n",
      "Validation score: 0.565421\n",
      "Iteration 6, loss = 1.23047430\n",
      "Validation score: 0.602804\n",
      "Iteration 7, loss = 0.85839823\n",
      "Validation score: 0.612150\n",
      "Iteration 8, loss = 0.55858659\n",
      "Validation score: 0.598131\n",
      "Iteration 9, loss = 0.34151424\n",
      "Validation score: 0.602804\n",
      "Iteration 10, loss = 0.20053239\n",
      "Validation score: 0.593458\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 2), vect__max_features=2500, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024), score=0.5280898876404494, total= 1.9min\n",
      "[CV] vect__ngram_range=(1, 2), vect__max_features=2500, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 5.34760477\n",
      "Validation score: 0.429907\n",
      "Iteration 2, loss = 3.42547301\n",
      "Validation score: 0.406542\n",
      "Iteration 3, loss = 2.66216549\n",
      "Validation score: 0.448598\n",
      "Iteration 4, loss = 2.15113966\n",
      "Validation score: 0.453271\n",
      "Iteration 5, loss = 1.67134765\n",
      "Validation score: 0.485981\n",
      "Iteration 6, loss = 1.22218398\n",
      "Validation score: 0.481308\n",
      "Iteration 7, loss = 0.81552935\n",
      "Validation score: 0.485981\n",
      "Iteration 8, loss = 0.49866164\n",
      "Validation score: 0.495327\n",
      "Iteration 9, loss = 0.29110853\n",
      "Validation score: 0.495327\n",
      "Iteration 10, loss = 0.17536553\n",
      "Validation score: 0.476636\n",
      "Iteration 11, loss = 0.11252287\n",
      "Validation score: 0.462617\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 2), vect__max_features=2500, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024), score=0.5545667447306791, total= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 83.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 5.51267340\n",
      "Validation score: 0.511682\n",
      "Iteration 2, loss = 2.96626409\n",
      "Validation score: 0.530374\n",
      "Iteration 3, loss = 1.91109901\n",
      "Validation score: 0.602804\n",
      "Iteration 4, loss = 1.10886245\n",
      "Validation score: 0.621495\n",
      "Iteration 5, loss = 0.53387495\n",
      "Validation score: 0.642523\n",
      "Iteration 6, loss = 0.21799897\n",
      "Validation score: 0.647196\n",
      "Iteration 7, loss = 0.09365365\n",
      "Validation score: 0.651869\n",
      "Iteration 8, loss = 0.05084629\n",
      "Validation score: 0.651869\n",
      "Iteration 9, loss = 0.03378924\n",
      "Validation score: 0.642523\n",
      "Iteration 10, loss = 0.02722593\n",
      "Validation score: 0.647196\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "rs_clf = RandomizedSearchCV(clf, rs_parameters, cv=2, n_iter=10, n_jobs=1, verbose=10, random_state=1)\n",
    "start = time.time()\n",
    "rs_clf = rs_clf.fit(X_train, encoded_y_train)\n",
    "rs_processing_time = (time.time() - start) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5832357761648326\n"
     ]
    }
   ],
   "source": [
    "best_score = rs_clf.best_score_\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vect__ngram_range': (1, 2), 'vect__max_features': 50000, 'tfidf__use_idf': True, 'clf__hidden_layer_sizes': (1024, 512)}\n"
     ]
    }
   ],
   "source": [
    "best_params = rs_clf.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cv': 2, 'error_score': 'raise', 'estimator__memory': None, 'estimator__steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
      "       warm_start=False))], 'estimator__vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'estimator__tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'estimator__clf': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
      "       warm_start=False), 'estimator__vect__analyzer': 'word', 'estimator__vect__binary': False, 'estimator__vect__decode_error': 'strict', 'estimator__vect__dtype': <class 'numpy.int64'>, 'estimator__vect__encoding': 'utf-8', 'estimator__vect__input': 'content', 'estimator__vect__lowercase': True, 'estimator__vect__max_df': 1.0, 'estimator__vect__max_features': None, 'estimator__vect__min_df': 1, 'estimator__vect__ngram_range': (1, 1), 'estimator__vect__preprocessor': None, 'estimator__vect__stop_words': None, 'estimator__vect__strip_accents': None, 'estimator__vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'estimator__vect__tokenizer': None, 'estimator__vect__vocabulary': None, 'estimator__tfidf__norm': 'l2', 'estimator__tfidf__smooth_idf': True, 'estimator__tfidf__sublinear_tf': False, 'estimator__tfidf__use_idf': True, 'estimator__clf__activation': 'relu', 'estimator__clf__alpha': 0.0001, 'estimator__clf__batch_size': 'auto', 'estimator__clf__beta_1': 0.9, 'estimator__clf__beta_2': 0.999, 'estimator__clf__early_stopping': True, 'estimator__clf__epsilon': 1e-08, 'estimator__clf__hidden_layer_sizes': (100,), 'estimator__clf__learning_rate': 'constant', 'estimator__clf__learning_rate_init': 0.001, 'estimator__clf__max_iter': 200, 'estimator__clf__momentum': 0.9, 'estimator__clf__nesterovs_momentum': True, 'estimator__clf__power_t': 0.5, 'estimator__clf__random_state': 1, 'estimator__clf__shuffle': True, 'estimator__clf__solver': 'adam', 'estimator__clf__tol': 0.0001, 'estimator__clf__validation_fraction': 0.1, 'estimator__clf__verbose': True, 'estimator__clf__warm_start': False, 'estimator': Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip...\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
      "       warm_start=False))]), 'fit_params': None, 'iid': True, 'n_iter': 10, 'n_jobs': 1, 'param_distributions': {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)], 'vect__max_features': (100000, 7500, 50000, 25000, 10000, 5000, 2500, 1000, 500, 300, 100), 'tfidf__use_idf': (True, False), 'clf__hidden_layer_sizes': ((2048, 1024), (2048, 512), (1024, 512), (512, 128), (4096, 1024), (4096, 512), (2048, 1024, 512), (1024, 512, 128))}, 'pre_dispatch': '2*n_jobs', 'random_state': 1, 'refit': True, 'return_train_score': 'warn', 'scoring': None, 'verbose': 10}\n"
     ]
    }
   ],
   "source": [
    "rs_clf_params = rs_clf.get_params()\n",
    "print(rs_clf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict \n",
    "rs_predicted = rs_clf.predict(X_test)\n",
    "#print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8365339578454333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# precision is a measure of result relevancy\n",
    "rs_precision = precision_score(encoded_y_test, rs_predicted, average='samples')\n",
    "print(rs_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7761036299765808\n"
     ]
    }
   ],
   "source": [
    "# recall is a measure of how many truly relevant results are returned\n",
    "rs_recall = recall_score(encoded_y_test, rs_predicted, average='samples')  \n",
    "print(rs_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7852556596409056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# F1 score is a weighted average of the precision and recall\n",
    "rs_f1 = f1_score(encoded_y_test, rs_predicted, average='samples') \n",
    "print(rs_f1)\n",
    "#49,46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.07      0.13       514\n",
      "          1       0.77      0.62      0.69      3890\n",
      "          2       0.87      0.11      0.20      1001\n",
      "          3       0.89      0.96      0.92     13175\n",
      "          4       1.00      0.16      0.28       331\n",
      "          5       1.00      0.02      0.04       163\n",
      "          6       0.87      0.81      0.84      8507\n",
      "          7       0.93      0.19      0.32       202\n",
      "          8       0.88      0.12      0.21      1001\n",
      "          9       1.00      0.14      0.25       169\n",
      "         10       0.92      0.07      0.13       514\n",
      "         11       0.96      0.29      0.44      1153\n",
      "         12       0.99      0.41      0.58       442\n",
      "         13       0.96      0.34      0.50       809\n",
      "\n",
      "avg / total       0.88      0.73      0.75     31871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(encoded_y_test, rs_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ergebnisse in Dateien speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = '../MLP'\n",
    "if not os.path.exists(output):\n",
    "    os.makedirs(output)\n",
    "    \n",
    "timestamp = time.strftime('%Y-%m-%d_%H.%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED:\n",
      "('histoire et archéologie_d',)\n",
      "TRUE:\n",
      "['histoire et archéologie_d']\n"
     ]
    }
   ],
   "source": [
    "# write real labels and predictions to file\n",
    "\n",
    "inverse_prediction = label_encoder.inverse_transform(rs_predicted)\n",
    "print('PREDICTED:')\n",
    "print(inverse_prediction[0])\n",
    "print('TRUE:')\n",
    "print(y_test[0])\n",
    "\n",
    "with open(output+'/MLP_disciplines_only_rs_predictions_%s.txt' % timestamp,\"w+\", encoding=\"utf8\") as preds:\n",
    "    preds.write(\"Predictions from classification with Multi-Layer-Perzeptron and vectorization in scikit-learn (disciplines only):\\n\\n\")\n",
    "    for ident, label, pred in zip(z_test, y_test, inverse_prediction):\n",
    "        label = sorted(label)\n",
    "        pred = sorted(pred)\n",
    "        preds.write(ident)\n",
    "        preds.write('\\n')\n",
    "        preds.write('TRUE: ')\n",
    "        for element in label:\n",
    "            preds.write('%s, ' % element)\n",
    "        preds.write('\\n')\n",
    "        preds.write('PRED: ')\n",
    "        for element in pred:\n",
    "            preds.write('%s, ' % element)\n",
    "        preds.write('\\n')\n",
    "        preds.write('\\n*********************\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write parameters and scores to file\n",
    "\n",
    "with open(output+'/MLP_disciplines_only_rs_params_%s.txt' % timestamp,\"w+\", encoding=\"utf8\") as params:\n",
    "    params.write(\"Parameters for classification with Multi-Layer-Perceptron and vectorization in scikit-learn from randomized search (disciplines only):\")\n",
    "    params.write(\"\\nprocessing_time: %s\" % rs_processing_time)\n",
    "    params.write(\"\\nparams:\")\n",
    "    for key, value in rs_clf_params.items():\n",
    "        params.write(\"\\n%s: %s\" % (key, value))\n",
    "    params.write(\"\\nbest params:\")\n",
    "    for key, value in best_params.items():\n",
    "        params.write(\"\\n%s: %s\" % (key, value))\n",
    "    params.write(\"\\nbest_score: %s\" % best_score)\n",
    "    params.write(\"\\nprecision: %s\" % rs_precision)\n",
    "    params.write(\"\\nrecall: %s\" % rs_recall)\n",
    "    params.write(\"\\nf1-score: %s\" % rs_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0     213.751160     62.983465         2.578432        0.312540   \n",
      "1     133.250301     11.063823         2.422165        0.125015   \n",
      "2      34.761969      0.257843         2.101830        0.070339   \n",
      "3     211.533094      6.914888         1.492365        0.085948   \n",
      "4     447.410220     88.869675         1.672081        0.109373   \n",
      "5     320.895678     53.615424         3.476956        0.117200   \n",
      "6      12.595193      0.562547         0.617239        0.023423   \n",
      "7      33.308454      1.023553         1.750224        0.078108   \n",
      "8     946.700818      6.047548         1.937708        0.015638   \n",
      "9     114.841112      4.500508         2.070546        0.054694   \n",
      "\n",
      "  param_vect__ngram_range param_vect__max_features param_tfidf__use_idf  \\\n",
      "0                  (1, 3)                     7500                False   \n",
      "1                  (1, 3)                     2500                False   \n",
      "2                  (1, 4)                     5000                False   \n",
      "3                  (1, 2)                    25000                 True   \n",
      "4                  (1, 2)                    50000                 True   \n",
      "5                  (1, 4)                    10000                 True   \n",
      "6                  (1, 1)                      500                False   \n",
      "7                  (1, 3)                      500                False   \n",
      "8                  (1, 1)                    50000                 True   \n",
      "9                  (1, 2)                     2500                 True   \n",
      "\n",
      "  param_clf__hidden_layer_sizes  \\\n",
      "0                  (4096, 1024)   \n",
      "1                  (4096, 1024)   \n",
      "2                    (512, 128)   \n",
      "3                   (1024, 512)   \n",
      "4                   (1024, 512)   \n",
      "5                  (4096, 1024)   \n",
      "6              (1024, 512, 128)   \n",
      "7             (2048, 1024, 512)   \n",
      "8                  (4096, 1024)   \n",
      "9                  (4096, 1024)   \n",
      "\n",
      "                                              params  split0_test_score  \\\n",
      "0  {'vect__ngram_range': (1, 3), 'vect__max_featu...           0.535112   \n",
      "1  {'vect__ngram_range': (1, 3), 'vect__max_featu...           0.507491   \n",
      "2  {'vect__ngram_range': (1, 4), 'vect__max_featu...           0.513109   \n",
      "3  {'vect__ngram_range': (1, 2), 'vect__max_featu...           0.568820   \n",
      "4  {'vect__ngram_range': (1, 2), 'vect__max_featu...           0.564139   \n",
      "5  {'vect__ngram_range': (1, 4), 'vect__max_featu...           0.552903   \n",
      "6  {'vect__ngram_range': (1, 1), 'vect__max_featu...           0.475655   \n",
      "7  {'vect__ngram_range': (1, 3), 'vect__max_featu...           0.464888   \n",
      "8  {'vect__ngram_range': (1, 1), 'vect__max_featu...           0.557584   \n",
      "9  {'vect__ngram_range': (1, 2), 'vect__max_featu...           0.528090   \n",
      "\n",
      "   split1_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
      "0           0.572365         0.553734        0.018626                5   \n",
      "1           0.537705         0.522594        0.015107                8   \n",
      "2           0.550820         0.531960        0.018856                7   \n",
      "3           0.592037         0.580426        0.011609                2   \n",
      "4           0.602342         0.583236        0.019102                1   \n",
      "5           0.583138         0.568017        0.015118                4   \n",
      "6           0.503044         0.489347        0.013695                9   \n",
      "7           0.497892         0.481386        0.016502               10   \n",
      "8           0.598126         0.577851        0.020271                3   \n",
      "9           0.554567         0.541325        0.013238                6   \n",
      "\n",
      "   split0_train_score  split1_train_score  mean_train_score  std_train_score  \n",
      "0            0.667916            0.919007          0.793462         0.125546  \n",
      "1            0.879625            0.784644          0.832135         0.047491  \n",
      "2            0.647307            0.642322          0.644814         0.002492  \n",
      "3            0.942389            0.947566          0.944977         0.002588  \n",
      "4            0.964871            0.945693          0.955282         0.009589  \n",
      "5            0.893677            0.958333          0.926005         0.032328  \n",
      "6            0.570023            0.519663          0.544843         0.025180  \n",
      "7            0.525527            0.501404          0.513466         0.012061  \n",
      "8            0.873068            0.868446          0.870757         0.002311  \n",
      "9            0.842623            0.885768          0.864195         0.021572  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "results = rs_clf.cv_results_\n",
    "df = pd.DataFrame(data=results)\n",
    "print(df)\n",
    "df.to_csv(output+'/MLP_disciplines_only_rs_results_%s.csv' % timestamp, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
