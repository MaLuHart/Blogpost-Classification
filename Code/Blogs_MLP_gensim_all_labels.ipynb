{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textklassifikation mit Vektorisierung in gensim und MLPClassifier \n",
    "Labels (Theme und Disziplinen) sind nicht reduziert (all_labels)\n",
    "\n",
    "Autorin: Maria Hartmann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "import multiprocessing # module for multiprocessing \n",
    "from sklearn.base import BaseEstimator\n",
    "import gensim # module for Doc2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from gensim.sklearn_api import D2VTransformer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # module to one-hot-encode the labels\n",
    "from sklearn.pipeline import Pipeline # assemples transormers \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # module to transform a count matrix to a normalized tf-idf representation\n",
    "from sklearn.neural_network import MLPClassifier # MultiLayerPerceptron classifier \n",
    "from sklearn.model_selection import RandomizedSearchCV # module for paramter optimization\n",
    "\n",
    "np.random.seed(7) # fix random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einlesen des Trainings- und Testdatensatzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = '../Datasets/all_labels_trainset.csv' \n",
    "testset = '../Datasets/all_labels_testset.csv' \n",
    "\n",
    "trainset_csv = pd.read_csv(trainset, delimiter=';')\n",
    "X_train = trainset_csv['text'].values\n",
    "y_train = trainset_csv['classes'].values\n",
    "z_train = trainset_csv['filename'].values\n",
    "\n",
    "testset_csv = pd.read_csv(testset, delimiter=';')\n",
    "X_test = testset_csv['text'].values\n",
    "y_test = testset_csv['classes'].values\n",
    "z_test = testset_csv['filename'].values\n",
    "\n",
    "# Splitten der Labels pro Blogbeitrag\n",
    "y_train = [e.split(', ') for e in y_train]\n",
    "y_test = [e.split(', ') for e in y_test]\n",
    "\n",
    "# Splitten der Texte in Wörter\n",
    "X_train = [e.split(' ') for e in X_train]\n",
    "X_test = [e.split(' ') for e in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nummer_212.txt\n",
      "['histoire_d', \"sciences de l'information et de la communication_d\", 'bibliothéconomie_d', 'histoire_t', 'histoire intellectuelle_t', 'histoire et sociologie des médias_t', 'histoire culturelle_t']\n",
      "['die', 'gemälde', 'der', 'habsburgischen', 'sammlungen', 'zu', 'wien', 'wurden', 'von', 'der', 'stallburg', 'ins', 'belvedere', 'transferiert', 'und', 'dort', 'von', 'christian', 'von', 'mechel', 'neu', 'angeordnet', 'und', 'aufgehängt']\n"
     ]
    }
   ],
   "source": [
    "print(z_train[0])\n",
    "print(y_train[0])\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stoppwortfilterung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(X_train):\n",
    "    #stopwords = open('../Preprocessing/filtered_words_MLP.txt', 'r', encoding='utf-8').read().splitlines()\n",
    "    stopwords = open('../Preprocessing/filtered_words.txt', 'r', encoding='utf-8').read().splitlines()\n",
    "    #stopwords = open('../Preprocessing/german_stopwords_plain.txt', 'r', encoding='utf-8').read().splitlines()\n",
    "    clean_textlist = []\n",
    "    for text in X_train:\n",
    "        clean_text = []\n",
    "        for word in text:\n",
    "            if word in stopwords:\n",
    "            #if word in stopwords[9:]: #Die ersten Zeilen enthalten eine Beschreibung\n",
    "                continue\n",
    "            else:\n",
    "                clean_text.append(word)\n",
    "        clean_textlist.append(clean_text)\n",
    "    #print(clean_textlist)\n",
    "    return clean_textlist\n",
    "\n",
    "X_train = remove_stopwords(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-hot-Kodierung der Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# k-hot-encode labels mit MultiLabelBinarizer\n",
    "label_encoder = MultiLabelBinarizer()\n",
    "encoded_y_train = label_encoder.fit_transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "print(encoded_y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n",
      "0 1914-1918_t\n",
      "1 1918-1939_t\n",
      "2 1939-1945_t\n",
      "3 1945-1989_t\n",
      "4 administration publique et développement_d\n",
      "5 anthropologie politique_t\n",
      "6 approches de corpus_t\n",
      "7 archives_t\n",
      "8 archéologie_d\n",
      "9 arts et humanités_d\n",
      "10 arts_d\n",
      "11 asie_t\n",
      "12 bas moyen âge_t\n",
      "13 bibliothéconomie_d\n",
      "14 biomédecine_d\n",
      "15 chine_t\n",
      "16 communication_d\n",
      "17 conflits_t\n",
      "18 digital humanities_t\n",
      "19 enquêtes_t\n",
      "20 europe centrale et orientale_t\n",
      "21 europe_t\n",
      "22 france_t\n",
      "23 guerres_t\n",
      "24 haut moyen âge_t\n",
      "25 histoire culturelle_t\n",
      "26 histoire de l'art_t\n",
      "27 histoire des religions_t\n",
      "28 histoire des sciences sociales_d\n",
      "29 histoire des sciences_t\n",
      "30 histoire du droit_t\n",
      "31 histoire et archéologie_d\n",
      "32 histoire et philosophie des sciences_d\n",
      "33 histoire et sociologie des médias_t\n",
      "34 histoire industrielle_t\n",
      "35 histoire intellectuelle_t\n",
      "36 histoire politique_t\n",
      "37 histoire sociale_t\n",
      "38 histoire urbaine_t\n",
      "39 histoire économique_t\n",
      "40 histoire_d\n",
      "41 histoire_t\n",
      "42 historiographie_t\n",
      "43 humanités pluridisciplinaires_d\n",
      "44 information_t\n",
      "45 langage_t\n",
      "46 langue et linguistique_d\n",
      "47 linguistique appliquée_d\n",
      "48 linguistique_t\n",
      "49 littérature_d\n",
      "50 littératures_t\n",
      "51 monde germanique_t\n",
      "52 moyen âge_t\n",
      "53 musique_d\n",
      "54 méthodes de traitement et de représentation_t\n",
      "55 patrimoine_t\n",
      "56 pays baltes et scandinaves_t\n",
      "57 pensée_t\n",
      "58 philosophie_d\n",
      "59 philosophie_t\n",
      "60 pluridisciplinarité_d\n",
      "61 prospectives_t\n",
      "62 psychisme_t\n",
      "63 psychologie expérimentale_d\n",
      "64 psychologie_d\n",
      "65 psychologie_t\n",
      "66 relations internationales_t\n",
      "67 religions_d\n",
      "68 religions_t\n",
      "69 représentations_t\n",
      "70 révolution française_t\n",
      "71 sciences auxiliaires de l'histoire_t\n",
      "72 sciences cognitives_t\n",
      "73 sciences de l'information et bibliothéconomie_d\n",
      "74 sciences de l'information et de la communication_d\n",
      "75 sciences de l'information_t\n",
      "76 sciences de l'éducation_t\n",
      "77 sciences politiques_d\n",
      "78 sciences sociales interdisciplinaires_d\n",
      "79 sociologie de la culture_t\n",
      "80 sociologie des religions_t\n",
      "81 sociologie des sciences_t\n",
      "82 sociologie du travail_t\n",
      "83 sociologie et anthropologie_d\n",
      "84 sociologie urbaine_t\n",
      "85 sociologie économique_t\n",
      "86 sociologie_d\n",
      "87 sociologie_t\n",
      "88 travail social et politique sociale_d\n",
      "89 travail social_d\n",
      "90 vie de la recherche_t\n",
      "91 violence_t\n",
      "92 xvie siècle_t\n",
      "93 xviie siècle_t\n",
      "94 xviiie siècle_t\n",
      "95 xxe siècle_t\n",
      "96 xxie siècle_t\n",
      "97 âges de la vie_t\n",
      "98 économie_d\n",
      "99 édition électronique_t\n",
      "100 éducation et sciences de l'éducation_d\n",
      "101 éducation spécialisée_d\n",
      "102 éducation_d\n",
      "103 éducation_t\n",
      "104 épistémologie et méthodes_t\n",
      "105 époque contemporaine_t\n",
      "106 époque moderne_t\n",
      "107 étude des genres_t\n",
      "108 études anciennes_d\n",
      "109 études asiatiques_d\n",
      "110 études des sciences_t\n",
      "111 études régionales_d\n",
      "112 études sur la famille_d\n",
      "113 études visuelles_t\n"
     ]
    }
   ],
   "source": [
    "print(len(label_encoder.classes_))\n",
    "for i, element in enumerate(label_encoder.classes_):\n",
    "    print(i, element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klassifikation der Daten mit gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = D2VTransformer(dm=0, window=10, iter=20, size=100, min_count=4, sample=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', vectorizer),\n",
    "                     ('clf', MLPClassifier(hidden_layer_sizes=(4096,1024), validation_fraction=0.1, early_stopping=True, verbose=True, random_state=1))\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\doc2vec.py:362: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 15.39062418\n",
      "Validation score: 0.386908\n",
      "Iteration 2, loss = 9.15380830\n",
      "Validation score: 0.437171\n",
      "Iteration 3, loss = 7.78852290\n",
      "Validation score: 0.459965\n",
      "Iteration 4, loss = 6.81823120\n",
      "Validation score: 0.496201\n",
      "Iteration 5, loss = 6.05225084\n",
      "Validation score: 0.501461\n",
      "Iteration 6, loss = 5.35493116\n",
      "Validation score: 0.509059\n",
      "Iteration 7, loss = 4.69862046\n",
      "Validation score: 0.541204\n",
      "Iteration 8, loss = 4.13840602\n",
      "Validation score: 0.541788\n",
      "Iteration 9, loss = 3.56103418\n",
      "Validation score: 0.531853\n",
      "Iteration 10, loss = 3.06816381\n",
      "Validation score: 0.551140\n",
      "Iteration 11, loss = 2.57990222\n",
      "Validation score: 0.542373\n",
      "Iteration 12, loss = 2.18571266\n",
      "Validation score: 0.555231\n",
      "Iteration 13, loss = 1.83785760\n",
      "Validation score: 0.560491\n",
      "Iteration 14, loss = 1.57701640\n",
      "Validation score: 0.549386\n",
      "Iteration 15, loss = 1.30962439\n",
      "Validation score: 0.556400\n",
      "Iteration 16, loss = 1.14745803\n",
      "Validation score: 0.555815\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "start = time.time()\n",
    "text_clf = text_clf.fit(X_train, encoded_y_train)\n",
    "processing_time = (time.time() - start) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('vect', D2VTransformer(alpha=0.025, batch_words=10000, cbow_mean=1, comment=None,\n",
      "        dbow_words=0, dm=0, dm_concat=0, dm_mean=None, dm_tag_count=1,\n",
      "        docvecs=None, docvecs_mapfile=None,\n",
      "        hashfxn=<built-in function hash>, hs=0, iter=20,\n",
      "        max_vocab_size=None, min_alpha=0.0001, min_count=4, negative=5,\n",
      "        sample=0, seed=1, size=100, sorted_vocab=1, trim_rule=None,\n",
      "        window=10, workers=3)), ('clf', MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(4096, 1024), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
      "       warm_start=False))], 'vect': D2VTransformer(alpha=0.025, batch_words=10000, cbow_mean=1, comment=None,\n",
      "        dbow_words=0, dm=0, dm_concat=0, dm_mean=None, dm_tag_count=1,\n",
      "        docvecs=None, docvecs_mapfile=None,\n",
      "        hashfxn=<built-in function hash>, hs=0, iter=20,\n",
      "        max_vocab_size=None, min_alpha=0.0001, min_count=4, negative=5,\n",
      "        sample=0, seed=1, size=100, sorted_vocab=1, trim_rule=None,\n",
      "        window=10, workers=3), 'clf': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(4096, 1024), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
      "       warm_start=False), 'vect__alpha': 0.025, 'vect__batch_words': 10000, 'vect__cbow_mean': 1, 'vect__comment': None, 'vect__dbow_words': 0, 'vect__dm': 0, 'vect__dm_concat': 0, 'vect__dm_mean': None, 'vect__dm_tag_count': 1, 'vect__docvecs': None, 'vect__docvecs_mapfile': None, 'vect__hashfxn': <built-in function hash>, 'vect__hs': 0, 'vect__iter': 20, 'vect__max_vocab_size': None, 'vect__min_alpha': 0.0001, 'vect__min_count': 4, 'vect__negative': 5, 'vect__sample': 0, 'vect__seed': 1, 'vect__size': 100, 'vect__sorted_vocab': 1, 'vect__trim_rule': None, 'vect__window': 10, 'vect__workers': 3, 'clf__activation': 'relu', 'clf__alpha': 0.0001, 'clf__batch_size': 'auto', 'clf__beta_1': 0.9, 'clf__beta_2': 0.999, 'clf__early_stopping': True, 'clf__epsilon': 1e-08, 'clf__hidden_layer_sizes': (4096, 1024), 'clf__learning_rate': 'constant', 'clf__learning_rate_init': 0.001, 'clf__max_iter': 200, 'clf__momentum': 0.9, 'clf__nesterovs_momentum': True, 'clf__power_t': 0.5, 'clf__random_state': 1, 'clf__shuffle': True, 'clf__solver': 'adam', 'clf__tol': 0.0001, 'clf__validation_fraction': 0.1, 'clf__verbose': True, 'clf__warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "clf_params = text_clf.get_params()\n",
    "print(clf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.418157331148784\n"
     ]
    }
   ],
   "source": [
    "print(processing_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predicted = text_clf.predict(X_test)\n",
    "#predicted_proba = text_clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8217658127545926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# precision is a measure of result relevancy\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(encoded_y_test, predicted, average='samples')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7562095863463324\n"
     ]
    }
   ],
   "source": [
    "# recall is a measure of how many truly relevant results are returned\n",
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(encoded_y_test, predicted, average='samples')  \n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7706134706298624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# F1 score is a weighted average of the precision and recall\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(encoded_y_test, predicted, average='samples') \n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = '../MLP/gensim_klein'\n",
    "if not os.path.exists(output):\n",
    "    os.makedirs(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write parameters and scores to file\n",
    "\n",
    "with open(output+'/MLP_gensim_all_labels_params.txt',\"a\", encoding=\"utf8\") as params:\n",
    "    params.write(\"\\n*********************************************************************************************\")\n",
    "    params.write(\"\\nParameters for classification with MLP and vectorization in gensim (all labels):\")\n",
    "    params.write(\"\\n*********************************************************************************************\")\n",
    "    params.write(\"\\n%s\" % text_clf.named_steps.vect)\n",
    "    params.write(\"\\n%s\" % text_clf.named_steps.clf)\n",
    "    #for key, value in clf_params.items():\n",
    "        #params.write(\"\\n%s: %s\" % (key, value))\n",
    "    params.write(\"\\nclasses: %s\" % text_clf.named_steps.clf.n_outputs_)\n",
    "    params.write(\"\\nlayers: %s\" % text_clf.named_steps.clf.n_layers_)\n",
    "    params.write(\"\\nactivation function output layer: %s\" % text_clf.named_steps.clf.out_activation_) \n",
    "    params.write(\"\\nepochs: %s\" % text_clf.named_steps.clf.n_iter_)\n",
    "    params.write(\"\\nprocessing time: %s\" % processing_time)\n",
    "    params.write(\"\\nSCORES:\")\n",
    "    params.write(\"\\nprecision: %s\" % precision)\n",
    "    params.write(\"\\nrecall: %s\" % recall)\n",
    "    params.write(\"\\nf1-score: %s\" % f1)\n",
    "    params.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Speicherung der vektrorisierten Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17109\n",
      "nummer.hypotheses.org/212\n"
     ]
    }
   ],
   "source": [
    "z_train = [e.replace('.txt', '') for e in z_train]\n",
    "z_test = [e.replace('.txt', '') for e in z_test]\n",
    "ident_train = [e.replace('_', '.hypotheses.org/') for e in z_train]\n",
    "ident_test = [e.replace('_', '.hypotheses.org/') for e in z_test]\n",
    "\n",
    "print(len(ident_train))\n",
    "print(ident_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17109, 100)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# vectorize textdata\n",
    "train_vect = vectorizer.transform(X_train)\n",
    "test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "print(train_vect.shape)\n",
    "print(type(train_vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<17109x100 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 1710900 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert vectorized textdata to sparse matrix\n",
    "train_matrix = sparse.csr_matrix(train_vect)\n",
    "test_matrix = sparse.csr_matrix(test_vect)\n",
    "\n",
    "train_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filename, classes, textvectors in csv file\n",
    "# trainset\n",
    "# speichert vektorisierten Text\n",
    "output_file_train = 'Datasets/all_labels_train_gensim_sparse_matrix.npz'\n",
    "scipy.sparse.save_npz('../'+output_file_train, train_matrix)\n",
    "\n",
    "# speichert filenames und classes\n",
    "with open('../Datasets/all_labels_gensim_train_idents_labels.csv', 'w', newline='', encoding=\"utf-8\") as traincsv:\n",
    "    train = csv.writer(traincsv, delimiter = \";\")\n",
    "    train.writerow([\"url\", \"classes\", \"filename\"])\n",
    "    \n",
    "    for ident, labels in zip(ident_train, y_train):\n",
    "        labellist = \", \".join(labels)\n",
    "        train.writerow([ident, labellist, output_file_train])\n",
    "\n",
    "# testset\n",
    "# speichert vektorisierten Text\n",
    "output_file_test = 'Datasets/all_labels_test_gensim_sparse_matrix.npz'\n",
    "scipy.sparse.save_npz('../'+output_file_test, test_matrix)\n",
    "\n",
    "# speichert filenames und classes\n",
    "with open('../Datasets/all_labels_gensim_test_idents_labels.csv', 'w', newline='', encoding=\"utf-8\") as testcsv:\n",
    "    test = csv.writer(testcsv, delimiter = \";\")\n",
    "    test.writerow([\"url\", \"classes\", \"filename\"])\n",
    "    \n",
    "    for ident, labels in zip(ident_test, y_test):\n",
    "        labellist = \", \".join(labels)\n",
    "        test.writerow([ident, labellist, output_file_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
