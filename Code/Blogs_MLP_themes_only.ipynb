{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textklassifikation mit Vektorisierung in scikit-learn und MLPClassifier \n",
    "Labels sind auf Themen reduziert (themes_only)\n",
    "\n",
    "Autorin: Maria Hartmann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # module to one-hot-encode the labels\n",
    "from sklearn.pipeline import Pipeline # assemples transormers \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # module to transform a count matrix to a normalized tf-idf representation\n",
    "from sklearn.neural_network import MLPClassifier # MultiLayerPerceptron classifier \n",
    "from sklearn.model_selection import RandomizedSearchCV # module for paramter optimization\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "np.random.seed(7) # fix random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einlesen des Trainings- und Testdatensatzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = '../Datasets/themes_only_trainset.csv' \n",
    "testset = '../Datasets/themes_only_testset.csv' \n",
    "\n",
    "trainset_csv = pd.read_csv(trainset, delimiter=';')\n",
    "X_train = trainset_csv['text'].values\n",
    "y_train = trainset_csv['classes'].values\n",
    "z_train = trainset_csv['filename'].values\n",
    "\n",
    "testset_csv = pd.read_csv(testset, delimiter=';')\n",
    "X_test = testset_csv['text'].values\n",
    "y_test = testset_csv['classes'].values\n",
    "z_test = testset_csv['filename'].values\n",
    "\n",
    "# Splitten der Labels pro Blogbeitrag\n",
    "y_train = [e.split(', ') for e in y_train]\n",
    "y_test = [e.split(', ') for e in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ordensgeschichte_6160.txt\n",
      "['histoire_t', 'époque moderne_t', 'moyen âge_t']\n",
      "das colloquium historicum wirsbergense e v chw lädt für freitag und samstag oktober zu einem wissenschaftlichen symposium in den kastenhof weismain lkr lichtenfels ein im mittelpunkt der tagung steht mauritius knauer der vor jahren in der bambergischen stadt weismain geboren wurde er trat in die zisterzienserabtei langheim ein nachdem er während des dreißigjährigen kriegs mehrere jahre in heiligenkreuz und wien verbracht hatte wurde er prior von langheim trat er als abt an die spitze des klosters er starb knauer war universalgelehrter er kämpfte für die rechte seines klosters besaß medizinische kenntnisse schrieb theologische werke und belebte kraftvoll die wirtschaft langheims bis heute ist er als verfasser des hundertjährigen kalenders weithin berühmt bei der tagung sollen leben und wirken knauers in größere zusammenhänge eingeordnet werden programm freitag oktober uhr prof dr günter dippold lichtenfels einführung in das tagungsthema anschließend dr stefan knoch bamberg handschriften von mauritius knauer in der staatsbibliothek bamberg uhr dr leonhard scherg marktheidenfeld die fränkischen zisterzienser zur zeit von mauritius knauer empfang samstag oktober uhr prof dr günter dippold lichtenfels die wirtschaft des klosters langheim zur zeit von mauritius knauer uhr andrea göldner m a weismain die stadt weismain zur zeit von mauritius knauer anschließend kurzer stadtrundgang mittagessen uhr prof dr dieter j weiß münchen die bamberger fürstbischöfe zur zeit von mauritius knauer uhr dr norbert jung bamberg die katholische theologie zur zeit von mauritius knauer die teilnahme an der ganzen tagung oder einzelnen vorträgen ist kostenlos es wird jedoch aus organisatorischen gründen um eine formlose anmeldung per e mail gebeten info chw franken de das programm als pdf http chw franken de files programm datei symposium pdf günter dippold colloquium historicum wirsbergense heimat und geschichtsfreunde in franken e v vorsitzender prof dr günter dippold lichtenfels bayreuth vorsitzender gerhard schmidt lichtenfels www chw franken de info chw franken de mauritius knauer http beacon findbuch de seealso pnd aks format sources id\n"
     ]
    }
   ],
   "source": [
    "print(z_train[0])\n",
    "print(y_train[0])\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-hot-Kodierung der Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# k-hot-encode labels mit MultiLabelBinarizer\n",
    "label_encoder = MultiLabelBinarizer()\n",
    "encoded_y_train = label_encoder.fit_transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "print(encoded_y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "0 anthropologie_t\n",
      "1 asie_t\n",
      "2 droit_t\n",
      "3 ethnologie_t\n",
      "4 europe_t\n",
      "5 géographie_t\n",
      "6 histoire_t\n",
      "7 information_t\n",
      "8 langage_t\n",
      "9 moyen âge_t\n",
      "10 pensée_t\n",
      "11 psychisme_t\n",
      "12 religions_t\n",
      "13 représentations_t\n",
      "14 sociologie_t\n",
      "15 éducation_t\n",
      "16 épistémologie et méthodes_t\n",
      "17 époque contemporaine_t\n",
      "18 époque moderne_t\n",
      "19 études des sciences_t\n",
      "20 études du politique_t\n"
     ]
    }
   ],
   "source": [
    "print(len(label_encoder.classes_))\n",
    "for i, element in enumerate(label_encoder.classes_):\n",
    "    print(i, element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vektorisierung und Klassifikation der Daten mit scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "stopwords = open('../Preprocessing/filtered_words.txt', 'r', encoding='utf-8').read().splitlines()\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1), max_features=max_features, stop_words=stopwords)\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"text_clf = Pipeline([#('vect', CountVectorizer(ngram_range=(1,4), max_df=0.9, min_df=0.01)),#min_df=0.0 auf min_df=0.01 geändert\\n                     ('vect', CountVectorizer(ngram_range=(1,4), max_features=max_features)),\\n                     ('tfidf', TfidfTransformer(use_idf=True)),\\n                     ('clf', MLPClassifier(hidden_layer_sizes=(1024,512), max_iter=500, validation_fraction=0.1, early_stopping=True, verbose=True, random_state=1)),\\n                    ])\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first try with best params for vect and tfidf from kNN classification\n",
    "\"\"\"text_clf = Pipeline([#('vect', CountVectorizer(ngram_range=(1,4), max_df=0.9, min_df=0.01)),#min_df=0.0 auf min_df=0.01 geändert\n",
    "                     ('vect', CountVectorizer(ngram_range=(1,4), max_features=max_features)),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                     ('clf', MLPClassifier(hidden_layer_sizes=(1024,512), max_iter=500, validation_fraction=0.1, early_stopping=True, verbose=True, random_state=1)),\n",
    "                    ])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', vectorizer), \n",
    "                     ('tfidf', tfidf_transformer),\n",
    "                     ('clf', MLPClassifier(hidden_layer_sizes=(2048,512), tol=0.0001, early_stopping=True, validation_fraction=0.1, verbose=True, random_state=1))\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.99724264\n",
      "Validation score: 0.475745\n",
      "Iteration 2, loss = 2.26204605\n",
      "Validation score: 0.606663\n",
      "Iteration 3, loss = 1.11150565\n",
      "Validation score: 0.624781\n",
      "Iteration 4, loss = 0.50408517\n",
      "Validation score: 0.623027\n",
      "Iteration 5, loss = 0.24275951\n",
      "Validation score: 0.614261\n",
      "Iteration 6, loss = 0.16684479\n",
      "Validation score: 0.607832\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "start = time.time()\n",
    "text_clf = text_clf.fit(X_train, encoded_y_train)\n",
    "processing_time = (time.time() - start) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=['und', 'die', 'der'], strip_accents=None,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(2048, 512), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
      "       warm_start=False))], 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=['und', 'die', 'der'], strip_accents=None,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(2048, 512), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
      "       warm_start=False), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': 10000, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': ['und', 'die', 'der'], 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__activation': 'relu', 'clf__alpha': 0.0001, 'clf__batch_size': 'auto', 'clf__beta_1': 0.9, 'clf__beta_2': 0.999, 'clf__early_stopping': True, 'clf__epsilon': 1e-08, 'clf__hidden_layer_sizes': (2048, 512), 'clf__learning_rate': 'constant', 'clf__learning_rate_init': 0.001, 'clf__max_iter': 200, 'clf__momentum': 0.9, 'clf__nesterovs_momentum': True, 'clf__power_t': 0.5, 'clf__random_state': 1, 'clf__shuffle': True, 'clf__solver': 'adam', 'clf__tol': 0.0001, 'clf__validation_fraction': 0.1, 'clf__verbose': True, 'clf__warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "clf_params = text_clf.get_params()\n",
    "print(clf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predicted = text_clf.predict(X_test)\n",
    "#predicted_proba = text_clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8621218192746944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# precision is a measure of result relevancy\n",
    "precision = precision_score(encoded_y_test, predicted, average='samples')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8009467040673212\n"
     ]
    }
   ],
   "source": [
    "# recall is a measure of how many truly relevant results are returned\n",
    "recall = recall_score(encoded_y_test, predicted, average='samples')  \n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8133429541213552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# F1 score is a weighted average of the precision and recall\n",
    "f1 = f1_score(encoded_y_test, predicted, average='samples') \n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = '../MLP'\n",
    "if not os.path.exists(output):\n",
    "    os.makedirs(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# write first parameters and scores to file\\n\\n#with open(output+\\'/MLP_themes_only_first_params.txt\\',\"w+\", encoding=\"utf8\") as params:\\nwith open(output+\\'/MLP_themes_only_first_params_max_features.txt\\',\"w+\", encoding=\"utf8\") as params:\\n    params.write(\"First parameters for classification with MLP (themes only):\")\\n    params.write(\"\\nprocessing_time: %s\" % processing_time)\\n    for key, value in clf_params.items():\\n        params.write(\"\\n%s: %s\" % (key, value))\\n    params.write(\"\\nactivation function output layer: %s\" % text_clf.named_steps.clf.out_activation_)    \\n    params.write(\"\\nprecision: %s\" % precision)\\n    params.write(\"\\nrecall: %s\" % recall)\\n    params.write(\"\\nf1-score: %s\" % f1)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# write first parameters and scores to file\n",
    "\n",
    "#with open(output+'/MLP_themes_only_first_params.txt',\"w+\", encoding=\"utf8\") as params:\n",
    "with open(output+'/MLP_themes_only_first_params_max_features.txt',\"w+\", encoding=\"utf8\") as params:\n",
    "    params.write(\"First parameters for classification with MLP (themes only):\")\n",
    "    params.write(\"\\nprocessing_time: %s\" % processing_time)\n",
    "    for key, value in clf_params.items():\n",
    "        params.write(\"\\n%s: %s\" % (key, value))\n",
    "    params.write(\"\\nactivation function output layer: %s\" % text_clf.named_steps.clf.out_activation_)    \n",
    "    params.write(\"\\nprecision: %s\" % precision)\n",
    "    params.write(\"\\nrecall: %s\" % recall)\n",
    "    params.write(\"\\nf1-score: %s\" % f1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write parameters and scores to file\n",
    "\n",
    "with open(output+'/MLP_themes_only_params.txt',\"a\", encoding=\"utf8\") as params:\n",
    "    params.write(\"\\n*********************************************************************************************\")\n",
    "    params.write(\"\\nParameters for classification with MLP (themes only):\")\n",
    "    params.write(\"\\n*********************************************************************************************\")\n",
    "    params.write(\"\\n%s\" % text_clf.named_steps.vect)\n",
    "    params.write(\"\\n%s\" % text_clf.named_steps.tfidf)\n",
    "    params.write(\"\\n%s\" % text_clf.named_steps.clf)\n",
    "    #for key, value in clf_params.items():\n",
    "        #params.write(\"\\n%s: %s\" % (key, value))\n",
    "    params.write(\"\\nclasses: %s\" % text_clf.named_steps.clf.n_outputs_)\n",
    "    params.write(\"\\nlayers: %s\" % text_clf.named_steps.clf.n_layers_)\n",
    "    params.write(\"\\nactivation function output layer: %s\" % text_clf.named_steps.clf.out_activation_) \n",
    "    params.write(\"\\nepochs: %s\" % text_clf.named_steps.clf.n_iter_)\n",
    "    params.write(\"\\nprocessing time: %s\" % processing_time)\n",
    "    params.write(\"\\nSCORES:\")\n",
    "    params.write(\"\\nprecision: %s\" % precision)\n",
    "    params.write(\"\\nrecall: %s\" % recall)\n",
    "    params.write(\"\\nf1-score: %s\" % f1)\n",
    "    params.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED:\n",
      "('histoire_t', 'époque contemporaine_t')\n",
      "TRUE:\n",
      "['époque contemporaine_t', 'histoire_t']\n"
     ]
    }
   ],
   "source": [
    "# write real labels and predictions to file\n",
    "\n",
    "inverse_prediction = label_encoder.inverse_transform(predicted)\n",
    "print('PREDICTED:')\n",
    "print(inverse_prediction[0])\n",
    "print('TRUE:')\n",
    "print(y_test[0])\n",
    "\n",
    "with open(output+'/MLP_themes_only_predictions.txt',\"w+\", encoding=\"utf8\") as preds:\n",
    "    preds.write(\"Predictions from classification with Multi-Layer-Perzeptron and vectorization in scikit-learn (themes only):\\n\\n\")\n",
    "    for ident, label, pred in zip(z_test, y_test, inverse_prediction):\n",
    "        label = sorted(label)\n",
    "        pred = sorted(pred)\n",
    "        preds.write(ident)\n",
    "        preds.write('\\n')\n",
    "        preds.write('TRUE: ')\n",
    "        for element in label:\n",
    "            preds.write('%s, ' % element)\n",
    "        preds.write('\\n')\n",
    "        preds.write('PRED: ')\n",
    "        for element in pred:\n",
    "            preds.write('%s, ' % element)\n",
    "        preds.write('\\n')\n",
    "        preds.write('\\n*********************\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speicherung der vektorisierten Textdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17109\n",
      "ordensgeschichte.hypotheses.org/6160\n"
     ]
    }
   ],
   "source": [
    "z_train = [e.replace('.txt', '') for e in z_train]\n",
    "z_test = [e.replace('.txt', '') for e in z_test]\n",
    "ident_train = [e.replace('_', '.hypotheses.org/') for e in z_train]\n",
    "ident_test = [e.replace('_', '.hypotheses.org/') for e in z_test]\n",
    "\n",
    "print(len(ident_train))\n",
    "print(ident_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17109, 10000)\n"
     ]
    }
   ],
   "source": [
    "# vectorize textdata\n",
    "train_vect = vectorizer.transform(X_train)\n",
    "train_tfidf = tfidf_transformer.transform(train_vect)\n",
    "print(train_tfidf.shape)\n",
    "\n",
    "test_vect = vectorizer.transform(X_test)\n",
    "test_tfidf = tfidf_transformer.transform(test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<17109x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2389735 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(test_tfidf))\n",
    "train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filename, classes, textvectors in csv file\n",
    "# trainset\n",
    "# speichert vektorisierten Text\n",
    "output_file_train = 'Datasets/themes_only_train_scikit-learn_sparse_matrix.npz'\n",
    "scipy.sparse.save_npz('../'+output_file_train, train_tfidf)\n",
    "\n",
    "# speichert filenames und classes\n",
    "with open('../Datasets/themes_only_train_idents_labels.csv', 'w', newline='', encoding=\"utf-8\") as traincsv:\n",
    "    train = csv.writer(traincsv, delimiter = \";\")\n",
    "    train.writerow([\"url\", \"classes\", \"filename\"])\n",
    "    \n",
    "    for ident, labels in zip(ident_train, y_train):\n",
    "        labellist = \", \".join(labels)\n",
    "        train.writerow([ident, labellist, output_file_train])\n",
    "\n",
    "# testset\n",
    "# speichert vektorisierten Text\n",
    "output_file_test = 'Datasets/themes_only_test_scikit-learn_sparse_matrix.npz'\n",
    "scipy.sparse.save_npz('../'+output_file_test, test_tfidf)\n",
    "\n",
    "# speichert filenames und classes\n",
    "with open('../Datasets/themes_only_test_idents_labels.csv', 'w', newline='', encoding=\"utf-8\") as testcsv:\n",
    "    test = csv.writer(testcsv, delimiter = \";\")\n",
    "    test.writerow([\"url\", \"classes\", \"filename\"])\n",
    "    \n",
    "    for ident, labels in zip(ident_test, y_test):\n",
    "        labellist = \", \".join(labels)\n",
    "        test.writerow([ident, labellist, output_file_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameteroptimierung mit Rastersuche (RandomizedSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', MLPClassifier(validation_fraction=0.1, early_stopping=True, verbose=True, random_state=1)),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter tuning with RandomSearch\n",
    "stopwords = open('../Preprocessing/filtered_words.txt', 'r', encoding='utf-8').read().splitlines()\n",
    "rs_parameters = {'vect__ngram_range': [(1,1),(1,2),(1,3),(1,4)], \n",
    "                 #'vect__max_df' : (0.7, 0.8, 0.85, 0.9, 0.95), #1.0 \n",
    "                 #'vect__min_df' : (0.01, 0.025, 0.05, 0.075, 0.1, 0.2), #0.0\n",
    "                 'vect__max_features': (100000,50000,25000,10000,7500,5000,2500,1000,500,300,100), \n",
    "                 'tfidf__use_idf': (True, False),\n",
    "                 'clf__hidden_layer_sizes': ((2048,1024),(2048,512),(1024,512),(512,128),(4096,1024),(4096,512),(2048,1024,512),(1024,512,128))\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "[CV] vect__ngram_range=(1, 3), vect__max_features=50000, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 8.85934247\n",
      "Validation score: 0.065421\n",
      "Iteration 2, loss = 5.28424927\n",
      "Validation score: 0.186916\n",
      "Iteration 3, loss = 3.84880149\n",
      "Validation score: 0.341121\n",
      "Iteration 4, loss = 2.87227908\n",
      "Validation score: 0.392523\n",
      "Iteration 5, loss = 1.98288717\n",
      "Validation score: 0.434579\n",
      "Iteration 6, loss = 1.24382519\n",
      "Validation score: 0.453271\n",
      "Iteration 7, loss = 0.70544635\n",
      "Validation score: 0.453271\n",
      "Iteration 8, loss = 0.37320448\n",
      "Validation score: 0.443925\n",
      "Iteration 9, loss = 0.18249447\n",
      "Validation score: 0.448598\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 3), vect__max_features=50000, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024), score=0.45582047685834504, total=14.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 15.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] vect__ngram_range=(1, 3), vect__max_features=50000, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 8.90021524\n",
      "Validation score: 0.369159\n",
      "Iteration 2, loss = 5.25294105\n",
      "Validation score: 0.266355\n",
      "Iteration 3, loss = 3.83664634\n",
      "Validation score: 0.341121\n",
      "Iteration 4, loss = 2.72994203\n",
      "Validation score: 0.425234\n",
      "Iteration 5, loss = 1.77956174\n",
      "Validation score: 0.448598\n",
      "Iteration 6, loss = 1.03134512\n",
      "Validation score: 0.457944\n",
      "Iteration 7, loss = 0.53778869\n",
      "Validation score: 0.471963\n",
      "Iteration 8, loss = 0.27010461\n",
      "Validation score: 0.467290\n",
      "Iteration 9, loss = 0.14069682\n",
      "Validation score: 0.467290\n",
      "Iteration 10, loss = 0.09050792\n",
      "Validation score: 0.481308\n",
      "Iteration 11, loss = 0.05512860\n",
      "Validation score: 0.462617\n",
      "Iteration 12, loss = 0.04279216\n",
      "Validation score: 0.490654\n",
      "Iteration 13, loss = 0.03999546\n",
      "Validation score: 0.467290\n",
      "Iteration 14, loss = 0.03706466\n",
      "Validation score: 0.485981\n",
      "Iteration 15, loss = 0.03004769\n",
      "Validation score: 0.481308\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 3), vect__max_features=50000, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024), score=0.46984572230014027, total=24.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 39.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] vect__ngram_range=(1, 3), vect__max_features=2500, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 8.33965188\n",
      "Validation score: 0.042056\n",
      "Iteration 2, loss = 5.34418872\n",
      "Validation score: 0.224299\n",
      "Iteration 3, loss = 4.40409877\n",
      "Validation score: 0.350467\n",
      "Iteration 4, loss = 3.83880245\n",
      "Validation score: 0.355140\n",
      "Iteration 5, loss = 3.39383933\n",
      "Validation score: 0.387850\n",
      "Iteration 6, loss = 2.93836412\n",
      "Validation score: 0.392523\n",
      "Iteration 7, loss = 2.49116517\n",
      "Validation score: 0.397196\n",
      "Iteration 8, loss = 2.05921736\n",
      "Validation score: 0.392523\n",
      "Iteration 9, loss = 1.64385238\n",
      "Validation score: 0.397196\n",
      "Iteration 10, loss = 1.28200432\n",
      "Validation score: 0.392523\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 3), vect__max_features=2500, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024), score=0.40860215053763443, total= 1.5min\n",
      "[CV] vect__ngram_range=(1, 3), vect__max_features=2500, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 41.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 8.29670063\n",
      "Validation score: 0.168224\n",
      "Iteration 2, loss = 5.36616712\n",
      "Validation score: 0.238318\n",
      "Iteration 3, loss = 4.44817882\n",
      "Validation score: 0.271028\n",
      "Iteration 4, loss = 3.84141687\n",
      "Validation score: 0.289720\n",
      "Iteration 5, loss = 3.37203429\n",
      "Validation score: 0.317757\n",
      "Iteration 6, loss = 2.91761168\n",
      "Validation score: 0.322430\n",
      "Iteration 7, loss = 2.47129096\n",
      "Validation score: 0.355140\n",
      "Iteration 8, loss = 2.05017780\n",
      "Validation score: 0.359813\n",
      "Iteration 9, loss = 1.65427666\n",
      "Validation score: 0.378505\n",
      "Iteration 10, loss = 1.26835294\n",
      "Validation score: 0.397196\n",
      "Iteration 11, loss = 0.97798107\n",
      "Validation score: 0.401869\n",
      "Iteration 12, loss = 0.72802888\n",
      "Validation score: 0.392523\n",
      "Iteration 13, loss = 0.54551809\n",
      "Validation score: 0.406542\n",
      "Iteration 14, loss = 0.40279123\n",
      "Validation score: 0.397196\n",
      "Iteration 15, loss = 0.30280283\n",
      "Validation score: 0.397196\n",
      "Iteration 16, loss = 0.22626666\n",
      "Validation score: 0.397196\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 3), vect__max_features=2500, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024), score=0.45535296867695185, total= 2.4min\n",
      "[CV] vect__ngram_range=(1, 4), vect__max_features=5000, tfidf__use_idf=False, clf__hidden_layer_sizes=(512, 128) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 43.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 13.39800502\n",
      "Validation score: 0.014019\n",
      "Iteration 2, loss = 9.09619269\n",
      "Validation score: 0.014019\n",
      "Iteration 3, loss = 6.10113403\n",
      "Validation score: 0.032710\n",
      "Iteration 4, loss = 5.42841348\n",
      "Validation score: 0.065421\n",
      "Iteration 5, loss = 5.08465631\n",
      "Validation score: 0.219626\n",
      "Iteration 6, loss = 4.80180043\n",
      "Validation score: 0.238318\n",
      "Iteration 7, loss = 4.49469448\n",
      "Validation score: 0.378505\n",
      "Iteration 8, loss = 4.19745652\n",
      "Validation score: 0.411215\n",
      "Iteration 9, loss = 3.92348671\n",
      "Validation score: 0.429907\n",
      "Iteration 10, loss = 3.67178218\n",
      "Validation score: 0.439252\n",
      "Iteration 11, loss = 3.44160106\n",
      "Validation score: 0.453271\n",
      "Iteration 12, loss = 3.21605738\n",
      "Validation score: 0.453271\n",
      "Iteration 13, loss = 2.99103320\n",
      "Validation score: 0.462617\n",
      "Iteration 14, loss = 2.77610044\n",
      "Validation score: 0.467290\n",
      "Iteration 15, loss = 2.56192770\n",
      "Validation score: 0.467290\n",
      "Iteration 16, loss = 2.35848451\n",
      "Validation score: 0.462617\n",
      "Iteration 17, loss = 2.16090893\n",
      "Validation score: 0.462617\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 4), vect__max_features=5000, tfidf__use_idf=False, clf__hidden_layer_sizes=(512, 128), score=0.41000467508181393, total=  33.1s\n",
      "[CV] vect__ngram_range=(1, 4), vect__max_features=5000, tfidf__use_idf=False, clf__hidden_layer_sizes=(512, 128) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 44.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 13.39468641\n",
      "Validation score: 0.014019\n",
      "Iteration 2, loss = 9.06917057\n",
      "Validation score: 0.014019\n",
      "Iteration 3, loss = 6.07206942\n",
      "Validation score: 0.182243\n",
      "Iteration 4, loss = 5.40355225\n",
      "Validation score: 0.028037\n",
      "Iteration 5, loss = 5.08669843\n",
      "Validation score: 0.186916\n",
      "Iteration 6, loss = 4.86012696\n",
      "Validation score: 0.196262\n",
      "Iteration 7, loss = 4.57970335\n",
      "Validation score: 0.228972\n",
      "Iteration 8, loss = 4.28835697\n",
      "Validation score: 0.285047\n",
      "Iteration 9, loss = 4.00554868\n",
      "Validation score: 0.341121\n",
      "Iteration 10, loss = 3.73835296\n",
      "Validation score: 0.350467\n",
      "Iteration 11, loss = 3.49258402\n",
      "Validation score: 0.350467\n",
      "Iteration 12, loss = 3.26051616\n",
      "Validation score: 0.369159\n",
      "Iteration 13, loss = 3.03416642\n",
      "Validation score: 0.383178\n",
      "Iteration 14, loss = 2.80746159\n",
      "Validation score: 0.383178\n",
      "Iteration 15, loss = 2.59761065\n",
      "Validation score: 0.383178\n",
      "Iteration 16, loss = 2.39495518\n",
      "Validation score: 0.387850\n",
      "Iteration 17, loss = 2.19990160\n",
      "Validation score: 0.397196\n",
      "Iteration 18, loss = 2.01498825\n",
      "Validation score: 0.406542\n",
      "Iteration 19, loss = 1.82670695\n",
      "Validation score: 0.411215\n",
      "Iteration 20, loss = 1.66062012\n",
      "Validation score: 0.415888\n",
      "Iteration 21, loss = 1.49855460\n",
      "Validation score: 0.411215\n",
      "Iteration 22, loss = 1.34761463\n",
      "Validation score: 0.420561\n",
      "Iteration 23, loss = 1.20903895\n",
      "Validation score: 0.425234\n",
      "Iteration 24, loss = 1.07702386\n",
      "Validation score: 0.415888\n",
      "Iteration 25, loss = 0.95791720\n",
      "Validation score: 0.415888\n",
      "Iteration 26, loss = 0.84618092\n",
      "Validation score: 0.425234\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 4), vect__max_features=5000, tfidf__use_idf=False, clf__hidden_layer_sizes=(512, 128), score=0.4525479195885928, total=  45.7s\n",
      "[CV] vect__ngram_range=(1, 2), vect__max_features=10000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 45.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 10.85639133\n",
      "Validation score: 0.014019\n",
      "Iteration 2, loss = 5.98910346\n",
      "Validation score: 0.014019\n",
      "Iteration 3, loss = 5.18175590\n",
      "Validation score: 0.210280\n",
      "Iteration 4, loss = 4.42362015\n",
      "Validation score: 0.373832\n",
      "Iteration 5, loss = 3.70880872\n",
      "Validation score: 0.373832\n",
      "Iteration 6, loss = 3.12227011\n",
      "Validation score: 0.369159\n",
      "Iteration 7, loss = 2.58434754\n",
      "Validation score: 0.383178\n",
      "Iteration 8, loss = 2.08561412\n",
      "Validation score: 0.420561\n",
      "Iteration 9, loss = 1.62625509\n",
      "Validation score: 0.425234\n",
      "Iteration 10, loss = 1.21722551\n",
      "Validation score: 0.439252\n",
      "Iteration 11, loss = 0.87386463\n",
      "Validation score: 0.462617\n",
      "Iteration 12, loss = 0.61450841\n",
      "Validation score: 0.457944\n",
      "Iteration 13, loss = 0.41422801\n",
      "Validation score: 0.462617\n",
      "Iteration 14, loss = 0.28201531\n",
      "Validation score: 0.467290\n",
      "Iteration 15, loss = 0.18891339\n",
      "Validation score: 0.462617\n",
      "Iteration 16, loss = 0.13263297\n",
      "Validation score: 0.471963\n",
      "Iteration 17, loss = 0.09587231\n",
      "Validation score: 0.467290\n",
      "Iteration 18, loss = 0.07274274\n",
      "Validation score: 0.471963\n",
      "Iteration 19, loss = 0.05669443\n",
      "Validation score: 0.471963\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 2), vect__max_features=10000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512), score=0.45535296867695185, total= 1.8min\n",
      "[CV] vect__ngram_range=(1, 2), vect__max_features=10000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 46.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 10.88796735\n",
      "Validation score: 0.004673\n",
      "Iteration 2, loss = 6.02847678\n",
      "Validation score: 0.014019\n",
      "Iteration 3, loss = 5.15745311\n",
      "Validation score: 0.168224\n",
      "Iteration 4, loss = 4.39075341\n",
      "Validation score: 0.350467\n",
      "Iteration 5, loss = 3.66226244\n",
      "Validation score: 0.387850\n",
      "Iteration 6, loss = 3.03256807\n",
      "Validation score: 0.397196\n",
      "Iteration 7, loss = 2.47105476\n",
      "Validation score: 0.420561\n",
      "Iteration 8, loss = 1.97291565\n",
      "Validation score: 0.439252\n",
      "Iteration 9, loss = 1.51767633\n",
      "Validation score: 0.448598\n",
      "Iteration 10, loss = 1.12257954\n",
      "Validation score: 0.443925\n",
      "Iteration 11, loss = 0.80581820\n",
      "Validation score: 0.434579\n",
      "Iteration 12, loss = 0.56583214\n",
      "Validation score: 0.467290\n",
      "Iteration 13, loss = 0.39339775\n",
      "Validation score: 0.448598\n",
      "Iteration 14, loss = 0.27489509\n",
      "Validation score: 0.453271\n",
      "Iteration 15, loss = 0.19996871\n",
      "Validation score: 0.453271\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 2), vect__max_features=10000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512), score=0.474520804114072, total= 1.4min\n",
      "[CV] vect__ngram_range=(1, 2), vect__max_features=25000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 48.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11.33435019\n",
      "Validation score: 0.018692\n",
      "Iteration 2, loss = 6.03764422\n",
      "Validation score: 0.280374\n",
      "Iteration 3, loss = 5.15689849\n",
      "Validation score: 0.112150\n",
      "Iteration 4, loss = 4.30326354\n",
      "Validation score: 0.406542\n",
      "Iteration 5, loss = 3.49599438\n",
      "Validation score: 0.401869\n",
      "Iteration 6, loss = 2.80338504\n",
      "Validation score: 0.420561\n",
      "Iteration 7, loss = 2.17593502\n",
      "Validation score: 0.439252\n",
      "Iteration 8, loss = 1.61268850\n",
      "Validation score: 0.471963\n",
      "Iteration 9, loss = 1.16728430\n",
      "Validation score: 0.485981\n",
      "Iteration 10, loss = 0.79675568\n",
      "Validation score: 0.490654\n",
      "Iteration 11, loss = 0.52770556\n",
      "Validation score: 0.467290\n",
      "Iteration 12, loss = 0.34477753\n",
      "Validation score: 0.481308\n",
      "Iteration 13, loss = 0.22126327\n",
      "Validation score: 0.471963\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 2), vect__max_features=25000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512), score=0.4544179523141655, total= 2.8min\n",
      "[CV] vect__ngram_range=(1, 2), vect__max_features=25000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 51.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11.38288603\n",
      "Validation score: 0.018692\n",
      "Iteration 2, loss = 5.99082984\n",
      "Validation score: 0.261682\n",
      "Iteration 3, loss = 5.17098169\n",
      "Validation score: 0.200935\n",
      "Iteration 4, loss = 4.30646853\n",
      "Validation score: 0.350467\n",
      "Iteration 5, loss = 3.45755353\n",
      "Validation score: 0.387850\n",
      "Iteration 6, loss = 2.71815401\n",
      "Validation score: 0.434579\n",
      "Iteration 7, loss = 2.06409155\n",
      "Validation score: 0.434579\n",
      "Iteration 8, loss = 1.49491309\n",
      "Validation score: 0.448598\n",
      "Iteration 9, loss = 1.03072098\n",
      "Validation score: 0.457944\n",
      "Iteration 10, loss = 0.68624883\n",
      "Validation score: 0.471963\n",
      "Iteration 11, loss = 0.43623048\n",
      "Validation score: 0.457944\n",
      "Iteration 12, loss = 0.27951448\n",
      "Validation score: 0.453271\n",
      "Iteration 13, loss = 0.18021092\n",
      "Validation score: 0.453271\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 2), vect__max_features=25000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512), score=0.47171575502571295, total= 2.8min\n",
      "[CV] vect__ngram_range=(1, 4), vect__max_features=7500, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 8.59172675\n",
      "Validation score: 0.280374\n",
      "Iteration 2, loss = 5.17778855\n",
      "Validation score: 0.345794\n",
      "Iteration 3, loss = 3.87847710\n",
      "Validation score: 0.415888\n",
      "Iteration 4, loss = 3.03816859\n",
      "Validation score: 0.453271\n",
      "Iteration 5, loss = 2.24960724\n",
      "Validation score: 0.485981\n",
      "Iteration 6, loss = 1.50936341\n",
      "Validation score: 0.476636\n",
      "Iteration 7, loss = 0.93364065\n",
      "Validation score: 0.504673\n",
      "Iteration 8, loss = 0.53696752\n",
      "Validation score: 0.495327\n",
      "Iteration 9, loss = 0.28744064\n",
      "Validation score: 0.485981\n",
      "Iteration 10, loss = 0.15054709\n",
      "Validation score: 0.509346\n",
      "Iteration 11, loss = 0.08358201\n",
      "Validation score: 0.485981\n",
      "Iteration 12, loss = 0.05178187\n",
      "Validation score: 0.509346\n",
      "Iteration 13, loss = 0.03691184\n",
      "Validation score: 0.504673\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 4), vect__max_features=7500, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024), score=0.4675081813931744, total= 4.2min\n",
      "[CV] vect__ngram_range=(1, 4), vect__max_features=7500, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 8.69944808\n",
      "Validation score: 0.093458\n",
      "Iteration 2, loss = 5.18814033\n",
      "Validation score: 0.327103\n",
      "Iteration 3, loss = 3.86785266\n",
      "Validation score: 0.406542\n",
      "Iteration 4, loss = 3.01545766\n",
      "Validation score: 0.429907\n",
      "Iteration 5, loss = 2.20390507\n",
      "Validation score: 0.397196\n",
      "Iteration 6, loss = 1.48004871\n",
      "Validation score: 0.429907\n",
      "Iteration 7, loss = 0.90699450\n",
      "Validation score: 0.448598\n",
      "Iteration 8, loss = 0.52597780\n",
      "Validation score: 0.462617\n",
      "Iteration 9, loss = 0.29464110\n",
      "Validation score: 0.443925\n",
      "Iteration 10, loss = 0.16497946\n",
      "Validation score: 0.462617\n",
      "Iteration 11, loss = 0.09832407\n",
      "Validation score: 0.453271\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 4), vect__max_features=7500, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024), score=0.485273492286115, total= 3.5min\n",
      "[CV] vect__ngram_range=(1, 1), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(1024, 512, 128) \n",
      "Iteration 1, loss = 11.59719766\n",
      "Validation score: 0.000000\n",
      "Iteration 2, loss = 6.43768116\n",
      "Validation score: 0.289720\n",
      "Iteration 3, loss = 5.36811662\n",
      "Validation score: 0.009346\n",
      "Iteration 4, loss = 5.06141590\n",
      "Validation score: 0.224299\n",
      "Iteration 5, loss = 4.88177058\n",
      "Validation score: 0.144860\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 1), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(1024, 512, 128), score=0.30949041608228145, total=   4.6s\n",
      "[CV] vect__ngram_range=(1, 1), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(1024, 512, 128) \n",
      "Iteration 1, loss = 11.85330006\n",
      "Validation score: 0.000000\n",
      "Iteration 2, loss = 6.65570933\n",
      "Validation score: 0.317757\n",
      "Iteration 3, loss = 5.43389094\n",
      "Validation score: 0.014019\n",
      "Iteration 4, loss = 5.12619720\n",
      "Validation score: 0.233645\n",
      "Iteration 5, loss = 4.94734992\n",
      "Validation score: 0.182243\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 1), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(1024, 512, 128), score=0.3141654978962132, total=   4.6s\n",
      "[CV] vect__ngram_range=(1, 3), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(2048, 1024, 512) \n",
      "Iteration 1, loss = 8.67360150\n",
      "Validation score: 0.247664\n",
      "Iteration 2, loss = 5.50671919\n",
      "Validation score: 0.074766\n",
      "Iteration 3, loss = 5.00571825\n",
      "Validation score: 0.116822\n",
      "Iteration 4, loss = 4.73008461\n",
      "Validation score: 0.163551\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 3), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(2048, 1024, 512), score=0.25806451612903225, total=  15.4s\n",
      "[CV] vect__ngram_range=(1, 3), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(2048, 1024, 512) \n",
      "Iteration 1, loss = 8.83960675\n",
      "Validation score: 0.070093\n",
      "Iteration 2, loss = 5.64743555\n",
      "Validation score: 0.177570\n",
      "Iteration 3, loss = 5.09361740\n",
      "Validation score: 0.224299\n",
      "Iteration 4, loss = 4.80936947\n",
      "Validation score: 0.271028\n",
      "Iteration 5, loss = 4.52091850\n",
      "Validation score: 0.308411\n",
      "Iteration 6, loss = 4.27048603\n",
      "Validation score: 0.355140\n",
      "Iteration 7, loss = 4.04116769\n",
      "Validation score: 0.359813\n",
      "Iteration 8, loss = 3.83486425\n",
      "Validation score: 0.369159\n",
      "Iteration 9, loss = 3.63172694\n",
      "Validation score: 0.369159\n",
      "Iteration 10, loss = 3.41922529\n",
      "Validation score: 0.378505\n",
      "Iteration 11, loss = 3.22320663\n",
      "Validation score: 0.425234\n",
      "Iteration 12, loss = 3.01847878\n",
      "Validation score: 0.397196\n",
      "Iteration 13, loss = 2.82234004\n",
      "Validation score: 0.373832\n",
      "Iteration 14, loss = 2.62190908\n",
      "Validation score: 0.387850\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 3), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(2048, 1024, 512), score=0.39738195418419825, total=  38.1s\n",
      "[CV] vect__ngram_range=(1, 1), vect__max_features=25000, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 8.99667692\n",
      "Validation score: 0.369159\n",
      "Iteration 2, loss = 5.42351274\n",
      "Validation score: 0.345794\n",
      "Iteration 3, loss = 3.84852166\n",
      "Validation score: 0.457944\n",
      "Iteration 4, loss = 2.77276046\n",
      "Validation score: 0.462617\n",
      "Iteration 5, loss = 1.80882118\n",
      "Validation score: 0.514019\n",
      "Iteration 6, loss = 1.06448624\n",
      "Validation score: 0.537383\n",
      "Iteration 7, loss = 0.57246827\n",
      "Validation score: 0.509346\n",
      "Iteration 8, loss = 0.28013286\n",
      "Validation score: 0.523364\n",
      "Iteration 9, loss = 0.13483110\n",
      "Validation score: 0.500000\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 1), vect__max_features=25000, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024), score=0.4642356241234222, total= 7.7min\n",
      "[CV] vect__ngram_range=(1, 1), vect__max_features=25000, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 9.05446393\n",
      "Validation score: 0.074766\n",
      "Iteration 2, loss = 5.38888513\n",
      "Validation score: 0.191589\n",
      "Iteration 3, loss = 3.85594053\n",
      "Validation score: 0.345794\n",
      "Iteration 4, loss = 2.73910992\n",
      "Validation score: 0.392523\n",
      "Iteration 5, loss = 1.75976211\n",
      "Validation score: 0.429907\n",
      "Iteration 6, loss = 0.99567806\n",
      "Validation score: 0.485981\n",
      "Iteration 7, loss = 0.51896178\n",
      "Validation score: 0.453271\n",
      "Iteration 8, loss = 0.25408378\n",
      "Validation score: 0.476636\n",
      "Iteration 9, loss = 0.13033369\n",
      "Validation score: 0.457944\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 1), vect__max_features=25000, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024), score=0.48200093501636276, total= 7.7min\n",
      "[CV] vect__ngram_range=(1, 2), vect__max_features=2500, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 8.30861849\n",
      "Validation score: 0.037383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 5.18235452\n",
      "Validation score: 0.294393\n",
      "Iteration 3, loss = 4.06946609\n",
      "Validation score: 0.378505\n",
      "Iteration 4, loss = 3.42844916\n",
      "Validation score: 0.387850\n",
      "Iteration 5, loss = 2.83040279\n",
      "Validation score: 0.397196\n",
      "Iteration 6, loss = 2.23557077\n",
      "Validation score: 0.406542\n",
      "Iteration 7, loss = 1.68436383\n",
      "Validation score: 0.415888\n",
      "Iteration 8, loss = 1.21256868\n",
      "Validation score: 0.425234\n",
      "Iteration 9, loss = 0.83616224\n",
      "Validation score: 0.429907\n",
      "Iteration 10, loss = 0.55335321\n",
      "Validation score: 0.425234\n",
      "Iteration 11, loss = 0.35439361\n",
      "Validation score: 0.425234\n",
      "Iteration 12, loss = 0.22415836\n",
      "Validation score: 0.420561\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 2), vect__max_features=2500, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024), score=0.45114539504441326, total= 1.8min\n",
      "[CV] vect__ngram_range=(1, 2), vect__max_features=2500, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 8.29096786\n",
      "Validation score: 0.088785\n",
      "Iteration 2, loss = 5.19491350\n",
      "Validation score: 0.261682\n",
      "Iteration 3, loss = 4.11581947\n",
      "Validation score: 0.275701\n",
      "Iteration 4, loss = 3.43029696\n",
      "Validation score: 0.299065\n",
      "Iteration 5, loss = 2.81647755\n",
      "Validation score: 0.355140\n",
      "Iteration 6, loss = 2.22092875\n",
      "Validation score: 0.378505\n",
      "Iteration 7, loss = 1.66782902\n",
      "Validation score: 0.397196\n",
      "Iteration 8, loss = 1.18719900\n",
      "Validation score: 0.406542\n",
      "Iteration 9, loss = 0.82158887\n",
      "Validation score: 0.401869\n",
      "Iteration 10, loss = 0.53765167\n",
      "Validation score: 0.406542\n",
      "Iteration 11, loss = 0.36406586\n",
      "Validation score: 0.397196\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 2), vect__max_features=2500, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024), score=0.4628330995792426, total= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 81.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 7.05014292\n",
      "Validation score: 0.336449\n",
      "Iteration 2, loss = 3.86136451\n",
      "Validation score: 0.411215\n",
      "Iteration 3, loss = 2.74549708\n",
      "Validation score: 0.478972\n",
      "Iteration 4, loss = 1.72115282\n",
      "Validation score: 0.495327\n",
      "Iteration 5, loss = 0.92737540\n",
      "Validation score: 0.514019\n",
      "Iteration 6, loss = 0.45040736\n",
      "Validation score: 0.511682\n",
      "Iteration 7, loss = 0.20937624\n",
      "Validation score: 0.511682\n",
      "Iteration 8, loss = 0.11273277\n",
      "Validation score: 0.500000\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "rs_clf = RandomizedSearchCV(clf, rs_parameters, cv=2, n_iter=10, n_jobs=1, verbose=10, random_state=1)\n",
    "start = time.time()\n",
    "rs_clf = rs_clf.fit(X_train, encoded_y_train)\n",
    "rs_processing_time = (time.time() - start) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47639083683964467\n"
     ]
    }
   ],
   "source": [
    "best_score = rs_clf.best_score_\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vect__ngram_range': (1, 4), 'vect__max_features': 7500, 'tfidf__use_idf': True, 'clf__hidden_layer_sizes': (4096, 1024)}\n"
     ]
    }
   ],
   "source": [
    "best_params = rs_clf.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cv': 2, 'error_score': 'raise', 'estimator__memory': None, 'estimator__steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
      "       warm_start=False))], 'estimator__vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'estimator__tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'estimator__clf': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
      "       warm_start=False), 'estimator__vect__analyzer': 'word', 'estimator__vect__binary': False, 'estimator__vect__decode_error': 'strict', 'estimator__vect__dtype': <class 'numpy.int64'>, 'estimator__vect__encoding': 'utf-8', 'estimator__vect__input': 'content', 'estimator__vect__lowercase': True, 'estimator__vect__max_df': 1.0, 'estimator__vect__max_features': None, 'estimator__vect__min_df': 1, 'estimator__vect__ngram_range': (1, 1), 'estimator__vect__preprocessor': None, 'estimator__vect__stop_words': None, 'estimator__vect__strip_accents': None, 'estimator__vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'estimator__vect__tokenizer': None, 'estimator__vect__vocabulary': None, 'estimator__tfidf__norm': 'l2', 'estimator__tfidf__smooth_idf': True, 'estimator__tfidf__sublinear_tf': False, 'estimator__tfidf__use_idf': True, 'estimator__clf__activation': 'relu', 'estimator__clf__alpha': 0.0001, 'estimator__clf__batch_size': 'auto', 'estimator__clf__beta_1': 0.9, 'estimator__clf__beta_2': 0.999, 'estimator__clf__early_stopping': True, 'estimator__clf__epsilon': 1e-08, 'estimator__clf__hidden_layer_sizes': (100,), 'estimator__clf__learning_rate': 'constant', 'estimator__clf__learning_rate_init': 0.001, 'estimator__clf__max_iter': 200, 'estimator__clf__momentum': 0.9, 'estimator__clf__nesterovs_momentum': True, 'estimator__clf__power_t': 0.5, 'estimator__clf__random_state': 1, 'estimator__clf__shuffle': True, 'estimator__clf__solver': 'adam', 'estimator__clf__tol': 0.0001, 'estimator__clf__validation_fraction': 0.1, 'estimator__clf__verbose': True, 'estimator__clf__warm_start': False, 'estimator': Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip...\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
      "       warm_start=False))]), 'fit_params': None, 'iid': True, 'n_iter': 10, 'n_jobs': 1, 'param_distributions': {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)], 'vect__max_features': (100000, 50000, 25000, 10000, 7500, 5000, 2500, 1000, 500, 300, 100), 'tfidf__use_idf': (True, False), 'clf__hidden_layer_sizes': ((2048, 1024), (2048, 512), (1024, 512), (512, 128), (4096, 1024), (4096, 512), (2048, 1024, 512), (1024, 512, 128))}, 'pre_dispatch': '2*n_jobs', 'random_state': 1, 'refit': True, 'return_train_score': 'warn', 'scoring': None, 'verbose': 10}\n"
     ]
    }
   ],
   "source": [
    "rs_clf_params = rs_clf.get_params()\n",
    "print(rs_clf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict \n",
    "rs_predicted = rs_clf.predict(X_test)\n",
    "#print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7979825433007968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# precision is a measure of result relevancy\n",
    "rs_precision = precision_score(encoded_y_test, rs_predicted, average='samples')\n",
    "print(rs_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.707436631792234\n"
     ]
    }
   ],
   "source": [
    "# recall is a measure of how many truly relevant results are returned\n",
    "rs_recall = recall_score(encoded_y_test, rs_predicted, average='samples')  \n",
    "print(rs_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7282131241187879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# F1 score is a weighted average of the precision and recall\n",
    "rs_f1 = f1_score(encoded_y_test, rs_predicted, average='samples') \n",
    "print(rs_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       173\n",
      "          1       1.00      0.49      0.66       265\n",
      "          2       0.00      0.00      0.00       224\n",
      "          3       0.00      0.00      0.00       173\n",
      "          4       0.64      0.50      0.57      2590\n",
      "          5       0.00      0.00      0.00       161\n",
      "          6       0.87      0.93      0.90     11321\n",
      "          7       0.69      0.42      0.52      2248\n",
      "          8       0.67      0.03      0.05       399\n",
      "          9       0.80      0.38      0.51      1286\n",
      "         10       0.85      0.17      0.28       770\n",
      "         11       0.00      0.00      0.00       202\n",
      "         12       1.00      0.13      0.23       542\n",
      "         13       0.71      0.39      0.50      2301\n",
      "         14       0.86      0.44      0.59      1477\n",
      "         15       0.94      0.50      0.65       606\n",
      "         16       0.83      0.77      0.80      7142\n",
      "         17       0.84      0.76      0.80      4105\n",
      "         18       0.75      0.35      0.47      1204\n",
      "         19       0.80      0.27      0.41      1052\n",
      "         20       1.00      0.06      0.11       455\n",
      "\n",
      "avg / total       0.80      0.64      0.68     38696\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(encoded_y_test, rs_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ergebnisse in Dateien speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = '../MLP'\n",
    "if not os.path.exists(output):\n",
    "    os.makedirs(output)\n",
    "    \n",
    "timestamp = time.strftime('%Y-%m-%d_%H.%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED:\n",
      "('histoire_t',)\n",
      "TRUE:\n",
      "['histoire_t', 'époque moderne_t', 'moyen âge_t']\n"
     ]
    }
   ],
   "source": [
    "# write real labels and predictions to file\n",
    "\n",
    "inverse_prediction = label_encoder.inverse_transform(rs_predicted)\n",
    "print('PREDICTED:')\n",
    "print(inverse_prediction[0])\n",
    "print('TRUE:')\n",
    "print(y_test[0])\n",
    "\n",
    "with open(output+'/MLP_themes_only_rs_predictions_%s.txt' % timestamp,\"w+\", encoding=\"utf8\") as preds:\n",
    "    preds.write(\"Predictions from classification with Multi-Layer-Perzeptron and vectorization in scikit-learn (themes only):\\n\\n\")\n",
    "    for ident, label, pred in zip(z_test, y_test, inverse_prediction):\n",
    "        label = sorted(label)\n",
    "        pred = sorted(pred)\n",
    "        preds.write(ident)\n",
    "        preds.write('\\n')\n",
    "        preds.write('TRUE: ')\n",
    "        for element in label:\n",
    "            preds.write('%s, ' % element)\n",
    "        preds.write('\\n')\n",
    "        preds.write('PRED: ')\n",
    "        for element in pred:\n",
    "            preds.write('%s, ' % element)\n",
    "        preds.write('\\n')\n",
    "        preds.write('\\n*********************\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write parameters and scores to file\n",
    "\n",
    "with open(output+'/MLP_themes_only_rs_params_%s.txt' % timestamp,\"w+\", encoding=\"utf8\") as params:\n",
    "    params.write(\"Parameters for classification with Multi-Layer-Perceptron and vectorization in scikit-learn from randomized search (themes only):\")\n",
    "    params.write(\"\\nprocessing_time: %s\" % rs_processing_time)\n",
    "    params.write(\"\\nparams:\")\n",
    "    for key, value in rs_clf_params.items():\n",
    "        params.write(\"\\n%s: %s\" % (key, value))\n",
    "    params.write(\"\\nbest params:\")\n",
    "    for key, value in best_params.items():\n",
    "        params.write(\"\\n%s: %s\" % (key, value))\n",
    "    params.write(\"\\nbest_score: %s\" % best_score)\n",
    "    params.write(\"\\nprecision: %s\" % rs_precision)\n",
    "    params.write(\"\\nrecall: %s\" % rs_recall)\n",
    "    params.write(\"\\nf1-score: %s\" % rs_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0    1183.272370    291.542017         2.687812        0.031244   \n",
      "1     115.247501     25.221695         1.953352        0.031260   \n",
      "2      37.715464      6.274170         1.750210        0.015628   \n",
      "3      96.042729     10.954436         1.070457        0.007830   \n",
      "4     168.191975      0.093761         1.195456        0.039067   \n",
      "5     228.214098     18.831045         2.617484        0.007812   \n",
      "6       4.125466      0.015626         0.562566        0.015629   \n",
      "7      25.409124     11.345033         1.429849        0.007813   \n",
      "8     459.773627      0.078133         1.343913        0.000001   \n",
      "9     100.081618      4.445804         1.531425        0.015628   \n",
      "\n",
      "  param_vect__ngram_range param_vect__max_features param_tfidf__use_idf  \\\n",
      "0                  (1, 3)                    50000                False   \n",
      "1                  (1, 3)                     2500                False   \n",
      "2                  (1, 4)                     5000                False   \n",
      "3                  (1, 2)                    10000                 True   \n",
      "4                  (1, 2)                    25000                 True   \n",
      "5                  (1, 4)                     7500                 True   \n",
      "6                  (1, 1)                      500                False   \n",
      "7                  (1, 3)                      500                False   \n",
      "8                  (1, 1)                    25000                 True   \n",
      "9                  (1, 2)                     2500                 True   \n",
      "\n",
      "  param_clf__hidden_layer_sizes  \\\n",
      "0                  (4096, 1024)   \n",
      "1                  (4096, 1024)   \n",
      "2                    (512, 128)   \n",
      "3                   (1024, 512)   \n",
      "4                   (1024, 512)   \n",
      "5                  (4096, 1024)   \n",
      "6              (1024, 512, 128)   \n",
      "7             (2048, 1024, 512)   \n",
      "8                  (4096, 1024)   \n",
      "9                  (4096, 1024)   \n",
      "\n",
      "                                              params  split0_test_score  \\\n",
      "0  {'vect__ngram_range': (1, 3), 'vect__max_featu...           0.455820   \n",
      "1  {'vect__ngram_range': (1, 3), 'vect__max_featu...           0.408602   \n",
      "2  {'vect__ngram_range': (1, 4), 'vect__max_featu...           0.410005   \n",
      "3  {'vect__ngram_range': (1, 2), 'vect__max_featu...           0.455353   \n",
      "4  {'vect__ngram_range': (1, 2), 'vect__max_featu...           0.454418   \n",
      "5  {'vect__ngram_range': (1, 4), 'vect__max_featu...           0.467508   \n",
      "6  {'vect__ngram_range': (1, 1), 'vect__max_featu...           0.309490   \n",
      "7  {'vect__ngram_range': (1, 3), 'vect__max_featu...           0.258065   \n",
      "8  {'vect__ngram_range': (1, 1), 'vect__max_featu...           0.464236   \n",
      "9  {'vect__ngram_range': (1, 2), 'vect__max_featu...           0.451145   \n",
      "\n",
      "   split1_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
      "0           0.469846         0.462833        0.007013                5   \n",
      "1           0.455353         0.431978        0.023375                7   \n",
      "2           0.452548         0.431276        0.021272                8   \n",
      "3           0.474521         0.464937        0.009584                3   \n",
      "4           0.471716         0.463067        0.008649                4   \n",
      "5           0.485273         0.476391        0.008883                1   \n",
      "6           0.314165         0.311828        0.002338               10   \n",
      "7           0.397382         0.327723        0.069659                9   \n",
      "8           0.482001         0.473118        0.008883                2   \n",
      "9           0.462833         0.456989        0.005844                6   \n",
      "\n",
      "   split0_train_score  split1_train_score  mean_train_score  std_train_score  \n",
      "0            0.786349            0.947172          0.866760         0.080411  \n",
      "1            0.547920            0.885460          0.716690         0.168770  \n",
      "2            0.503039            0.738195          0.620617         0.117578  \n",
      "3            0.944367            0.886396          0.915381         0.028986  \n",
      "4            0.829827            0.856475          0.843151         0.013324  \n",
      "5            0.948107            0.889201          0.918654         0.029453  \n",
      "6            0.314165            0.309490          0.311828         0.002338  \n",
      "7            0.260870            0.463768          0.362319         0.101449  \n",
      "8            0.816269            0.836372          0.826321         0.010051  \n",
      "9            0.830295            0.798504          0.814399         0.015895  \n"
     ]
    }
   ],
   "source": [
    "results = rs_clf.cv_results_\n",
    "df = pd.DataFrame(data=results)\n",
    "print(df)\n",
    "df.to_csv(output+'/MLP_themes_only_rs_results_%s.csv' % timestamp, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
