{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textklassifikation mit Vektorisierung in scikit-learn und MLPClassifier \n",
    "Labels (Themen und Disziplinen) sind auf höchste Hierarchieebene reduziert (reduced_labels)\n",
    "\n",
    "Autorin: Maria Hartmann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # module to one-hot-encode the labels\n",
    "from sklearn.pipeline import Pipeline # assemples transormers \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # module to transform a count matrix to a normalized tf-idf representation\n",
    "from sklearn.neural_network import MLPClassifier # MultiLayerPerceptron classifier \n",
    "from sklearn.model_selection import RandomizedSearchCV # module for paramter optimization\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "np.random.seed(7) # fix random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einlesen des Trainings- und Testdatensatzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = '../Datasets/reduced_labels_trainset.csv' \n",
    "testset = '../Datasets/reduced_labels_testset.csv' \n",
    "\n",
    "trainset_csv = pd.read_csv(trainset, delimiter=';')\n",
    "X_train = trainset_csv['text'].values\n",
    "y_train = trainset_csv['classes'].values\n",
    "z_train = trainset_csv['filename'].values\n",
    "\n",
    "testset_csv = pd.read_csv(testset, delimiter=';')\n",
    "X_test = testset_csv['text'].values\n",
    "y_test = testset_csv['classes'].values\n",
    "z_test = testset_csv['filename'].values\n",
    "\n",
    "# Splitten der Labels pro Blogbeitrag\n",
    "y_train = [e.split(', ') for e in y_train]\n",
    "y_test = [e.split(', ') for e in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archivalia_575.txt\n",
      "['pluridisciplinarité_d', 'épistémologie et méthodes_t', 'histoire_t', 'histoire et archéologie_d']\n",
      "diese titelformulierung der hab in ihrer handschriftendatenbank ist besonders sinnreich http diglib hab de db mss list ms id aug f mit digitalisat ich möchte nicht wissen wieviele forscher sich auf die fehlanzeige des verlinkten opacs der grundsätzlich nichts zu den handschriften ausspuckt während die ältere dokumentation funktioniert verlassen und so unnötig rechercheaufwand betreiben müssen kürzt man die signatur findet man den hinweis auf die münchner schedel ausstellung welt des wissens bevor ich das tat erkannte ich hartmann schedels ziemlich unverwechselbare schriftzüge und bemerkte bei einem blick auf die münchner digitalisate den typischen signaturzettel auf dem titel das bemerkenswerte familienbuch der nürnberger familie grabner erscheint in der liste von schedels büchern bei stauber https archive org stream dieschedelscheb hartgoog page n mode up die handschriftendatenbank ist denkbar benutzerunfreundlich fürs browsen nicht digitalisierter handschriften verwende ich in unkenntnis eines besseren zugriffs die url von nutzen ist allerdings die volltextsuche die trunkierung mit ermöglicht\n"
     ]
    }
   ],
   "source": [
    "print(z_train[0])\n",
    "print(y_train[0])\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-hot-Kodierung der Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# k-hot-encode labels mit MultiLabelBinarizer\n",
    "label_encoder = MultiLabelBinarizer()\n",
    "encoded_y_train = label_encoder.fit_transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "print(encoded_y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "0 administration publique et développement_d\n",
      "1 anthropologie_t\n",
      "2 arts et humanités_d\n",
      "3 asie_t\n",
      "4 bibliothéconomie_d\n",
      "5 droit_t\n",
      "6 ethnologie_t\n",
      "7 europe_t\n",
      "8 géographie_t\n",
      "9 histoire et archéologie_d\n",
      "10 histoire_t\n",
      "11 information_t\n",
      "12 langage_t\n",
      "13 langue et linguistique_d\n",
      "14 littérature_d\n",
      "15 moyen âge_t\n",
      "16 pensée_t\n",
      "17 pluridisciplinarité_d\n",
      "18 psychisme_t\n",
      "19 psychologie_d\n",
      "20 religions_t\n",
      "21 représentations_t\n",
      "22 sciences de l'information et de la communication_d\n",
      "23 sciences de la santé et de la santé publique_d\n",
      "24 sciences politiques_d\n",
      "25 sociologie et anthropologie_d\n",
      "26 sociologie_t\n",
      "27 travail social et politique sociale_d\n",
      "28 éducation_d\n",
      "29 éducation_t\n",
      "30 épistémologie et méthodes_t\n",
      "31 époque contemporaine_t\n",
      "32 époque moderne_t\n",
      "33 études des sciences_t\n",
      "34 études du politique_t\n"
     ]
    }
   ],
   "source": [
    "print(len(label_encoder.classes_))\n",
    "for i, element in enumerate(label_encoder.classes_):\n",
    "    print(i, element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vektorisierung und Klassifikation der Daten mit scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "stopwords = open('../Preprocessing/filtered_words.txt', 'r', encoding='utf-8').read().splitlines()\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1), max_features=max_features, stop_words=stopwords)\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"text_clf = Pipeline([#('vect', CountVectorizer(ngram_range=(1,4), max_df=0.9, min_df=0.01)),#min_df=0.0 auf min_df=0.01 geändert\\n                     ('vect', CountVectorizer(ngram_range=(1,4), max_features=max_features)),\\n                     ('tfidf', TfidfTransformer(use_idf=True)),\\n                     ('clf', MLPClassifier(hidden_layer_sizes=(1024,512), max_iter=500, validation_fraction=0.1, early_stopping=True, verbose=True, random_state=1)),\\n                    ])\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first try with best params for vect and tfidf from kNN classification\n",
    "\"\"\"text_clf = Pipeline([#('vect', CountVectorizer(ngram_range=(1,4), max_df=0.9, min_df=0.01)),#min_df=0.0 auf min_df=0.01 geändert\n",
    "                     ('vect', CountVectorizer(ngram_range=(1,4), max_features=max_features)),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                     ('clf', MLPClassifier(hidden_layer_sizes=(1024,512), max_iter=500, validation_fraction=0.1, early_stopping=True, verbose=True, random_state=1)),\n",
    "                    ])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', vectorizer), \n",
    "                     ('tfidf', tfidf_transformer),\n",
    "                     ('clf', MLPClassifier(hidden_layer_sizes=(4096, 1024), tol=0.0001, early_stopping=True, validation_fraction=0.1, verbose=True, random_state=1))\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 7.71963544\n",
      "Validation score: 0.417884\n",
      "Iteration 2, loss = 3.51733982\n",
      "Validation score: 0.542373\n",
      "Iteration 3, loss = 1.53049433\n",
      "Validation score: 0.548802\n",
      "Iteration 4, loss = 0.61838623\n",
      "Validation score: 0.556400\n",
      "Iteration 5, loss = 0.31824075\n",
      "Validation score: 0.552893\n",
      "Iteration 6, loss = 0.23207093\n",
      "Validation score: 0.537697\n",
      "Iteration 7, loss = 0.21266148\n",
      "Validation score: 0.534775\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "start = time.time()\n",
    "text_clf = text_clf.fit(X_train, encoded_y_train)\n",
    "processing_time = (time.time() - start) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=['und', 'die', 'der'], strip_accents=None,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(4096, 1024), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
      "       warm_start=False))], 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=['und', 'die', 'der'], strip_accents=None,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(4096, 1024), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
      "       warm_start=False), 'vect__analyzer': 'word', 'vect__binary': False, 'vect__decode_error': 'strict', 'vect__dtype': <class 'numpy.int64'>, 'vect__encoding': 'utf-8', 'vect__input': 'content', 'vect__lowercase': True, 'vect__max_df': 1.0, 'vect__max_features': 10000, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': ['und', 'die', 'der'], 'vect__strip_accents': None, 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__tokenizer': None, 'vect__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__activation': 'relu', 'clf__alpha': 0.0001, 'clf__batch_size': 'auto', 'clf__beta_1': 0.9, 'clf__beta_2': 0.999, 'clf__early_stopping': True, 'clf__epsilon': 1e-08, 'clf__hidden_layer_sizes': (4096, 1024), 'clf__learning_rate': 'constant', 'clf__learning_rate_init': 0.001, 'clf__max_iter': 200, 'clf__momentum': 0.9, 'clf__nesterovs_momentum': True, 'clf__power_t': 0.5, 'clf__random_state': 1, 'clf__shuffle': True, 'clf__solver': 'adam', 'clf__tol': 0.0001, 'clf__validation_fraction': 0.1, 'clf__verbose': True, 'clf__warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "clf_params = text_clf.get_params()\n",
    "print(clf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predicted = text_clf.predict(X_test)\n",
    "#predicted_proba = text_clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8596379037823638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# precision is a measure of result relevancy\n",
    "precision = precision_score(encoded_y_test, predicted, average='samples')\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8138036428534324\n"
     ]
    }
   ],
   "source": [
    "# recall is a measure of how many truly relevant results are returned\n",
    "recall = recall_score(encoded_y_test, predicted, average='samples')  \n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8247182393457454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# F1 score is a weighted average of the precision and recall\n",
    "f1 = f1_score(encoded_y_test, predicted, average='samples') \n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = '../MLP'\n",
    "if not os.path.exists(output):\n",
    "    os.makedirs(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#with open(output+\\'/MLP_reduced_labels_first_params.txt\\',\"w+\", encoding=\"utf8\") as params:\\nwith open(output+\\'/MLP_reduced_labels_first_params_max_features.txt\\',\"w+\", encoding=\"utf8\") as params:\\n    params.write(\"First parameters for classification with MLP (reduced labels):\")\\n    params.write(\"\\nprocessing_time: %s\" % processing_time)\\n    for key, value in clf_params.items():\\n        params.write(\"\\n%s: %s\" % (key, value))\\n    params.write(\"\\nactivation function output layer: %s\" % text_clf.named_steps.clf.out_activation_)    \\n    params.write(\"\\nprecision: %s\" % precision)\\n    params.write(\"\\nrecall: %s\" % recall)\\n    params.write(\"\\nf1-score: %s\" % f1)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write first parameters and scores to file\n",
    "\"\"\"\n",
    "#with open(output+'/MLP_reduced_labels_first_params.txt',\"w+\", encoding=\"utf8\") as params:\n",
    "with open(output+'/MLP_reduced_labels_first_params_max_features.txt',\"w+\", encoding=\"utf8\") as params:\n",
    "    params.write(\"First parameters for classification with MLP (reduced labels):\")\n",
    "    params.write(\"\\nprocessing_time: %s\" % processing_time)\n",
    "    for key, value in clf_params.items():\n",
    "        params.write(\"\\n%s: %s\" % (key, value))\n",
    "    params.write(\"\\nactivation function output layer: %s\" % text_clf.named_steps.clf.out_activation_)    \n",
    "    params.write(\"\\nprecision: %s\" % precision)\n",
    "    params.write(\"\\nrecall: %s\" % recall)\n",
    "    params.write(\"\\nf1-score: %s\" % f1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write parameters and scores to file\n",
    "\n",
    "with open(output+'/MLP_reduced_labels_params.txt',\"a\", encoding=\"utf8\") as params:\n",
    "    params.write(\"\\n*********************************************************************************************\")\n",
    "    params.write(\"\\nParameters for classification with MLP (reduced labels):\")\n",
    "    params.write(\"\\n*********************************************************************************************\")\n",
    "    params.write(\"\\n%s\" % text_clf.named_steps.vect)\n",
    "    params.write(\"\\n%s\" % text_clf.named_steps.tfidf)\n",
    "    params.write(\"\\n%s\" % text_clf.named_steps.clf)\n",
    "    #for key, value in clf_params.items():\n",
    "        #params.write(\"\\n%s: %s\" % (key, value))\n",
    "    params.write(\"\\nclasses: %s\" % text_clf.named_steps.clf.n_outputs_)\n",
    "    params.write(\"\\nlayers: %s\" % text_clf.named_steps.clf.n_layers_)\n",
    "    params.write(\"\\nactivation function output layer: %s\" % text_clf.named_steps.clf.out_activation_) \n",
    "    params.write(\"\\nepochs: %s\" % text_clf.named_steps.clf.n_iter_)\n",
    "    params.write(\"\\nprocessing time: %s\" % processing_time)\n",
    "    params.write(\"\\nSCORES:\")\n",
    "    params.write(\"\\nprecision: %s\" % precision)\n",
    "    params.write(\"\\nrecall: %s\" % recall)\n",
    "    params.write(\"\\nf1-score: %s\" % f1)\n",
    "    params.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED:\n",
      "('histoire et archéologie_d', 'histoire_t', 'pluridisciplinarité_d', 'épistémologie et méthodes_t')\n",
      "TRUE:\n",
      "['pluridisciplinarité_d', 'épistémologie et méthodes_t', 'histoire_t', 'histoire et archéologie_d']\n"
     ]
    }
   ],
   "source": [
    "# write real labels and predictions to file\n",
    "\n",
    "inverse_prediction = label_encoder.inverse_transform(predicted)\n",
    "print('PREDICTED:')\n",
    "print(inverse_prediction[0])\n",
    "print('TRUE:')\n",
    "print(y_test[0])\n",
    "\n",
    "with open(output+'/MLP_reduced_labels_predictions.txt',\"w+\", encoding=\"utf8\") as preds:\n",
    "    preds.write(\"Predictions from classification with Multi-Layer-Perzeptron and vectorization in scikit-learn (reduced labels):\\n\\n\")\n",
    "    for ident, label, pred in zip(z_test, y_test, inverse_prediction):\n",
    "        label = sorted(label)\n",
    "        pred = sorted(pred)\n",
    "        preds.write(ident)\n",
    "        preds.write('\\n')\n",
    "        preds.write('TRUE: ')\n",
    "        for element in label:\n",
    "            preds.write('%s, ' % element)\n",
    "        preds.write('\\n')\n",
    "        preds.write('PRED: ')\n",
    "        for element in pred:\n",
    "            preds.write('%s, ' % element)\n",
    "        preds.write('\\n')\n",
    "        preds.write('\\n*********************\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speicherung der vektorisierten Textdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17109\n",
      "archivalia.hypotheses.org/575\n"
     ]
    }
   ],
   "source": [
    "z_train = [e.replace('.txt', '') for e in z_train]\n",
    "z_test = [e.replace('.txt', '') for e in z_test]\n",
    "ident_train = [e.replace('_', '.hypotheses.org/') for e in z_train]\n",
    "ident_test = [e.replace('_', '.hypotheses.org/') for e in z_test]\n",
    "\n",
    "print(len(ident_train))\n",
    "print(ident_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17109, 10000)\n"
     ]
    }
   ],
   "source": [
    "# vectorize textdata\n",
    "train_vect = vectorizer.transform(X_train)\n",
    "train_tfidf = tfidf_transformer.transform(train_vect)\n",
    "print(train_tfidf.shape)\n",
    "\n",
    "test_vect = vectorizer.transform(X_test)\n",
    "test_tfidf = tfidf_transformer.transform(test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<17109x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2386946 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(test_tfidf))\n",
    "train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filename, classes, textvectors in csv file\n",
    "# trainset\n",
    "# speichert vektorisierten Text\n",
    "output_file_train = 'Datasets/reduced_labels_train_scikit-learn_sparse_matrix.npz'\n",
    "scipy.sparse.save_npz('../'+output_file_train, train_tfidf)\n",
    "\n",
    "# speichert filenames und classes\n",
    "with open('../Datasets/reduced_labels_train_idents_labels.csv', 'w', newline='', encoding=\"utf-8\") as traincsv:\n",
    "    train = csv.writer(traincsv, delimiter = \";\")\n",
    "    train.writerow([\"url\", \"classes\", \"filename\"])\n",
    "    \n",
    "    for ident, labels in zip(ident_train, y_train):\n",
    "        labellist = \", \".join(labels)\n",
    "        train.writerow([ident, labellist, output_file_train])\n",
    "\n",
    "# testset\n",
    "# speichert vektorisierten Text\n",
    "output_file_test = 'Datasets/reduced_labels_test_scikit-learn_sparse_matrix.npz'\n",
    "scipy.sparse.save_npz('../'+output_file_test, test_tfidf)\n",
    "\n",
    "# speichert filenames und classes\n",
    "with open('../Datasets/reduced_labels_test_idents_labels.csv', 'w', newline='', encoding=\"utf-8\") as testcsv:\n",
    "    test = csv.writer(testcsv, delimiter = \";\")\n",
    "    test.writerow([\"url\", \"classes\", \"filename\"])\n",
    "    \n",
    "    for ident, labels in zip(ident_test, y_test):\n",
    "        labellist = \", \".join(labels)\n",
    "        test.writerow([ident, labellist, output_file_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameteroptimierung mit Rastersuche (RandomizedSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', MLPClassifier(validation_fraction=0.1, early_stopping=True, verbose=True, random_state=1)),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter tuning with RandomSearch\n",
    "stopwords = open('../Preprocessing/filtered_words.txt', 'r', encoding='utf-8').read().splitlines()\n",
    "rs_parameters = {'vect__ngram_range': [(1,1),(1,2),(1,3),(1,4)], \n",
    "                 #'vect__max_df' : (0.7, 0.8, 0.85, 0.9, 0.95), #1.0 \n",
    "                 #'vect__min_df' : (0.01, 0.025, 0.05, 0.075, 0.1, 0.2), #0.0\n",
    "                 'vect__max_features': (100000,50000,25000,10000,7500,5000,2500,1000,500,300,100), \n",
    "                 'tfidf__use_idf': (True, False),\n",
    "                 'clf__hidden_layer_sizes': ((2048,1024),(2048,512),(1024,512),(512,128),(4096,1024),(4096,512),(2048,1024,512),(1024,512,128))\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "[CV] vect__ngram_range=(1, 3), vect__max_features=50000, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 15.12070958\n",
      "Validation score: 0.252336\n",
      "Iteration 2, loss = 9.24104634\n",
      "Validation score: 0.233645\n",
      "Iteration 3, loss = 7.09170742\n",
      "Validation score: 0.369159\n",
      "Iteration 4, loss = 5.57069004\n",
      "Validation score: 0.373832\n",
      "Iteration 5, loss = 4.31373701\n",
      "Validation score: 0.401869\n",
      "Iteration 6, loss = 3.07877318\n",
      "Validation score: 0.406542\n",
      "Iteration 7, loss = 1.97087511\n",
      "Validation score: 0.406542\n",
      "Iteration 8, loss = 1.13798802\n",
      "Validation score: 0.406542\n",
      "Iteration 9, loss = 0.62190408\n",
      "Validation score: 0.392523\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 3), vect__max_features=50000, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024), score=0.423562412342216, total=18.0min\n",
      "[CV] vect__ngram_range=(1, 3), vect__max_features=50000, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 18.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 15.01348098\n",
      "Validation score: 0.331776\n",
      "Iteration 2, loss = 9.08658896\n",
      "Validation score: 0.242991\n",
      "Iteration 3, loss = 6.95678728\n",
      "Validation score: 0.392523\n",
      "Iteration 4, loss = 5.43340642\n",
      "Validation score: 0.397196\n",
      "Iteration 5, loss = 4.20915201\n",
      "Validation score: 0.439252\n",
      "Iteration 6, loss = 3.06746001\n",
      "Validation score: 0.425234\n",
      "Iteration 7, loss = 2.04330463\n",
      "Validation score: 0.439252\n",
      "Iteration 8, loss = 1.22044929\n",
      "Validation score: 0.425234\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 3), vect__max_features=50000, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024), score=0.4123422159887798, total=16.1min\n",
      "[CV] vect__ngram_range=(1, 3), vect__max_features=2500, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 34.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 14.39995582\n",
      "Validation score: 0.228972\n",
      "Iteration 2, loss = 9.09420726\n",
      "Validation score: 0.182243\n",
      "Iteration 3, loss = 7.82584241\n",
      "Validation score: 0.303738\n",
      "Iteration 4, loss = 6.92446330\n",
      "Validation score: 0.359813\n",
      "Iteration 5, loss = 6.30585603\n",
      "Validation score: 0.378505\n",
      "Iteration 6, loss = 5.73371433\n",
      "Validation score: 0.359813\n",
      "Iteration 7, loss = 5.15573059\n",
      "Validation score: 0.392523\n",
      "Iteration 8, loss = 4.52645966\n",
      "Validation score: 0.392523\n",
      "Iteration 9, loss = 3.89004369\n",
      "Validation score: 0.383178\n",
      "Iteration 10, loss = 3.26583304\n",
      "Validation score: 0.397196\n",
      "Iteration 11, loss = 2.70945162\n",
      "Validation score: 0.378505\n",
      "Iteration 12, loss = 2.18720186\n",
      "Validation score: 0.387850\n",
      "Iteration 13, loss = 1.73991232\n",
      "Validation score: 0.397196\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 3), vect__max_features=2500, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024), score=0.3856942496493689, total= 2.4min\n",
      "[CV] vect__ngram_range=(1, 3), vect__max_features=2500, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 36.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 14.16998955\n",
      "Validation score: 0.042056\n",
      "Iteration 2, loss = 8.82424272\n",
      "Validation score: 0.214953\n",
      "Iteration 3, loss = 7.47789710\n",
      "Validation score: 0.228972\n",
      "Iteration 4, loss = 6.53883306\n",
      "Validation score: 0.317757\n",
      "Iteration 5, loss = 5.87674678\n",
      "Validation score: 0.350467\n",
      "Iteration 6, loss = 5.29483122\n",
      "Validation score: 0.345794\n",
      "Iteration 7, loss = 4.76255494\n",
      "Validation score: 0.341121\n",
      "Iteration 8, loss = 4.22638375\n",
      "Validation score: 0.350467\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 3), vect__max_features=2500, tfidf__use_idf=False, clf__hidden_layer_sizes=(4096, 1024), score=0.38242169237961665, total= 1.5min\n",
      "[CV] vect__ngram_range=(1, 4), vect__max_features=5000, tfidf__use_idf=False, clf__hidden_layer_sizes=(512, 128) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 38.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 22.47471053\n",
      "Validation score: 0.004673\n",
      "Iteration 2, loss = 15.84262625\n",
      "Validation score: 0.000000\n",
      "Iteration 3, loss = 10.03718067\n",
      "Validation score: 0.285047\n",
      "Iteration 4, loss = 9.13126051\n",
      "Validation score: 0.000000\n",
      "Iteration 5, loss = 8.59102419\n",
      "Validation score: 0.182243\n",
      "Iteration 6, loss = 8.19559289\n",
      "Validation score: 0.214953\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 4), vect__max_features=5000, tfidf__use_idf=False, clf__hidden_layer_sizes=(512, 128), score=0.3010752688172043, total=  21.7s\n",
      "[CV] vect__ngram_range=(1, 4), vect__max_features=5000, tfidf__use_idf=False, clf__hidden_layer_sizes=(512, 128) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 38.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 22.51899479\n",
      "Validation score: 0.000000\n",
      "Iteration 2, loss = 15.82219651\n",
      "Validation score: 0.004673\n",
      "Iteration 3, loss = 9.79089336\n",
      "Validation score: 0.214953\n",
      "Iteration 4, loss = 8.77919503\n",
      "Validation score: 0.004673\n",
      "Iteration 5, loss = 8.25845709\n",
      "Validation score: 0.023364\n",
      "Iteration 6, loss = 7.88158931\n",
      "Validation score: 0.219626\n",
      "Iteration 7, loss = 7.53577248\n",
      "Validation score: 0.191589\n",
      "Iteration 8, loss = 7.17905569\n",
      "Validation score: 0.336449\n",
      "Iteration 9, loss = 6.81807363\n",
      "Validation score: 0.327103\n",
      "Iteration 10, loss = 6.46259435\n",
      "Validation score: 0.341121\n",
      "Iteration 11, loss = 6.11634956\n",
      "Validation score: 0.355140\n",
      "Iteration 12, loss = 5.79120908\n",
      "Validation score: 0.378505\n",
      "Iteration 13, loss = 5.48997100\n",
      "Validation score: 0.401869\n",
      "Iteration 14, loss = 5.20591130\n",
      "Validation score: 0.401869\n",
      "Iteration 15, loss = 4.92926799\n",
      "Validation score: 0.429907\n",
      "Iteration 16, loss = 4.66506529\n",
      "Validation score: 0.429907\n",
      "Iteration 17, loss = 4.40371282\n",
      "Validation score: 0.434579\n",
      "Iteration 18, loss = 4.14660051\n",
      "Validation score: 0.425234\n",
      "Iteration 19, loss = 3.89265392\n",
      "Validation score: 0.411215\n",
      "Iteration 20, loss = 3.64909727\n",
      "Validation score: 0.406542\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 4), vect__max_features=5000, tfidf__use_idf=False, clf__hidden_layer_sizes=(512, 128), score=0.3964469378214119, total=  46.8s\n",
      "[CV] vect__ngram_range=(1, 2), vect__max_features=10000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 39.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 19.19010335\n",
      "Validation score: 0.000000\n",
      "Iteration 2, loss = 10.24073553\n",
      "Validation score: 0.032710\n",
      "Iteration 3, loss = 8.82108807\n",
      "Validation score: 0.074766\n",
      "Iteration 4, loss = 7.74096819\n",
      "Validation score: 0.257009\n",
      "Iteration 5, loss = 6.69332161\n",
      "Validation score: 0.327103\n",
      "Iteration 6, loss = 5.85154905\n",
      "Validation score: 0.345794\n",
      "Iteration 7, loss = 5.12273967\n",
      "Validation score: 0.345794\n",
      "Iteration 8, loss = 4.40320513\n",
      "Validation score: 0.359813\n",
      "Iteration 9, loss = 3.68015477\n",
      "Validation score: 0.397196\n",
      "Iteration 10, loss = 2.98556641\n",
      "Validation score: 0.397196\n",
      "Iteration 11, loss = 2.33673950\n",
      "Validation score: 0.411215\n",
      "Iteration 12, loss = 1.79069529\n",
      "Validation score: 0.415888\n",
      "Iteration 13, loss = 1.34002318\n",
      "Validation score: 0.411215\n",
      "Iteration 14, loss = 0.99009425\n",
      "Validation score: 0.411215\n",
      "Iteration 15, loss = 0.72697616\n",
      "Validation score: 0.415888\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 2), vect__max_features=10000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512), score=0.42964001870032725, total= 1.8min\n",
      "[CV] vect__ngram_range=(1, 2), vect__max_features=10000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 41.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 19.10933142\n",
      "Validation score: 0.014019\n",
      "Iteration 2, loss = 9.76014574\n",
      "Validation score: 0.224299\n",
      "Iteration 3, loss = 8.54804985\n",
      "Validation score: 0.070093\n",
      "Iteration 4, loss = 7.44903729\n",
      "Validation score: 0.233645\n",
      "Iteration 5, loss = 6.39800191\n",
      "Validation score: 0.341121\n",
      "Iteration 6, loss = 5.55204112\n",
      "Validation score: 0.373832\n",
      "Iteration 7, loss = 4.83752818\n",
      "Validation score: 0.387850\n",
      "Iteration 8, loss = 4.14838162\n",
      "Validation score: 0.397196\n",
      "Iteration 9, loss = 3.46762536\n",
      "Validation score: 0.401869\n",
      "Iteration 10, loss = 2.80204917\n",
      "Validation score: 0.397196\n",
      "Iteration 11, loss = 2.18942217\n",
      "Validation score: 0.401869\n",
      "Iteration 12, loss = 1.67278726\n",
      "Validation score: 0.392523\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 2), vect__max_features=10000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512), score=0.4053295932678822, total= 1.4min\n",
      "[CV] vect__ngram_range=(1, 2), vect__max_features=25000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 42.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 19.92482588\n",
      "Validation score: 0.000000\n",
      "Iteration 2, loss = 10.28714186\n",
      "Validation score: 0.056075\n",
      "Iteration 3, loss = 8.88558999\n",
      "Validation score: 0.107477\n",
      "Iteration 4, loss = 7.52675969\n",
      "Validation score: 0.289720\n",
      "Iteration 5, loss = 6.33764400\n",
      "Validation score: 0.378505\n",
      "Iteration 6, loss = 5.34705915\n",
      "Validation score: 0.383178\n",
      "Iteration 7, loss = 4.43916451\n",
      "Validation score: 0.401869\n",
      "Iteration 8, loss = 3.58418425\n",
      "Validation score: 0.406542\n",
      "Iteration 9, loss = 2.74534615\n",
      "Validation score: 0.443925\n",
      "Iteration 10, loss = 1.99240793\n",
      "Validation score: 0.429907\n",
      "Iteration 11, loss = 1.38593612\n",
      "Validation score: 0.448598\n",
      "Iteration 12, loss = 0.93951869\n",
      "Validation score: 0.425234\n",
      "Iteration 13, loss = 0.62652935\n",
      "Validation score: 0.429907\n",
      "Iteration 14, loss = 0.42594345\n",
      "Validation score: 0.429907\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 2), vect__max_features=25000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512), score=0.4268349696119682, total= 3.6min\n",
      "[CV] vect__ngram_range=(1, 2), vect__max_features=25000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 46.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 19.78341904\n",
      "Validation score: 0.014019\n",
      "Iteration 2, loss = 9.85799114\n",
      "Validation score: 0.060748\n",
      "Iteration 3, loss = 8.56935007\n",
      "Validation score: 0.084112\n",
      "Iteration 4, loss = 7.22868399\n",
      "Validation score: 0.308411\n",
      "Iteration 5, loss = 6.01023061\n",
      "Validation score: 0.373832\n",
      "Iteration 6, loss = 4.99303493\n",
      "Validation score: 0.392523\n",
      "Iteration 7, loss = 4.11787718\n",
      "Validation score: 0.420561\n",
      "Iteration 8, loss = 3.29667575\n",
      "Validation score: 0.439252\n",
      "Iteration 9, loss = 2.50458013\n",
      "Validation score: 0.462617\n",
      "Iteration 10, loss = 1.80793569\n",
      "Validation score: 0.457944\n",
      "Iteration 11, loss = 1.24191355\n",
      "Validation score: 0.453271\n",
      "Iteration 12, loss = 0.84497808\n",
      "Validation score: 0.420561\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 2), vect__max_features=25000, tfidf__use_idf=True, clf__hidden_layer_sizes=(1024, 512), score=0.4165497896213184, total= 3.2min\n",
      "[CV] vect__ngram_range=(1, 4), vect__max_features=7500, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 14.59425162\n",
      "Validation score: 0.182243\n",
      "Iteration 2, loss = 9.01815365\n",
      "Validation score: 0.275701\n",
      "Iteration 3, loss = 7.00854303\n",
      "Validation score: 0.350467\n",
      "Iteration 4, loss = 5.87561444\n",
      "Validation score: 0.387850\n",
      "Iteration 5, loss = 4.87596775\n",
      "Validation score: 0.397196\n",
      "Iteration 6, loss = 3.85926125\n",
      "Validation score: 0.411215\n",
      "Iteration 7, loss = 2.83295294\n",
      "Validation score: 0.415888\n",
      "Iteration 8, loss = 1.91274366\n",
      "Validation score: 0.406542\n",
      "Iteration 9, loss = 1.20388036\n",
      "Validation score: 0.397196\n",
      "Iteration 10, loss = 0.73031976\n",
      "Validation score: 0.397196\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 4), vect__max_features=7500, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024), score=0.42449742870500234, total= 4.0min\n",
      "[CV] vect__ngram_range=(1, 4), vect__max_features=7500, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 14.48195263\n",
      "Validation score: 0.014019\n",
      "Iteration 2, loss = 8.68344274\n",
      "Validation score: 0.331776\n",
      "Iteration 3, loss = 6.65026315\n",
      "Validation score: 0.425234\n",
      "Iteration 4, loss = 5.45140576\n",
      "Validation score: 0.434579\n",
      "Iteration 5, loss = 4.43584455\n",
      "Validation score: 0.453271\n",
      "Iteration 6, loss = 3.44243422\n",
      "Validation score: 0.462617\n",
      "Iteration 7, loss = 2.42821113\n",
      "Validation score: 0.462617\n",
      "Iteration 8, loss = 1.58286579\n",
      "Validation score: 0.448598\n",
      "Iteration 9, loss = 0.99274402\n",
      "Validation score: 0.434579\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 4), vect__max_features=7500, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024), score=0.42075736325385693, total= 3.6min\n",
      "[CV] vect__ngram_range=(1, 1), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(1024, 512, 128) \n",
      "Iteration 1, loss = 20.60084835\n",
      "Validation score: 0.000000\n",
      "Iteration 2, loss = 11.01804392\n",
      "Validation score: 0.051402\n",
      "Iteration 3, loss = 9.28087590\n",
      "Validation score: 0.000000\n",
      "Iteration 4, loss = 8.67678351\n",
      "Validation score: 0.210280\n",
      "Iteration 5, loss = 8.36203207\n",
      "Validation score: 0.154206\n",
      "Iteration 6, loss = 8.07949575\n",
      "Validation score: 0.177570\n",
      "Iteration 7, loss = 7.80813109\n",
      "Validation score: 0.177570\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 1), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(1024, 512, 128), score=0.21458625525946703, total=   7.2s\n",
      "[CV] vect__ngram_range=(1, 1), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(1024, 512, 128) \n",
      "Iteration 1, loss = 20.65699519\n",
      "Validation score: 0.000000\n",
      "Iteration 2, loss = 10.55952231\n",
      "Validation score: 0.294393\n",
      "Iteration 3, loss = 8.93997659\n",
      "Validation score: 0.000000\n",
      "Iteration 4, loss = 8.34167412\n",
      "Validation score: 0.079439\n",
      "Iteration 5, loss = 8.06192707\n",
      "Validation score: 0.107477\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 1), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(1024, 512, 128), score=0.30481533426834967, total=   5.6s\n",
      "[CV] vect__ngram_range=(1, 3), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(2048, 1024, 512) \n",
      "Iteration 1, loss = 15.38107542\n",
      "Validation score: 0.308411\n",
      "Iteration 2, loss = 9.42194198\n",
      "Validation score: 0.000000\n",
      "Iteration 3, loss = 8.54561874\n",
      "Validation score: 0.252336\n",
      "Iteration 4, loss = 8.11979476\n",
      "Validation score: 0.247664\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 3), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(2048, 1024, 512), score=0.3010752688172043, total=  18.9s\n",
      "[CV] vect__ngram_range=(1, 3), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(2048, 1024, 512) \n",
      "Iteration 1, loss = 15.32025103\n",
      "Validation score: 0.014019\n",
      "Iteration 2, loss = 8.94613266\n",
      "Validation score: 0.000000\n",
      "Iteration 3, loss = 8.16030358\n",
      "Validation score: 0.261682\n",
      "Iteration 4, loss = 7.70290914\n",
      "Validation score: 0.191589\n",
      "Iteration 5, loss = 7.29454368\n",
      "Validation score: 0.238318\n",
      "Iteration 6, loss = 6.91854462\n",
      "Validation score: 0.289720\n",
      "Iteration 7, loss = 6.60259714\n",
      "Validation score: 0.317757\n",
      "Iteration 8, loss = 6.32240534\n",
      "Validation score: 0.345794\n",
      "Iteration 9, loss = 6.06667792\n",
      "Validation score: 0.322430\n",
      "Iteration 10, loss = 5.76652411\n",
      "Validation score: 0.331776\n",
      "Iteration 11, loss = 5.51650451\n",
      "Validation score: 0.317757\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 3), vect__max_features=500, tfidf__use_idf=False, clf__hidden_layer_sizes=(2048, 1024, 512), score=0.3501636278634876, total=  38.6s\n",
      "[CV] vect__ngram_range=(1, 1), vect__max_features=25000, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 15.56304635\n",
      "Validation score: 0.046729\n",
      "Iteration 2, loss = 9.34263509\n",
      "Validation score: 0.135514\n",
      "Iteration 3, loss = 7.17339429\n",
      "Validation score: 0.392523\n",
      "Iteration 4, loss = 5.49064493\n",
      "Validation score: 0.392523\n",
      "Iteration 5, loss = 4.07376494\n",
      "Validation score: 0.406542\n",
      "Iteration 6, loss = 2.79570763\n",
      "Validation score: 0.443925\n",
      "Iteration 7, loss = 1.67218186\n",
      "Validation score: 0.448598\n",
      "Iteration 8, loss = 0.90310313\n",
      "Validation score: 0.425234\n",
      "Iteration 9, loss = 0.44648128\n",
      "Validation score: 0.434579\n",
      "Iteration 10, loss = 0.23634419\n",
      "Validation score: 0.411215\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 1), vect__max_features=25000, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024), score=0.42262739597942967, total=10.4min\n",
      "[CV] vect__ngram_range=(1, 1), vect__max_features=25000, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 15.33226129\n",
      "Validation score: 0.303738\n",
      "Iteration 2, loss = 9.08453687\n",
      "Validation score: 0.233645\n",
      "Iteration 3, loss = 6.89109537\n",
      "Validation score: 0.387850\n",
      "Iteration 4, loss = 5.25159836\n",
      "Validation score: 0.378505\n",
      "Iteration 5, loss = 3.87233517\n",
      "Validation score: 0.415888\n",
      "Iteration 6, loss = 2.67445103\n",
      "Validation score: 0.425234\n",
      "Iteration 7, loss = 1.68880772\n",
      "Validation score: 0.411215\n",
      "Iteration 8, loss = 0.95309926\n",
      "Validation score: 0.420561\n",
      "Iteration 9, loss = 0.52469896\n",
      "Validation score: 0.415888\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 1), vect__max_features=25000, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024), score=0.4170172978027116, total= 9.4min\n",
      "[CV] vect__ngram_range=(1, 2), vect__max_features=2500, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 14.35582389\n",
      "Validation score: 0.144860\n",
      "Iteration 2, loss = 8.86733544\n",
      "Validation score: 0.210280\n",
      "Iteration 3, loss = 7.35971899\n",
      "Validation score: 0.345794\n",
      "Iteration 4, loss = 6.39926244\n",
      "Validation score: 0.355140\n",
      "Iteration 5, loss = 5.64296979\n",
      "Validation score: 0.378505\n",
      "Iteration 6, loss = 4.88144221\n",
      "Validation score: 0.369159\n",
      "Iteration 7, loss = 4.09195100\n",
      "Validation score: 0.411215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 3.29933501\n",
      "Validation score: 0.397196\n",
      "Iteration 9, loss = 2.55264577\n",
      "Validation score: 0.415888\n",
      "Iteration 10, loss = 1.91188189\n",
      "Validation score: 0.411215\n",
      "Iteration 11, loss = 1.40030028\n",
      "Validation score: 0.406542\n",
      "Iteration 12, loss = 0.99164132\n",
      "Validation score: 0.392523\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 2), vect__max_features=2500, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024), score=0.40766713417484807, total= 2.2min\n",
      "[CV] vect__ngram_range=(1, 2), vect__max_features=2500, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024) \n",
      "Iteration 1, loss = 14.09429188\n",
      "Validation score: 0.032710\n",
      "Iteration 2, loss = 8.67543111\n",
      "Validation score: 0.214953\n",
      "Iteration 3, loss = 7.10329494\n",
      "Validation score: 0.299065\n",
      "Iteration 4, loss = 6.06989272\n",
      "Validation score: 0.336449\n",
      "Iteration 5, loss = 5.30147006\n",
      "Validation score: 0.355140\n",
      "Iteration 6, loss = 4.56967449\n",
      "Validation score: 0.359813\n",
      "Iteration 7, loss = 3.86378976\n",
      "Validation score: 0.369159\n",
      "Iteration 8, loss = 3.14945164\n",
      "Validation score: 0.383178\n",
      "Iteration 9, loss = 2.46419831\n",
      "Validation score: 0.387850\n",
      "Iteration 10, loss = 1.85799854\n",
      "Validation score: 0.397196\n",
      "Iteration 11, loss = 1.36346823\n",
      "Validation score: 0.392523\n",
      "Iteration 12, loss = 0.98829897\n",
      "Validation score: 0.401869\n",
      "Iteration 13, loss = 0.72529536\n",
      "Validation score: 0.392523\n",
      "Iteration 14, loss = 0.52464605\n",
      "Validation score: 0.373832\n",
      "Iteration 15, loss = 0.38587812\n",
      "Validation score: 0.387850\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[CV]  vect__ngram_range=(1, 2), vect__max_features=2500, tfidf__use_idf=True, clf__hidden_layer_sizes=(4096, 1024), score=0.4071996259934549, total= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 83.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11.98153405\n",
      "Validation score: 0.172897\n",
      "Iteration 2, loss = 6.83311338\n",
      "Validation score: 0.413551\n",
      "Iteration 3, loss = 5.34259274\n",
      "Validation score: 0.415888\n",
      "Iteration 4, loss = 3.94714883\n",
      "Validation score: 0.436916\n",
      "Iteration 5, loss = 2.56927131\n",
      "Validation score: 0.455607\n",
      "Iteration 6, loss = 1.49744491\n",
      "Validation score: 0.474299\n",
      "Iteration 7, loss = 0.83678381\n",
      "Validation score: 0.474299\n",
      "Iteration 8, loss = 0.46732589\n",
      "Validation score: 0.469626\n",
      "Iteration 9, loss = 0.27468449\n",
      "Validation score: 0.462617\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "rs_clf = RandomizedSearchCV(clf, rs_parameters, cv=2, n_iter=10, n_jobs=1, verbose=10, random_state=1)\n",
    "start = time.time()\n",
    "rs_clf = rs_clf.fit(X_train, encoded_y_train)\n",
    "rs_processing_time = (time.time() - start) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42262739597942967\n"
     ]
    }
   ],
   "source": [
    "best_score = rs_clf.best_score_\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vect__ngram_range': (1, 4), 'vect__max_features': 7500, 'tfidf__use_idf': True, 'clf__hidden_layer_sizes': (4096, 1024)}\n"
     ]
    }
   ],
   "source": [
    "best_params = rs_clf.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cv': 2, 'error_score': 'raise', 'estimator__memory': None, 'estimator__steps': [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
      "       warm_start=False))], 'estimator__vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'estimator__tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'estimator__clf': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
      "       warm_start=False), 'estimator__vect__analyzer': 'word', 'estimator__vect__binary': False, 'estimator__vect__decode_error': 'strict', 'estimator__vect__dtype': <class 'numpy.int64'>, 'estimator__vect__encoding': 'utf-8', 'estimator__vect__input': 'content', 'estimator__vect__lowercase': True, 'estimator__vect__max_df': 1.0, 'estimator__vect__max_features': None, 'estimator__vect__min_df': 1, 'estimator__vect__ngram_range': (1, 1), 'estimator__vect__preprocessor': None, 'estimator__vect__stop_words': None, 'estimator__vect__strip_accents': None, 'estimator__vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'estimator__vect__tokenizer': None, 'estimator__vect__vocabulary': None, 'estimator__tfidf__norm': 'l2', 'estimator__tfidf__smooth_idf': True, 'estimator__tfidf__sublinear_tf': False, 'estimator__tfidf__use_idf': True, 'estimator__clf__activation': 'relu', 'estimator__clf__alpha': 0.0001, 'estimator__clf__batch_size': 'auto', 'estimator__clf__beta_1': 0.9, 'estimator__clf__beta_2': 0.999, 'estimator__clf__early_stopping': True, 'estimator__clf__epsilon': 1e-08, 'estimator__clf__hidden_layer_sizes': (100,), 'estimator__clf__learning_rate': 'constant', 'estimator__clf__learning_rate_init': 0.001, 'estimator__clf__max_iter': 200, 'estimator__clf__momentum': 0.9, 'estimator__clf__nesterovs_momentum': True, 'estimator__clf__power_t': 0.5, 'estimator__clf__random_state': 1, 'estimator__clf__shuffle': True, 'estimator__clf__solver': 'adam', 'estimator__clf__tol': 0.0001, 'estimator__clf__validation_fraction': 0.1, 'estimator__clf__verbose': True, 'estimator__clf__warm_start': False, 'estimator': Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip...\n",
      "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
      "       warm_start=False))]), 'fit_params': None, 'iid': True, 'n_iter': 10, 'n_jobs': 1, 'param_distributions': {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)], 'vect__max_features': (100000, 50000, 25000, 10000, 7500, 5000, 2500, 1000, 500, 300, 100), 'tfidf__use_idf': (True, False), 'clf__hidden_layer_sizes': ((2048, 1024), (2048, 512), (1024, 512), (512, 128), (4096, 1024), (4096, 512), (2048, 1024, 512), (1024, 512, 128))}, 'pre_dispatch': '2*n_jobs', 'random_state': 1, 'refit': True, 'return_train_score': 'warn', 'scoring': None, 'verbose': 10}\n"
     ]
    }
   ],
   "source": [
    "rs_clf_params = rs_clf.get_params()\n",
    "print(rs_clf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict \n",
    "rs_predicted = rs_clf.predict(X_test)\n",
    "#print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8223399547439526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# precision is a measure of result relevancy\n",
    "rs_precision = precision_score(encoded_y_test, rs_predicted, average='samples')\n",
    "print(rs_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7248613465297666\n"
     ]
    }
   ],
   "source": [
    "# recall is a measure of how many truly relevant results are returned\n",
    "rs_recall = recall_score(encoded_y_test, rs_predicted, average='samples')  \n",
    "print(rs_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7522693151660362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# F1 score is a weighted average of the precision and recall\n",
    "rs_f1 = f1_score(encoded_y_test, rs_predicted, average='samples') \n",
    "print(rs_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.21      0.32       515\n",
      "          1       1.00      0.02      0.05       173\n",
      "          2       0.75      0.53      0.62      3892\n",
      "          3       0.99      0.41      0.58       265\n",
      "          4       0.80      0.19      0.31      1003\n",
      "          5       1.00      0.04      0.09       224\n",
      "          6       1.00      0.01      0.01       173\n",
      "          7       0.73      0.45      0.56      2592\n",
      "          8       0.00      0.00      0.00       161\n",
      "          9       0.89      0.94      0.91     13173\n",
      "         10       0.88      0.92      0.90     11322\n",
      "         11       0.73      0.35      0.48      2249\n",
      "         12       0.97      0.10      0.18       398\n",
      "         13       0.98      0.19      0.32       332\n",
      "         14       0.00      0.00      0.00       162\n",
      "         15       0.82      0.33      0.47      1286\n",
      "         16       0.65      0.18      0.28       770\n",
      "         17       0.82      0.82      0.82      8507\n",
      "         18       0.94      0.14      0.25       202\n",
      "         19       0.93      0.14      0.24       202\n",
      "         20       0.97      0.07      0.13       542\n",
      "         21       0.69      0.38      0.49      2301\n",
      "         22       0.78      0.19      0.30      1003\n",
      "         23       1.00      0.05      0.10       169\n",
      "         24       0.66      0.20      0.31       515\n",
      "         25       0.72      0.51      0.60      1152\n",
      "         26       0.82      0.51      0.63      1478\n",
      "         27       0.95      0.55      0.69       441\n",
      "         28       0.85      0.46      0.59       808\n",
      "         29       0.90      0.52      0.66       606\n",
      "         30       0.82      0.79      0.80      7140\n",
      "         31       0.87      0.73      0.79      4103\n",
      "         32       0.77      0.29      0.42      1203\n",
      "         33       0.78      0.30      0.43      1051\n",
      "         34       0.94      0.07      0.14       455\n",
      "\n",
      "avg / total       0.83      0.68      0.71     70568\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(encoded_y_test, rs_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ergebnisse in Dateien speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = '../MLP'\n",
    "if not os.path.exists(output):\n",
    "    os.makedirs(output)\n",
    "    \n",
    "timestamp = time.strftime('%Y-%m-%d_%H.%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED:\n",
      "('histoire et archéologie_d', 'histoire_t', 'pluridisciplinarité_d', 'épistémologie et méthodes_t')\n",
      "TRUE:\n",
      "['pluridisciplinarité_d', 'épistémologie et méthodes_t', 'histoire_t', 'histoire et archéologie_d']\n"
     ]
    }
   ],
   "source": [
    "# write real labels and predictions to file\n",
    "\n",
    "inverse_prediction = label_encoder.inverse_transform(rs_predicted)\n",
    "print('PREDICTED:')\n",
    "print(inverse_prediction[0])\n",
    "print('TRUE:')\n",
    "print(y_test[0])\n",
    "\n",
    "with open(output+'/MLP_reducedlabels_rs_predictions_%s.txt' % timestamp,\"w+\", encoding=\"utf8\") as preds:\n",
    "    preds.write(\"Predictions from classification with Multi-Layer-Perzeptron and vectorization in scikit-learn (reduced labels):\\n\\n\")\n",
    "    for ident, label, pred in zip(z_test, y_test, inverse_prediction):\n",
    "        label = sorted(label)\n",
    "        pred = sorted(pred)\n",
    "        preds.write(ident)\n",
    "        preds.write('\\n')\n",
    "        preds.write('TRUE: ')\n",
    "        for element in label:\n",
    "            preds.write('%s, ' % element)\n",
    "        preds.write('\\n')\n",
    "        preds.write('PRED: ')\n",
    "        for element in pred:\n",
    "            preds.write('%s, ' % element)\n",
    "        preds.write('\\n')\n",
    "        preds.write('\\n*********************\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write parameters and scores to file\n",
    "\n",
    "with open(output+'/MLP_reducedlabels_rs_params_%s.txt' % timestamp,\"w+\", encoding=\"utf8\") as params:\n",
    "    params.write(\"Parameters for classification with Multi-Layer-Perceptron and vectorization in scikit-learn from randomized search (reduced labels):\")\n",
    "    params.write(\"\\nprocessing_time: %s\" % rs_processing_time)\n",
    "    params.write(\"\\nparams:\")\n",
    "    for key, value in rs_clf_params.items():\n",
    "        params.write(\"\\n%s: %s\" % (key, value))\n",
    "    params.write(\"\\nbest params:\")\n",
    "    for key, value in best_params.items():\n",
    "        params.write(\"\\n%s: %s\" % (key, value))\n",
    "    params.write(\"\\nbest_score: %s\" % best_score)\n",
    "    params.write(\"\\nprecision: %s\" % rs_precision)\n",
    "    params.write(\"\\nrecall: %s\" % rs_recall)\n",
    "    params.write(\"\\nf1-score: %s\" % rs_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0    1021.157406     57.766818         3.453526        0.015617   \n",
      "1     116.286588     27.620311         2.453402        0.015627   \n",
      "2      32.347425     12.563904         2.023651        0.039051   \n",
      "3      95.268586     10.196465         1.343918        0.015611   \n",
      "4     201.663418     13.282741         1.445475        0.054693   \n",
      "5     223.447138     10.907479         3.297263        0.062524   \n",
      "6       5.922545      0.750085         0.617257        0.054694   \n",
      "7      27.104635      9.930817         1.742378        0.085952   \n",
      "8     589.957340     29.487709         1.742385        0.007812   \n",
      "9     146.305600     15.822109         1.984600        0.015627   \n",
      "\n",
      "  param_vect__ngram_range param_vect__max_features param_tfidf__use_idf  \\\n",
      "0                  (1, 3)                    50000                False   \n",
      "1                  (1, 3)                     2500                False   \n",
      "2                  (1, 4)                     5000                False   \n",
      "3                  (1, 2)                    10000                 True   \n",
      "4                  (1, 2)                    25000                 True   \n",
      "5                  (1, 4)                     7500                 True   \n",
      "6                  (1, 1)                      500                False   \n",
      "7                  (1, 3)                      500                False   \n",
      "8                  (1, 1)                    25000                 True   \n",
      "9                  (1, 2)                     2500                 True   \n",
      "\n",
      "  param_clf__hidden_layer_sizes  \\\n",
      "0                  (4096, 1024)   \n",
      "1                  (4096, 1024)   \n",
      "2                    (512, 128)   \n",
      "3                   (1024, 512)   \n",
      "4                   (1024, 512)   \n",
      "5                  (4096, 1024)   \n",
      "6              (1024, 512, 128)   \n",
      "7             (2048, 1024, 512)   \n",
      "8                  (4096, 1024)   \n",
      "9                  (4096, 1024)   \n",
      "\n",
      "                                              params  split0_test_score  \\\n",
      "0  {'vect__ngram_range': (1, 3), 'vect__max_featu...           0.423562   \n",
      "1  {'vect__ngram_range': (1, 3), 'vect__max_featu...           0.385694   \n",
      "2  {'vect__ngram_range': (1, 4), 'vect__max_featu...           0.301075   \n",
      "3  {'vect__ngram_range': (1, 2), 'vect__max_featu...           0.429640   \n",
      "4  {'vect__ngram_range': (1, 2), 'vect__max_featu...           0.426835   \n",
      "5  {'vect__ngram_range': (1, 4), 'vect__max_featu...           0.424497   \n",
      "6  {'vect__ngram_range': (1, 1), 'vect__max_featu...           0.214586   \n",
      "7  {'vect__ngram_range': (1, 3), 'vect__max_featu...           0.301075   \n",
      "8  {'vect__ngram_range': (1, 1), 'vect__max_featu...           0.422627   \n",
      "9  {'vect__ngram_range': (1, 2), 'vect__max_featu...           0.407667   \n",
      "\n",
      "   split1_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
      "0           0.412342         0.417952        0.005610                4   \n",
      "1           0.382422         0.384058        0.001636                7   \n",
      "2           0.396447         0.348761        0.047686                8   \n",
      "3           0.405330         0.417485        0.012155                5   \n",
      "4           0.416550         0.421692        0.005143                2   \n",
      "5           0.420757         0.422627        0.001870                1   \n",
      "6           0.304815         0.259701        0.045115               10   \n",
      "7           0.350164         0.325619        0.024544                9   \n",
      "8           0.417017         0.419822        0.002805                3   \n",
      "9           0.407200         0.407433        0.000234                6   \n",
      "\n",
      "   split0_train_score  split1_train_score  mean_train_score  std_train_score  \n",
      "0            0.638616            0.529687          0.584151         0.054465  \n",
      "1            0.586723            0.432445          0.509584         0.077139  \n",
      "2            0.304815            0.486676          0.395746         0.090930  \n",
      "3            0.755961            0.538569          0.647265         0.108696  \n",
      "4            0.814867            0.691445          0.753156         0.061711  \n",
      "5            0.664797            0.604488          0.634642         0.030154  \n",
      "6            0.231884            0.300608          0.266246         0.034362  \n",
      "7            0.304815            0.386629          0.345722         0.040907  \n",
      "8            0.803179            0.693782          0.748481         0.054698  \n",
      "9            0.660589            0.855072          0.757831         0.097242  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\hartmann\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "results = rs_clf.cv_results_\n",
    "df = pd.DataFrame(data=results)\n",
    "print(df)\n",
    "df.to_csv(output+'/MLP_reduced_labels_rs_results_%s.csv' % timestamp, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
